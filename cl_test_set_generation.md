# Computation and Language Paper Abstracts Generated by GPT-2 (Test set)

1. **Generalizing Natural Language Analysis through Span-relation Representations**
  Natural Language Processing (NLP) tasks, such as information extraction, question answering, and information extraction from text, are difficult to scale due to high complexity in the number of possible cases that the same questions can be asked. In this paper, we propose a system which learns a set of span-relation representations which can be used for a range of NLP tasks and generalizes natural language analysis from scratch. We show that the representations learn to capture the structural properties of the input sentence and are capable of recovering correct answers for various problems. We also show that the learned representations generalize over training data to new tasks, and that the span-relation representations encode grammatical properties of the sentences.
  **Translationese as a Language in "Multilingual" NMT**
2. The cross-lingual nature of neural machine translation (NMT) stems from the dependency of the translation process on the syntax of a specific source language. Despite this dependency, most research on multilingual NMT has focused on translation in monolingual settings. We report experimental results for multilingual translation using a variant of neural MT. This variant aims at translating meaning more accurately by using a multilingual word representation. By presenting three different NMT architectures using bilingual and monolingual datasets in English, French, and German, we evaluate their translation quality on the WMT'14 English-German and WMT'14 English-French translation tasks. Our results show that the multilingual NMT architecture improves both translation accuracy and speed, but it performs similarly to monolingual NMT on all three tasks.
  **Stylized Text Generation Using Wasserstein Autoencoders with a Mixture of Gaussian Prior**
  In this paper, we aim at creating personalized stylized text by adapting a simple neural-network-based model to text generation of representative phrases in different styles. We utilize a Wasserstein autoencoder to automatically generate styles-specific representations of phrases, and then use these representations in a regularized version of variational auto-encoder to generate personalized stylized texts. The proposed model is able to achieve the best of both worlds, i.e., generating styles-specific content with a small amount of training data and resulting in high-quality stylized texts for some sub-domains of a domain. The proposed model was evaluated on the two benchmark datasets of German and Spanish, and the results show a very significant performance gain for both datasets over the traditional LaTeX-based LaTeX-to-Text (L2T) approach.
3. **YELM: End-to-End Contextualized Entity Linking**
  We introduce YELM, a method for full-reference contextualized entity linking using a sequence of high-level reasoning to extract contexts from text. YELM brings together techniques for single-document entity linking, multi-document entity linking, and multi-document entity knowledge graph linking. We show that YELM can achieve state-of-the-art results across diverse data sets including biomedical, medical social media, and business records, and more than $2000$ times faster than previous methods.
  **Queens are Powerful too: Mitigating Gender Bias in Dialogue Generation**
4. In dialogue generation, the conversational partner typically assumes a male or female conversational persona, thus requiring fine-tuning the dialogue model to reflect this gender bias in response-dependent utterances. In this work, we evaluate the gender bias of deep neural network-based dialogue models in two ways. First, we compare models with and without gender bias when constructing initializations, and find that gender bias can be maintained while training, even when the task does not change during training. Second, we take advantage of models pre-trained on the general language modeling task and then fine-tune them for response-dependent utterances. We report that pre-training yields improvements in response-dependent state abstraction, as measured by performance in two domain adaptation tasks ( Dialogue Evaluation Center and Biography Queries), on 4 diverse datasets.
  **Conditional Response Generation Using Variational Alignment**
  How to generate a response to an incoming query is critical for various applications such as review answering, spam detection, and social media communication. For this purpose, we propose a conditional variational autoencoder (cVAE) model that learns to generate a response conditioned on the underlying query. The cVAE uses a latent variable to represent a query vector, and another latent variable to generate an encoded response vector. The encoded response vector captures various aspects of the query that are rarely or never handled by the encoder. We show that the cVAE generates nearly ideal response vectors. This method can be used to generate responses to query-based retargeting and recommendation methods as well as to create response templates for personalization and fusion. To the best of our knowledge, the cVAE is the first model for conditional response generation and is also the first model for probabilistic inference that is conditioned on the input and constructed using a shared latent space. This allows us to draw connections between the probabilistic inference model used for conditional inference in Bayesian networks and the VAE model used in this paper.
5. **PoD: Positional Dependency-Based Word Embedding for Aspect Term Extraction**
  We propose a technique for aspect term extraction which can be applied to a wide variety of natural language processing tasks. We employ a self-attention network (SAN) to capture the relationship between tokens in two different utterances. In this paper, we present a decomposition of these two networks, where our main architecture serves as the state of the art for parsing sentiment words, whereas our baseline utilizes a bag-of-words approach. As a side contribution, we make the large-scale training corpus for aspect term extraction publicly available.
  **Zero-shot Entity Linking with Dense Entity Retrieval**
6. Zero-shot entity linking (ZEL) models typically require the annotations of both entities and attributes, making them labor-intensive to train and hard to generalize. To address this problem, in this paper, we propose a generic entity linker that only requires entity labels and only needs both the entity and attribute labels, alleviating the data sparsity issue. Specifically, we first create a knowledge base of entity and attribute annotations, which can be readily translated to existing datasets. Then, we use entity and attribute labels to define a binary label map, and resolve entity and attribute relationship to predicted labels by iteratively forward-passing the label map through a bidirectional LSTM. Furthermore, we exploit entity attributes to quantify the discriminative power of the label maps, and use them to train a multi-scale residual network to learn a dense mapping between all entity and attribute labels. As a result, we achieve state-of-the-art entity link accuracy, while also speeding up inference compared to existing methods.
  **Error Analysis for Vietnamese Dependency Parsing**
  Sentence parsing in Vietnamese has long been considered as a challenging task due to the rich morphology of the language. It is still open as to how Vietnamese dependency parsing can be improved, due to the state-of-the-art models based on recurrent neural networks (RNN). We introduce a new independent error analysis scheme that tries to study the relationship between sentence quality and the intrinsic difficulty of parsing. The experiments show that our scheme helps us discover that the hierarchical models significantly outperform the state-of-the-art models in Vietnamese dependency parsing. Our analysis also offers insight to how the model quality depends on the relation between the input sentence and the parse result.
7. **Sentence Meta-Embeddings for Unsupervised Semantic Textual Similarity**
  Real-world natural language understanding is difficult, especially for low-resource languages. Thus, there is a great need to develop methods that are robust for such settings. The reason for this is the high discrepancy between the distribution of words in natural language and in controlled natural-language text. As a result, languages are much more similar to each other in supervised text than in natural language text. Moreover, these distributional differences between natural language texts are not yet well-labeled. Consequently, standard word embeddings may not be trained well enough to model distributional differences between natural language texts and other language text. In this work, we propose the first unsupervised textual similarity metric called the sentence meta-embedding, which is based on the sentence-level data distribution instead of the word-level data distribution. We show that by using this metric the discriminative power of distributional word embeddings can be improved, and the semantic similarity between natural language texts can be better estimated. Furthermore, we compare different word embedding models on our sentence meta-embedding.
  **Learning to Copy for Automatic Post-Editing**
8. In this paper, we consider the problem of automatic post-editing. We propose a new network architecture with three components: a weakly supervised content-aware contextual network, a network of copy modules and a data attention network. In the proposed network, the contextual network produces a multi-level representation of the content, which is used by copy modules to generate good copy. Then we exploit this representation with the attention network to create a sequence of regions with shared content. Then we use the results of the attention network to train a content-aware prior for the copy modules, which makes the generation of better copy possible. In the experiments, we evaluate the proposed network on two standard benchmarks, and show that it performs better than state-of-the-art baselines.
  **Vietnamese transition-based dependency parsing with supertag features**
  Transition-based dependency parsers trained by considering multiple constraints have shown their accuracy in comparison to those of self-attention-based dependency parsers. However, there is still an outstanding gap between them. The reason is that self-attention-based parsers do not use a transition-based representation. In this paper, we propose a simple yet effective method to apply transition-based representations to parser models. Our method aims to improve self-attention-based parsing and is derived from the idea that high-level information can be incorporated into parse trees by using supertags. In addition, we propose a transition-based regularization based on the S-tree transition matrix to encourage parser training that overcomes the strong gradient vanishing problem. Experiments show that our method improves parser accuracy in comparison to some state-of-the-art self-attention-based parsers in a number of languages.
9. **Hierarchical Graph Network for Multi-hop Question Answering**
  Multi-hop Question Answering (QA) has become a popular research topic due to its important applications, such as online learning and product recommendation. Previous research on QA mainly focuses on either achieving the global state of knowledge from individual steps or applying model based methods for sub-categorization to reach the global state. However, the previous methods make the naive assumption that all the entities/links in the hierarchy are present in the answer in all steps and thus fail to achieve the global state. In this work, we propose the Hierarchical Graph Network (HiGNN), a novel model to learn the global state for all the entities and links by simultaneously learning both the entity representation and the link representation from the entire hierarchy. In addition, HiGNN also employs a sub-critic mechanism for controlling the exploration of links in the hidden state space. The proposed HiGNN is a mixed-integer programming model and hence, naturally can be solved efficiently by both integer linear programming (ILP) and multi-point linear programming (MPLP) solvers. Experiments on the benchmark dataset demonstrate that our proposed model outperforms state-of-the-art methods on both the predictive accuracy and performance over a single-hop QA task.
  **Hate Speech Detection on Vietnamese Social Media Text using the Bi-GRU-LSTM-CNN Model**
10. With the rapid development of social media, there are plenty of racist and hateful speech posted everyday. It is very important to identify these offensive messages and suppress them before they are spread. There are no state-of-the-art hate speech detection methods in Vietnamese social media text. We propose a Vietnamese hate speech detection method based on the Bi-GRU-LSTM-CNN model. The proposed model includes two LSTM modules to extract the content features and a CNN module to classify the messages. Experimental results on the hateful comment data show that the proposed method outperforms state-of-the-art methods and achieves 84.75% and 71.77% of accuracy for the categories of anti-blackness and anti-white racism respectively.
  **Multi-Perspective Inferrer: Reasoning Sentences Relationship from Holistic Perspective**
  Generating a multi-perspective interpretation is the task of inferring, given a pair of sentences, a viewpoint that yields a complete narrative. To this end, we first introduce a new annotated multi-perspective dataset called Multi-Perspective Inferrer (MPI). MPI consists of simple sentences, taken from real world text (big or small). Each sentence is annotated by the speaker(s), the subject(s) and the verb, and we also provide two score systems, i.e. one by independent human raters and the other by supervised machine learning algorithms. Such dataset complements existing multimodal datasets. Finally, we introduce a novel semi-supervised model called Multi-Perspective Inferrer (MP-PI), which generates a multi-perspective inference given a set of pairs of sentences. Our experiment results on MPI show that MP-PI achieves state-of-the-art performance on two common multi-perspective inference tasks, i.e. speaker classification and subject/verb classification.
11. **ConveRT: Efficient and Accurate Conversational Representations from Transformers**
   A deep-learning-based model for high-level conversation understanding called convolutional neural network (ConvNet) has been proposed in recent years and achieved state-of-the-art results on standard benchmarks, e.g., Switchboard, DynaSpeech and AMI-CC. Despite the remarkable performances of ConvNet, a large-scale human evaluation is missing. In this paper, we present ConveRT, a deep-learning-based model that generates generic conversational responses from questions and other conversational input and is highly efficient, using only 200 hours of training data and 50ms for the prediction on a single GPU. Experimental results with the Switchboard dataset show that ConveRT produces responses that are comparable with the latest state-of-the-art models, and are also much more human-like. Our model is also faster and more accurate, because it only needs to decode individual word tokens.
   **BERT is Not a Knowledge Base (Yet): Factual Knowledge vs. Name-Based Reasoning in Unsupervised QA**
12. As part of the NLP family, attention-based encoder-decoder models are at the heart of state-of-the-art systems for question answering in large corpora. In this work we investigate whether this family of models can perform similar language processing tasks that require reasoning, which are beyond simple parse or sentence understanding. We then show that, despite the impressive empirical results obtained in several languages, there is still much work to be done when it comes to computing under what conditions these models are capable of learning to answer questions about facts, and which questions must they answer for further progress. We provide a set of empirical findings that confirm that current QA models do not answer questions that require reasoning; however, we also show that under a version of this task where a large and sparse training corpus is available, general knowledge should not be needed. Overall, these results indicate that our understanding of QA tasks that require linguistic reasoning is still very limited.
   **Bootstrapping Disjoint Datasets for Multilingual Multimodal Representation Learning**
   In this work, we investigate a modular approach to transfer learning across language-pair pair mappings that we refer to as bootstrapping. We focus on learning cross-lingual representations for multimodal applications in the context of image captioning. In particular, we propose a universal cross-lingual captioning network that encodes multimodal information in a single shared embedding space. Given a target sentence, we feed it through an attention module, which predicts to which of the target languages the corresponding image belongs. We then independently train two representations: the captionive encoder and the bilingual encoder. To compare our method with existing cross-lingual methods, we propose two language-pair similarity measures. We demonstrate the effectiveness of our method on three multimodal image captioning tasks: monolingual image captioning, captioning cross-lingual sentences and image retrieval. We achieve state-of-the-art performance with the latter task.
13. **On Architectures for Including Visual Information in Neural Language Models for Image Description**
   Training neural language models on images with auxiliary visual information is a challenge because it is fundamentally non-differentiable, expensive, and difficult to collect enough training data for a large-scale dataset. It is also difficult to combine multiple auxiliary modalities at training time for a given target modality. In this paper, we present an architecture for combining multiple modalities, i.e., multi-modal LM-NLMs, where the intermediate representation is obtained by simultaneously learning a set of representations that encode different visual cues in multiple latent spaces. Our experimental results demonstrate that multi-modal LM-NLMs consistently outperform previous multimodal models on both visual and natural language recognition tasks on six standard datasets.
   **Subjective Sentiment Analysis for Arabic Newswire Comments**
14. The term `Newswire' is used in the Semitic languages to refer to news articles and blogs (especially online magazines and blogs, such as Blogger and Tumblr) during the past decade. There are now millions of Arabic Newswire comments to virtually every English Newswire article and every English Newswire blog article. With the increase of the usage of Arabic Newswire, a standard system is developed to automatically classify the Arabic Newswire comments into four basic categories: positive, neutral, negative and other. These four categories are represented with different word counts which lead to a more than 100% accuracy. In this paper, we use the popular AND/OR search engine for Arabic news articles to develop a classifier to classify newswire comments. Our algorithm utilizes word count, syntactic tree, WordNet, and dependency tree in order to extract semantic and style information from the Arabic Newswire comments. Furthermore, with the help of the tree structure, we combine the global information of this text into a top-level classifier to accurately classify the Arabic Newswire comments.
   **Code-Mixed to Monolingual Translation Framework**
   We present a methodology that automatically infers the high-quality translations from high-resource monolingual text using multi-source disambiguation. To this end, we propose the Code-Mixed to Monolingual Translation Framework (CMNTF). The proposed framework includes several levels of ensembles: (1) monolingual network using static semantic embeddings, and (2) model trained with attention based on noisy multi-source disambiguation. We demonstrate that the proposed methods yield a performance comparable to that of monolingual machine translation methods, while at the same time significantly reducing the amount of training time.
15. **Multi-Sentence Argument Linking**
   Recent approaches to argument linking have relied on bidirectional long short-term memory (BiLSTM) networks which remember history information for better matching of arguments. However, the detailed context of an argument or a concept is largely ignored and the emphasis placed on short-term memory limits its efficiency. In this work, we use Gaussian language model to contextualize the context in which an argument is linked, by learning the state of the context model with independent LSTMs. We obtain model that generalizes well across different datasets, while matching the state of the art for argument linking on a number of standard datasets.
   **Dynamic Knowledge Graph Construction for Zero-shot Commonsense Question Answering**
16. Question answering (QA) in the form of a visual question-answering (VQA) task is an emerging area for deep learning research and real-world applications. At present, however, the models based on visual QA tasks are largely incapable of dealing with zero-shot knowledge graph construction, i.e. learning knowledge from the open-world, and may perform poorly in a zero-shot VQA scenario. In this paper, we propose a novel end-to-end deep learning model to tackle this problem. Our model learns a self-attentive latent space of knowledge graph facts via a combination of convolutional neural networks (CNNs) and word embeddings and can be utilized to build a general knowledge graph, which is employed as input to a visual QA model for visualizing QA instances in a VQA scenario. Extensive experiments on benchmark datasets demonstrate that our model outperforms state-of-the-art models in all three evaluation metrics.
   **CCMatrix: Mining Billions of High-Quality Parallel Sentences on the WEB**
   In this paper, we introduce the very first publicly available parallel sentence mining corpus, collectively dubbed the ``WEB'' (Weighted Answer Backstream) corpus. The WEB corpus contains 2 billion parallel sentences consisting of 60 billion parallel facts extracted from Web pages, news sites, and social media. In particular, we aim at providing the first service enabling researchers and practitioners to perform high-quality supervised training of neural models for these tasks. To facilitate parallel sentence mining research, the WEB corpus is available under a per-user per-word license and remains under constant ownership of the project organizers, whose collective labor will be used to create robust machine learning systems for downstream applications such as information retrieval, news summarization, document classification, etc. The WEB corpus is distributed under a per-user per-word license, which in addition to its practical use makes it especially suitable for online collaborative research and development. Furthermore, we introduce a new algorithm, CCMatrix, for mining parallel sentences with respect to their document-based representations. CCMatrix works by establishing the correspondence between document-based representations computed by neural models and the knowledge bases that they use to make predictions. Therefore, our approach is capable of mining parallel sentences which are consistently better than those discovered by neural models trained with existing supervised learning techniques. Our results show that the mining efficiency of CCMatrix approaches has been significantly improved, achieving state-of-the-art results for the downstream tasks of information retrieval, news summarization, document classification and cross-sentence retrieval.
17. **r/Fakeddit: A New Multimodal Benchmark Dataset for Fine-grained Fake News Detection**
   With the explosion of the social media community and the spread of fake news on it, it is a well-known fact that the social media users share and spread fake news stories. Consequently, a massive number of fake news stories, also known as fake news, are also generated in the Internet. In order to prevent these fake news from spreading and affecting the public trust in the Internet, many fake news detection systems have been developed and deployed to detect fake news stories. However, fake news stories are often generated in diverse domains. These false news stories also differ from real news in several aspects, such as wording, distribution channel, and type of user. With the constant growth of the online social community, more and more fake news stories are being generated. Moreover, the publics' trust in the Internet has been diminishing. Consequently, there are many fake news stories being spread in social media. In order to solve the problem of fake news detection, in this paper, we developed a large-scale and multimodal benchmark dataset, called r/Fakeddit. We also introduced a new weakly supervised dataset based on Wikipedia, called WikipediaFC. The benchmark dataset and weakly supervised dataset are released to promote further research on fake news detection. Experimental results show that the multimodal dataset and weakly supervised dataset provide promising opportunities for future research on fake news detection.
   **Correcting Sociodemographic Selection Biases for Accurate Population Prediction from Social Media**
18. Computational power of machine learning, especially machine learning applied to social media data, has dramatically increased in recent years. However, the multidimensional nature of social media data presents multiple issues for analysis. First, a direct consequence of social media platforms' emphasis on self-expression and originality, is the propensity for users to share their true-intentions about others. These aspects of the population contribute to certain biases in the prediction of demographic groups, making accurate population prediction even more challenging. Second, social media has evolved over time, evolving along gender and race demographics. As an example, news media are used as a powerful tool to spread information about certain social issues that impact an individual's life. Moreover, social media has increased the sharing of stories and messages from various communities and topics. The increased volume of social media and online publication of stories can add further layers of noise to population prediction. To address these issues, this paper employs social media data to obtain demographic profiles of social media users in a natural setting. As a benchmark to study this problem, we use three publicly available data sets, including the Social Media and Census Twitter Twitter, the Internet Adult population Twitter, and the Facebook User Aging Dataset. The comparison between the results obtained using the traditional machine learning tools such as Grams model, Naive Bayes model, and support vector machines, and the machine learning results with statistical inference approaches like random forests and k-nearest neighbors, reveals that the popular machine learning tools, especially those in the machine learning community, have limitations in handling the challenges in evaluating demographic information from social media.
   **Don't Say That! Making Inconsistent Dialogue Unlikely with Unlikelihood Training**
   Misleading aspects of non-constructive dialogues are frequent problems in non-constructive turn-taking dialogue systems, but even rarer are effective ways to deal with them: ensure the dialogue acts are unlikely given the dialog history, and use additional evidence that preserves consistency between responses. Though a variety of cross-domain training methods have been developed, to the best of our knowledge no general-purpose method has been proposed to leverage more than the standard likelihood loss with features extracted from a conversation tree. In this paper, we propose a highly-efficient adversarial training algorithm for non-constructive turn-taking dialogue, termed Unlikelihood-Train. It automatically identifies the plausibility of each response given a conversation history, and hence can be reliably trained end-to-end for consistency preservation. Compared with baseline methods, despite its simplicity, Contrarylihood-Train exhibits strong performance on the Dialogue State Tracking Challenge, outperforming the competition winner by up to 2.6% absolute points on average.
19. **Decompressing Knowledge Graph Representations for Link Prediction**
   Learning a compact yet discriminative knowledge graph embedding is an essential task for link prediction in knowledge graphs. Existing techniques usually generate suboptimal approximations and involve an expensive sampling strategy. In this paper, we propose to learn embedding by reducing the redundancy in a data-driven way and use as prior the suboptimal embeddings generated by prior techniques. In particular, we propose to decompose the embedding space into two subspaces, one for representing the node dependencies and the other for representing the knowledge graph structure. We treat the knowledge graph structure as prior knowledge, and embed the knowledge graph structure in a lower dimensional subspace by selecting low-dimensional embeddings. Experimental results on link prediction on real-world datasets show that our proposed model improves the generalization of the embedding by as much as 3.56\% in terms of Link Quality. Moreover, we can transfer the learned embedding to other tasks, such as link prediction on text corpora.
   **Social Bias Frames: Reasoning about Social and Power Implications of Language**
20. In this paper we describe how a particular form of implicit social bias, called social bias, manifests in language that may distort, undermine or reinforce racial and gender biases. We argue that understanding social bias requires the development of language models and for automatic processing of language, such models need to capture language patterns that are indicative of social bias. We then present a computational model based on a latent space of discourse aspects that are governed by a complex network of internal and external relations. Our results support the conclusions of recent literature on language models and social bias and suggest a new way of relating social bias to language.
   **INSET: Sentence Infilling with Inter-sentential Generative Pre-training**
   Infilling sentences is a fundamental step for natural language understanding (NLU). However, existing neural models often emphasize sentence repetition over sentence uniqueness, leading to low-quality infilled sentences. To this end, we propose a generative pre-training (GPT) method to enable joint training of sentence and knowledge latent representation (KL) layers. GPT-LSTM jointly infills both the prior information of sentence and KL contents, which leads to better NLU performance. In addition, a feedback-based attention mechanism is added to enhance the capacity of KL, which prevents the model from trivializing sentence content. The proposed GPT-LSTM can improve the NLU performance by at least 3.3 BLEU on five NLU benchmarks.
21. **CamemBERT: a Tasty French Language Model**
   A French language model named CamemBERT (Cam$^3$Berth$^3$T) was developed. Given the gap between language models on wide speech recognition and the amount of data, we have also developed a modular OCR decoder named as CrossDigit. It is built using the OCR encoder, a MIE decoder and OCR decoder trained on two different speech recognition corpora to replace the original OCR decoder. The two corpora were constructed by disambiguating the performance of two different MIE sentence encoders on each corpus. The two corpora were disambiguated at an individual level. The proposed model was evaluated on two different language modeling datasets. The baseline LSTM language model yielded 9.87% error rate on the TIMIT dataset. However, the context-aware LSTM language model trained on the second sentence of the TIMIT corpus resulted in 0.831% error rate. The results show that the system works with the high quality of the high-quality corpora and the low quality of the lower quality corpus.
   **Two-Headed Monster And Crossed Co-Attention Networks**
22. In this paper, we address the cross-view sentiment prediction problem with the help of two-headed deep attention networks. Two-headed attention networks are built on top of fully convolutional networks, i.e. the decoder is operated in two-headed. Two-headed attention networks can not only learn to focus on both positive and negative information (positive decoder), but can also learn to jointly focus on both positive and negative information (negative decoder), which is beneficial for cross-view sentiment prediction. We theoretically analyze the effectiveness of the proposed two-headed attention networks on image and text classification tasks, and experimentally validate the effectiveness of the two-headed attention networks on two benchmark data sets, i.e. the REBEL dataset and the SVT dataset.
   **A Re-evaluation of Knowledge Graph Completion Methods**
   Knowledge graph completion has recently received a lot of attention due to the practicality of its use in many applications. While most previous works focus on predicting missing facts, here we aim to analyze the assumptions and limitations of existing methods and propose an evaluation framework to further advance the state-of-the-art. We first identify the problems associated with existing knowledge graph completion methods, which could be addressed by enforcing different types of constraints. We then present some interesting work that addresses these limitations and, surprisingly, also tackles the situation in which the required knowledge is not provided. We further explore different directions for improving the effectiveness of existing completion methods, such as automatic ranking of obtained triples or weighted verification to show that the proposed evaluation framework can be used for assessing the quality of existing methods. Our work shows that by enforcing different types of constraints, such as (i) having enough information for feature extraction and (ii) addressing for the issue in which the necessary knowledge is not available, methods can learn a more compact and accurate representations of facts that help them to help users to make better inferences.
23. **Semantic Noise Matters for Neural Natural Language Generation**
   In this paper, we show that the randomness of the initial word order of neural language models significantly affects its quality of generated text. In particular, we find that using an initial order of words significantly reduces the quality of the result while using a fixed order provides the same quality. In other words, a `noise model' generalizes better than a `noise clean' model. In addition, we also find that a set of semantics priors that capture the importance of words in a document further improves the performance of a neural natural language generator.
   **Efficient Dialogue State Tracking by Selectively Overwriting Memory**
24. Efficient human-computer interaction relies on dialogue state tracking (DST), a challenging problem due to the unavailability of large amounts of direct human feedback. We present a systematic approach that reconstructs dialogue states from memory from a single turn-based neural network, with negligible human annotation cost. Specifically, we perform neural architecture search and explore different combinations of neural structures for DST, performing ablations for parameter space exploration and introducing novel loss functions for search and model parameter tuning. We perform experiments on the newly released Dialog System Technology Challenge (DSTC4) dataset, which contains human feedback on 636 dialogues. The experimental results show that our memory-augmented model achieves state-of-the-art results (85.2% mean precision and 62.2% recall) for DST, with a fast runtime of 1.83 seconds and a larger average precision than a recently published network based on recurrent neural networks, which is found to be similar to human-held human opinion.
   **Searching for Legal Clauses by Analogy. Few-shot Semantic Retrieval Shared Task**
   This paper describes a shared task at Twenty-fourth Conference on Uncertainty in Artificial Intelligence on named entity recognition (NER) where four teams from ten participants provided a detailed description of a model for NER. This is the first shared task on this topic. All of the teams focused on the problem of finding similarity/abstraction between legal clauses and their text descriptions. The proposed models were evaluated on legal documents from a publicly available corpus and also two different datasets (Resource and Answer Set Programming) of natural language text. As the first shared task, all submitted models are reviewed by human judges and experts. Following this review, the final results of each team were analyzed to gain a better understanding of the difficulty of the problem and best models. The shared task was hosted in WN2019 along with the TED Legal School Shared Task 2018.
25. **Word Sense Disambiguation using Knowledge-based Word Similarity**
   This paper proposes the concept of semantic role of word sense disambiguation (S$_o$S$). The semantic role of a word is estimated using the hypothesis of its semantic role. It is shown that for any given sentence, the semantic role of its word is the probability of its sense-segmentation; and the marginal probabilities of its senses are its probabilities of disambiguation. For this purpose, a Bayesian approach is used to calculate a log-probability of a word's sense. Experimental results of classifying words according to their sense segmentations on the web-based dictionary, Lexical Web, and LSMDC, indicate the feasibility of using the proposed method.
   **Can Monolingual Pretrained Models Help Cross-Lingual Classification?**
26. The rise of cross-lingual embeddings for knowledge graph (KG) data has led to great improvement of cross-lingual tasks. However, most existing methods only target the semi-supervised and unsupervised tasks of learning a monolingual embedding from monolingual data, which may not fully leverage the large-scale unlabeled data. To address this, in this paper, we introduce a novel semi-supervised learning method, namely Tensor-Wise Transformer Network (TW-TTN), for the cross-lingual knowledge graph inference task. TW-TTN exploits both the standard Transformer network as well as the tensor-wise transformer networks (TW-TWNs) to achieve the state-of-the-art results. Extensive experiments on the popular benchmark datasets show that our proposed method significantly outperforms state-of-the-art methods for the semi-supervised KG knowledge graph inference task.
   **Improving BERT Fine-tuning with Embedding Normalization**
   Recent years have witnessed a dramatic improvement of BERT fine-tuning performance on a wide range of benchmark tasks. However, these large-scale BERT fine-tuning efforts have remained largely unaware of the factors that contribute to its success. In this paper, we analyze the effects of various factors of the task on the fine-tuning performance and propose a methodology that helps identify the effective factors for achieving improved performance. Our analysis reveals that an effective BERT training dataset needs a large and highly variable vocabulary size, which is seldom reported in existing BERT fine-tuning works. We also show that it is necessary to boost the average BERT transferability by reusing pre-trained BERT models that have achieved state-of-the-art performance on a domain-specific task. We conduct extensive experiments on large-scale fine-tuning of Stanford Sentiment Treebank and benchmark datasets. We find that the factors that contribute to BERT fine-tuning performance are the most effective and comprehensive in enhancing its performance.
27. **Un systeme de lemmatisation pour les applications de TALN**
   Topic models have become an important component of scientific writing systems. These models are often considered as auxiliary tasks, allowing information to be extracted from scientific writing but without giving its authoring or stylistic characteristics. The paper presents a method to automatically enrich the extraction of genre information in a text written by scientists using a topic model.
   **Evaluating Voice Conversion-based Privacy Protection against Informed Attackers**
28. Voice Conversion-based Voice Security (VC-VS) is an emerging research topic to protect individual's privacy and security while speaking. Many attempts have been made to construct efficient speech based VC-VS model. The typical problem in creating effective VC-VS model is that the forensic experts do not have sufficient knowledge of the fingerprinting methods and text representations used by attackers to generate VOIDS, and there is no easy way to get such knowledge. This paper aims to evaluate the attack generality of voice conversion-based VOIDS model. In the process, we have analyzed the well-known VOIDS model, provided some insights on model capability and vulnerability. Finally, we have presented a proposal on how adversarial attack to obtain VOIDS text could be effectively addressed using open source ML based attacks.
   **Language Model-Driven Unsupervised Neural Machine Translation**
   Despite the recent advances in unsupervised neural machine translation (NMT), designing good unsupervised NMT models is still a challenging task. In this paper, we propose an end-to-end NMT model, the Hierarchical U-Net, to learn the structural features of the target language. To overcome the over-generalization issue in supervised NMT, we employ a group-wise attention mechanism to refine the original sentence composition. The proposed model achieves competitive translation quality compared to the state-of-the-art unsupervised NMT methods. Furthermore, we propose a straightforward yet effective way to apply the inherent parallelism between hierarchical U-Nets and recurrent neural network recurrent neural network, so that we can further boost the performance.
29. **Classical linear logic, cobordisms and categorial grammars**
   This paper presents the mechanisms by which the classical logics of [D&S, Neuberg, Pottier and Crawfort, NYL] -- namely notions of (best, highest, all, top) correspondences, equivalence classes, incomparable, top-down, and up-down structures -- have been extended to go beyond the specific levels of data structure and constructions as associated with these logics. We give a semantical analysis of the combinations of homomorphisms and matchings. Furthermore, we describe a context-free categorial grammars whose syntactic structure extends classical logics. These results, in combination with known extensions of the classical categorial grammar of A. Mercer, extend existing works in the categorial literature.
   **Zero-Shot Fine-Grained Style Transfer: Leveraging Distributed Continuous Style Representations to Transfer To Unseen Styles**
30. Fine-grained style transfer (FGST) aims to transfer artistic style in unstructured multi-media. The term multi-media means more than just the content, e.g. photos, videos, text or handwritten text. We formulate FGST as a multimodal problem, where the style representations encode the relations between content and style. Previous researches focus on transferring image style or text style, i.e., creating the target style from a content style image. However, generating a new style is more challenging since it requires modeling the relations between content and style and imposing them to the style translation from a source style image. To this end, we propose a generic FGST framework that is able to transfer between multiple styles and semantically justify the style transfer. The key to our framework is a novel loss function that optimizes the relationship between content and style. The proposed method can not only transfer between multiple styles and semantically justify the style transfer, but also generate a new style image of the target style by deep style embeddings. We demonstrate the effectiveness of our framework in comparisons with state-of-the-art FGST methods.
   **Unsupervised Annotation of Phenotypic Abnormalities via Semantic Latent Representations on Electronic Health Records**
   Automatic discovery of impairment biomarkers is an important step towards development of biomarkers for detecting specific diseases or abnormalities. Existing approaches typically use unsupervised data mining approaches to extract specific features from unstructured electronic health records (EHR). When these unstructured features are in a different language, it is challenging to integrate them in supervised methods. In this paper, we propose a Semantic Latent Representation-based Classification method to augment non-structured EHR features to make use of machine translation between multiple languages. We demonstrate that our method successfully identifies impairment biomarkers for both healthy and impaired individuals in a large-scale EHR corpus.
31. **Emotional Voice Conversion using Multitask Learning with Text-to-speech**
   To convert emotional voice to speech effectively, training a good speech recognizer from emotional speech is crucial. The task is formulated as an emotion-modulated sequence of text-to-speech (TTS) conversion. The model is an unsupervised multi-task network that performs a text-to-speech conversion from a raw speech signal to a neutral speech. We build a TTS model in which emotion sequence is modelled by a mixture of ReLU and batch normalization layers and the neutral speech is modelled by recurrent neural networks and an attention mechanism. The model is trained on a large quantity of unlabelled emotional speech, and it improves the speech quality with an improvement in word error rate (WER) of 7.14% on the newly-completed test set of Emotional Vocal Conversion Challenge. The model is also evaluated with a synthetic emotional TTS benchmark, and it outperforms the baseline that does not use acoustic information to improve the speech quality.
   **Biomedical Evidence Generation Engine**
32. This paper describes the Biomedical Evidence Generation Engine (BEG), a software platform for the automated generation of qualitative biomedical reports. BEG can automate the text generation of documents (e.g. recommendations, reports) by automatically utilizing text from existing content-based search engines, describing documents by linking them to high-quality search queries, and generating summaries based on these retrieved results. BEG is a modular system that can be built upon the implementations of various machine learning components and can incorporate either pre-trained word embeddings or supervised machine learning models for report generation. BEG is particularly suited for the extraction of any type of clinical evidence from clinical literature. BEG is the first application of machine learning to text generation and to the automated analysis of medical documents. Evaluation of BEG's results and performance on standard datasets demonstrate the effectiveness of BEG as an evidence-generation engine.
   **DialogAct2Vec: Towards End-to-End Dialogue Agent by Multi-Task Representation Learning**
   In this work, we present DialogAct2Vec, an end-to-end dialogue agent trained on both automatic and human evaluation metrics by multi-task representation learning. We model dialog act annotations, knowledge bases, and knowledge graphs as a set of bags. The dialog acts for each bag are correlated with its other bags and hence, we explicitly model bags. We then learn a bag-of-words (BoW) representation for each bag and then use the representation in end-to-end training. To measure the performance, we use the summarization module of the standard BERT model. We achieve the new state-of-the-art results on the task of automatic response generation. In addition, we report the state-of-the-art performance on the task of human-human conversation. In all cases, our proposed model outperforms previous methods by significant margins.
33. **A unified sequence-to-sequence front-end model for Mandarin text-to-speech synthesis**
   As natural language generation (NLG) systems become more complex, their computational burden grows very rapidly as well. In this paper, we present a hybrid approach for sequential text-to-speech synthesis, in which a seq2seq front-end model performs the generation of the utterances, and a base-model performs a speech recognition (SBR) on the generated utterances. We propose a maximum-entropy deep adversarial learning (MEGAL) model, and show that it can be modified to synthesize any speech synthesized by a vanilla SBR model. Evaluations on two Mandarin speech corpora show that our model generates clean speech with a high success rate, and it is competitive with the vanilla SBR model.
   **Text classification with pixel embedding**
34. We propose a method for learning a general latent representation of text images in an unsupervised fashion. Textual embeddings map input images to embedding spaces that can be quantized to support efficient downstream recognition tasks, such as text classification. The inherent correlation between an embedding space and a test vector produces meaningful representations, allowing the whole network to be trained on as few or as many training samples as desired. We show that our method is able to use as few as 4 embeddings per pixel, resulting in a latent vector representation that is comparable to other methods on a number of text classification benchmarks.
   **TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection**
   Modern neural network models perform great on answer sentence selection tasks, especially when trained on a large corpus and carefully fine-tuned. However, the success of such models can be attributed to the expressiveness of the encoder and decoder branches that compose the deep neural network. We have recently introduced the Ensemble Convolutional Neural Network, an ensemble of CNNs pre-trained on large corpora and fine-tuned for a new task. In this work, we consider a slightly more advanced pre-trained pre-trained Transformer model, and compare our proposed ensemble model TANDA with three state-of-the-art models. For two data sets, the ensemble model TANDA is clearly better than the state-of-the-art models. For the third data set, TANDA outperforms the state-of-the-art models in the experiments we carried out. In some of the experiments, the ensemble model TANDA showed an increase in performance compared to the state-of-the-art models.
35. **Leveraging Dependency Forest for Neural Medical Relation Extraction**
   Neural relation extraction methods usually consider distant supervision, where each relation triplet is labeled by at least one relation triplet triplet, but this hardly addresses various medical relatedness. To address this problem, we consider the recently proposed distant dependency tree and propose two methods, which are the first zero-shot relation extraction methods which use the dependency tree based relation triplets. The first method is named as Joint Entity Biases-guided Tree, and the second one is named as Joint Entity Embeddings based Tree. Both methods are jointly optimized using entity-specific soft-alignments. We first conduct experiments on MedAL, a large-scale relation extraction dataset and two kinds of existing relation extractors. Experimental results show that Joint Entity Biases-guided Tree achieves the state-of-the-art performance, using less than half of the relations in MedAL. We further discuss how the word embedding for dependency trees can be used for relation extraction, and demonstrate how a structured reasoning module can be introduced to the framework to enhance the relation extraction results.
   **A hybrid text normalization system using multi-head self-attention for mandarin**
36. Automatic text normalization has been mostly developed for English and Chinese languages. Manual Chinese text normalization is considered a hard task, which usually involves difficulties such as different font styles and the complicated script system. In this paper, we build a hybrid Chinese text normalization system for mandarin. To perform a complete text normalization, we divide a sentence into several heads and apply multi-head self-attention based language model for each head. Experimental results show that the proposed method achieves a total error rate of 0.004 on tested mandarin sentences. We also release a text normalization dataset of mandarin samples which we use for training of our model.
   **Attending to Entities for Better Text Understanding**
   The language of Wikipedia aims at processing and interpreting texts and it is the most widely used information source, yet understanding has achieved remarkable success in recent years. The recent work has focused on the identification of entities and the collection of relations in Wikipedia. This article is about the alternative entity focus within a reasoning framework known as top-down parser, developed by Mamdani et al. The discourse relation between an entity and its description has been identified as the main object of attention in Wikipedia texts. A particular focus point is the discourse relation between a named entity and its description. This article focuses on some shortcomings in the previous works that have focused on the identification of the discourse relation between entities. The motivation for the work is the publication of a novel RDF graph parser for clinical texts that uses a lookahead based greedy aggregation algorithm and thereby highlights the attention of an entity within the article. An experiment conducted on two data sets extracted from the Freebase PubMed repository shows the usefulness of the algorithm.
37. **Understanding BERT performance in propaganda analysis**
   This paper analyzes the performance of BERT (Bidirectional Encoder Representations from Transformers) in various binary classification tasks in tweets related to Russian influence campaigns, namely, tweets containing hate, suicidal, sarcasm, drug, Wikileaks, etc. We compare BERT to the Transformer, the state-of-the-art multilingual model in text classification. We show that the BERT model is not only significantly better than Transformer for these tasks, but also significantly better than DeepWalk, another state-of-the-art multilingual model for sentiment classification.
   **Long-span language modeling for speech recognition**
38. We present a method for long-span language modeling for speech recognition that is based on deep learning. While long short-term memory networks have shown great promise for language modeling, they have been shown to have significant room for improvement. One of the challenges of this method is that the training data is split across time, and hence the model's modeling capacity is compromised by overfitting in terms of training time. We alleviate this problem by allowing the model to work in an iterative manner, starting from a given data segment and expanding the model over time. We do this by treating each time step as a one-shot example, and then using backpropagation to update the model parameters. We also perform non-iterative model regularization at training time to prevent model overfitting and to prevent the model from drifting toward an uninformative state. We evaluate our approach on the Switchboard Speech Recognition Evaluation dataset, which consists of paired text transcriptions for thousands of hours of phone conversations. On this dataset, our model significantly outperforms the state of the art, reaching competitive results compared to a full scale, trained baseline.
   **A Syntax-aware Multi-task Learning Framework for Chinese Semantic Role Labeling**
   Previous work on end-to-end Chinese semantic role labeling (CSPL) has largely focused on leveraging language and syntax information to assign roles. In this paper, we propose a multi-task learning framework that models multi-task and attention information jointly to better leverage both tasks. Specifically, we devise a Syntax-aware Attention Network (SAAN) to leverage syntactic parses of labeled sentences to predict role scores for labeled sentences. A multi-task regularizer is then designed to encourage co-training of each of these two subtasks. To evaluate our model, we run the Model Interpretation Competition, and our best model achieves the 4th out of 568 submissions on the test data.
39. **NegBERT: A Transfer Learning Approach for Negation Detection and Scope Resolution**
   Automatic negation detection is a critical step in negation resolution. However, the existing approaches either (1) rely on the assumption that all possible assignments for a given sentence are true, and therefore, all negation cases will be detected in training, which is unrealistic; or (2) assume that a manually-generated list of all possible assignments for a sentence is possible, however, this list is not complete. In this paper, we propose NegBERT, a transfer learning approach which uses a neural network model to simultaneously detect negation and scope resolution. It enables us to gain a better understanding of how negation cases are compiled, rather than by limiting the scope of cases manually-generated by an analyst. Experiments conducted on a standard negation dataset show that NegBERT can detect an overwhelming number of negative sentences without missing any positives. Further, NegBERT achieves state-of-the-art results in two subtasks (Negation Extraction and Negation Handling) on a new dataset (Neutrals in English) with limited training samples.
   **A Survey on Why-Type Question Answering Systems**
40. When humans use a language to communicate with others, they often ask questions in order to gain insights and engage the other person. For example, if we talk about finding a lost dog or the home of a lost pet, we ask questions that are based on an explicit understanding of the situation. In this paper, we focus on why-type question answering, that is, answering questions that are generated by a question-answering algorithm from a given text. The paper also introduces, evaluates and contrasts how various approaches to building a question answering system compare to each other. To appear in Theory and Practice of Logic Programming (TPLP).
   **Morphological Segmentation Inside-Out**
   Semantic segmentation inside-out (LESO) is an old problem that has gained traction due to its applicability in various scenarios. Recent studies have shown that two-stage LESO is suboptimal, due to its tight constraints during training. In this paper, we propose to overcome these issues by proposing a model which is able to utilize the complementary information from adjacent superpixels to obtain accurate boundaries. Our model achieves a result of 73.0\% at 19.4 SNR on PASCAL VOC 2012 test data with a DenseNet-121 as the backbone network, which outperforms existing methods for boundary detection.
41. **'Warriors of the Word' -- Deciphering Lyrical Topics in Music and Their Connection to Audio Feature Dimensions Based on a Corpus of Over 100,000 Metal Songs**
   This paper presents an unsupervised model for discovering textual features of lyrics and music using large-scale audio files. Audio feature dimensions which indicate the semantic similarity between music and words are extracted and then used to build a bilingual word embedding. Then the top-k relational songs and their top-k musical features are obtained. When examined on a linear discriminant analysis, the feature dimensions are associated to two or more qualitative features. Furthermore, these results are able to detect a number of linguistic properties which are not detected by prior cross-correlation methods. Overall, this study provides new insights and discoveries, which also form the basis of further studies in music transcription.
   **Character-based NMT with Transformer**
42. We introduce novel Character-based Transformer, a kind of Transformer model with character-based encoders, which achieve competitive NMT performance with Transformer on both Chinese-to-English and English-to-German tasks. In contrast to previous character-based Transformer, our model (with an attention gate) learns semantic encodings of character strings directly from raw characters in the input stream, without relying on beam search (as Transformer). To the best of our knowledge, this is the first work to propose such character-based Transformer model in NMT. Experimental results on WMT14 English-German and WMT16 English-French translation tasks show that our model outperforms other character-based Transformer models in translation quality.
   **Can a Gorilla Ride a Camel? Learning Semantic Plausibility from Text**
   The ability to conjure plausible combinations of symbolic elements is fundamental for producing meaning from language. We present a new approach for automatically generating linguistic outputs that are made to plausibly combine over various semantic levels: sentences, words, phrases and sentences over words. The underlying semantic approach to the generation is given through a novel neural architecture that we term Semantic Plausibility Neural Network (SPMN). SpMN is trained on 2.2 million examples of conceptual combinations that are made to plausibly combine elements of different linguistic forms. After being trained, SPMN is able to make plausible outputs in high degrees of syntactical and semantic consistency, generating a full lexicon of conceptual combinations that span the entire span of grammatical form, in a large fraction of the time required by previous systems.
43. **Word-level Lexical Normalisation using Context-Dependent Embeddings**
   This paper proposes a novel approach for learning contextual embeddings that automatically converts the meaning of words to words in different contexts. We use this contextual embedding for learning word-level lexical normalisation that attempts to reduce the incidence of similar words in different contexts. We train a convolutional neural network on different context corpora and show that the contextual embeddings outperform the word embeddings in the same context.
   **Unsupervised Pre-training for Natural Language Generation: A Literature Review**
44. While recent work in image and audio generation has enjoyed major breakthroughs, we lack a comparable proof-of-concept to get us excited about generative modeling in general. This is partially due to the extremely complex design of many state-of-the-art models, their lack of a large and diverse collection of examples to train on, and the often prohibitive cost of annotating them with ground truth labels. In this work, we present a comprehensive literature review of recent results on unsupervised pre-training in image and audio generative modeling, focusing on their strengths, weaknesses, and impact on the research community.
   **Mark my Word: A Sequence-to-Sequence Approach to Definition Modeling**
   The EMNLP generation system needs to generate effective and fluent natural language. However, efficient and scalable word generation needs a variety of information during the generation process. Existing generation systems usually generate sentences with only word and character probabilities. In this paper, we explore the possibility of learning a more robust model that is able to jointly model word and character probabilities. We propose a dynamic definition sequence model to capture the complex semantics of words and characters in sentences. Our approach to learning word and character probabilities focuses on sequence modeling and can capture long-term dependencies among words and characters. We compare our dynamic definition sequence model to a pair of encoder-decoder models on English-to-German translation tasks. Experiments demonstrate that our model outperforms the previous methods in terms of grammatical correctness and fluency.
45. **Prevalence of code mixing in semi-formal patient communication in low resource languages of South Africa**
   The rapid growth of data in healthcare systems has led to a requirement for increased interaction between clinicians and patients. Code is an essential means for healthcare systems to efficiently inform and engage patients. Code mixing, also known as unsupervised learning and which involves one or more discrete codes, has been an issue in healthcare. In this paper, we explore the use of machine learning techniques to detect code mixing patterns in patient conversations in a South African environment. Our results, which are based on an adaptation of Hidden Markov Model and Random Forest, show that code mixing is the most commonly occurring type of unsupervised learning patterns and that the detection rates of unsupervised pattern occurrences are considerably lower than those of supervised pattern occurrences.
   **KEPLER: A Unified Model for Knowledge Embedding and Pre-trained Language Representation**
46. Knowledge representation and embedding of entities and their relationships are two critical and interrelated aspects of many NLP tasks. Both of them have traditionally relied on a textual representation of a document (e.g., a vocabulary, POS tag, etc.). Most recent work, however, has tried to address these two aspects jointly. In this paper, we propose a unified representation model for knowledge embedding and language representation that tackles both aspects with attention-based multi-task learning. By doing so, our approach achieves state-of-the-art performance in knowledge representation, in terms of error rate and word error rate on two downstream tasks. Moreover, its performance is competitive with the state-of-the-art methods, while requiring fewer parameters and being interpretable. Our implementation and accompanying models will be made publicly available.
   **Towards Supervised Extractive Text Summarization via RNN-based Sequence Classification**
   Extractive text summarization is of great importance for research in natural language processing. However, the accuracy of extractive summarization cannot be guaranteed due to various factors, such as the large amount of training data used in machine learning research and the inherent sparsity in the existing literature. In this paper, we propose a RNN-based system for extractive text summarization. The proposed RNN-based model considers the structural relations between a sentence and the salient sentences in a document, and then learns a sequence of words to summarize the text. To the best of our knowledge, this is the first RNN-based system that successfully predicts the summarization of full-text documents. Empirical results show that the proposed method significantly outperforms the existing extractive summarization methods.
47. **Adapting and evaluating a deep learning language model for clinical why-question answering**
   We describe an adaptation framework and evaluation criteria to perform automatic adaptation of a pre-trained neural language model for clinical why-question answering. It is expected that this method will enable a variety of machine learning based question answering tasks that may be difficult for human to perform.
   **What do you mean, BERT? Assessing BERT as a Distributional Semantics Model**
48. The BERT (Bidirectional Encoder Representations from Transformers) model recently demonstrated state-of-the-art performance in Natural Language Understanding. Despite its success, the fact that it is an encoder-only model has led some to argue that it lacks a rich, distributional interpretation. A deeper investigation into the representations of BERT, using a neural distributional semantics model, confirms that BERT does indeed learn a distributional representation.
   **FAQ-based Question Answering via Knowledge Anchors**
   We present a new task of investigating the grounding of questions in text through their use of knowledge anchors. We define the problem as answering a question by returning the answer text in the form of a Knowledge-Aware Query (KABQ) document, where the text is given by a deep neural network trained for example with answer extractors. Existing knowledge-based question answering (KBOQ) benchmarks have established accuracy of 90% or more. Such benchmarking, however, is entirely based on topic modeling on sentence-level co-occurrence data (i.e., WordNet and Wikidata). In contrast, our KABQ task aims to find the right anchor text for a given question without any topic modeling. To achieve this goal, we propose to exploit a knowledge base and a flexible technique for embedding the document to the word embedding space of the knowledge base. Our method generates anchor vectors for the question along with its entity annotations and various query entities. For each entity, we apply an entity type classification method to identify whether the entity is included in the answer vector. By applying these vectors to a corpus of query entities and their textual explanations, we further predict the anchor text for a specific entity. Thus we leverage the location of the word embeddings to refine the answer embeddings for the question. Experimental results on a set of open-domain KBOQ datasets demonstrate that our technique outperforms the current state-of-the-art method.
49. **Exploiting Token and Path-based Representations of Code for Identifying Security-Relevant Commits**
   The pace of software development has produced a large number of software products and modules, known as software delivery cycle frameworks (SDBFs). The resulting release chains are large, complex, and repetitive, making the identification of security-relevant SDBFs particularly challenging. Existing solutions for the identification of SDBFs rely heavily on pre-processing by tokenizing or copying files from source to target or vice versa, using static analysis methods. In contrast, our new approach is to analyze files independently of the SDBFs they were delivered from, on the basis of patterns they share in common with other code. We apply a set of rules to the resulting sets of tokenized and copied files to produce a set of paths, such that these paths can be effectively used to identify SDBFs. Our approach was evaluated with a set of security-relevant SDBFs extracted from a large set of smart card operating system (OS) binaries, each of which contains a large number of commits from different patches in a project. We compared our approach against a set of baseline static analysis methods and applied it to a large set of static analysis reports. Our results demonstrate the effectiveness of the proposed approach and provide information about the contexts in which the developers are likely to check for security vulnerabilities in their code.
   **Understanding Troll Writing as a Linguistic Phenomenon**
50. There are many kinds of toxic language that can easily be perceived as deliberate trolling, but the situation is complicated by the fact that some words (e.g., bullying) are ubiquitous and others (e.g., brain-bang) are not. We need tools to understand how these words are being used and whether they are contributing to toxic communities. In this work, we construct a lexical space called Toxic Talk Meme Dataset (TTD), which contains a total of 5083 tweets from over 7,500 users that were labeled as toxic or non-toxic according to the LAMBADA model. By training neural language models on these tweets, we can cluster users into toxic and non-toxic communities. Using the LAMBADA data, we can detect clusters of toxic users at the tweet level that can be used to understand the form of trolling (e.g., bullying vs. brain-bang). The first feature of TTD is the specific amount of abuse in tweets. Although we see a large amount of abuse in the tweet data, we find that our task is a challenging problem because a single tweet could contain a mixture of different types of abuse (e.g., bullying and brain-bang) with different directions of abuse. This mixed nature of the data poses a real challenge, which we use to improve the feature learning and further interpret the features. We show that these features are useful to identify the communities and users that are targeted by trolling.
   **Question-Conditioned Counterfactual Image Generation for VQA**
   Image captioning is a simple yet promising paradigm to tackle challenging visual-language pair learning tasks. In recent years, more and more methods have been introduced for solving this task and the most representative model is the memory-augmented encoder-decoder framework, but these methods are incapable of generating realistic counterfactual explanations. In this paper, we propose a new image captioning model, namely VQA-MC, by focusing on counterfactual image generation. Compared with the standard memory-augmented encoder-decoder model, the proposed model naturally generates counterfactual images for which the explanation-conditioned objective function is a stronger form of reasoning. In addition, VQA-MC is designed based on efficient model compression and deep learning strategies. Furthermore, we introduce a novel self-attention network to refine the generated counterfactual images. Extensive experiments on the standard VQA dataset show that VQA-MC not only outperforms existing state-of-the-art models by a large margin, but also demonstrates better interpretability and robustness on image categories with salient information.
51. **Iterative Answer Prediction with Pointer-Augmented Multimodal Transformers for TextVQA**
   In this work we present an extension of Answer Prediction based on the use of multimodal information in the training. We analyze the success of our model and suggest new practical approaches to make the use of multimodal information in training more effective.
   **Ethanos: Lightweight Bootstrapping for Ethereum**
52. We present a lightweight virtual machine to perform simple, computationally-efficient tokenization of Ethereum's blockchain. This bootstrapping makes it possible to execute a Turing-complete computation on the network with a minimal amount of resources. In contrast to traditional virtualization approaches that generate a massive amount of partitions (data partitions), our bootstrapping reduces the total size of the state space by only 1-2% of the original Ethereum blockchain. We test our bootstrapping on two Ethereum models, the MetaMask and the Dapplet, with an average reduction of up to 88% and 45% partitions respectively.
   **Contextual Recurrent Units for Cloze-style Reading Comprehension**
   We propose a new decoding architecture for Cloze-style reading comprehension. The proposed model exploits contextual recurrent units to capture temporally variable dependencies among the structured outputs. Compared to competing approaches, the proposed architecture is both significantly faster and more accurate.
53. **Training a code-switching language model with monolingual data**
   We study the problem of the training of a language model with monolingual training data. In contrast to previous work, where only one language can be used, this paper considers the use of two languages. This enables us to exploit the strengths of each language, particularly the importance of modeling both gender and semantics. We propose a data augmentation approach to handle the imbalanced data and a new objective to encourage model adaptation. We conduct an in-depth analysis of the resulting language model, which demonstrates its effectiveness for both text classification and question answering.
   **Enhanced Meta-Learning for Cross-lingual Named Entity Recognition with Minimal Resources**
54. Cross-lingual Named Entity Recognition (CLEFR) is a challenging task because named entities often contain contextual information not shared between languages. Moreover, this task is often conducted with limited or no labeled data in a new language, raising the need for training and memory resources. To this end, we propose a novel meta-learning framework for CLEFR. The framework is designed to learn the semantic representations and the models of word and character embedding vectors simultaneously. For example, the entity representation, i.e., the representations of its names, is learnt to be similar to the target language's named entity dictionary. In addition, the shared representation between languages is also learned to be similar, i.e., the shared embeddings are obtained by concatenating the entities that belong to different languages. To ensure that the learning and model parameters are not over-trained, we further propose a two-step approach. In the first step, we propose a min-target-to-target learning strategy to maximize the learning of target language specific representations. In the second step, we present a new short-term memory algorithm to acquire and update the model parameters over time. Experiments are conducted on multiple languages, namely German, English and French, and show that our proposed CLEFR framework is very effective. It outperforms the state-of-the-art approaches and achieves competitive results on multiple CLEFR datasets.
   **The Eighth Dialog System Technology Challenge**
   This paper describes the Eighth Dialog System Technology Challenge (DSTC8), a competition organized at the beginning of 2018 by the University of California, Riverside to investigate different abstract-domain approaches to resolve undecidability questions in industrial-sized dialog systems. Each abstract-domain system is evaluated on a public dataset consisting of 5500 dialogs collected from different classes of problem domains. We assess the most suitable data pre-processing techniques to improve the performance of the system in decidable domains. The six abstract-domain systems participated in the competition and the evaluation metrics consist of various measures from classification to human evaluation. The corpus of the competition has been made publicly available.
55. **Utterance-to-Utterance Interactive Matching Network for Multi-Turn Response Selection in Retrieval-Based Chatbots**
   In this paper, we present the task of answering questions from a chatbot. The main idea of the task is to find the best answer that best captures the user intent. Previous work has focused on achieving one-to-one learning between the system and the user, which is sub-optimal in both time and scalability. On the other hand, we argue that such a task should also be useful to a human. Therefore, we propose a multi-turn interactive matching network (MTIM), which takes into account the user's past responses and the system's own past responses. In addition, the proposed model is easily trainable, easy to extend, and lightweight. In the experiments on a real-world retrieval-based chatbot dataset, the proposed method significantly outperforms the baseline system, achieving up to 27% improvement and 53% improvement over the previous state-of-the-art model in F1 scores. The code will be made publicly available at: https://github.com/iia-miu/MTIM
   **Robust Reading Comprehension with Linguistic Constraints via Posterior Regularization**
56. Automatic text comprehension is a core task in natural language understanding. Neural models have achieved impressive results on several benchmark datasets. However, in real-world applications, such as restaurant reviews, full-text information is often unavailable. A textual description of an item is often linked to a single view, for example a paragraph from a grocery store checkout. We show that enforcing linguistic constraints on these views leads to significantly improved solutions, compared to traditional heuristic strategies such as supervision. More specifically, we show that models can be trained to identify salient descriptions in large video scenes and then be fed the descriptions as inputs for language-based systems. The resulting text-based models have higher BLEU scores than previous neural approaches and achieve a 10% relative improvement on scene comprehension tasks in the dining room context.
   **Using natural language processing to extract health-related causality from Twitter messages**
   We investigate how natural language processing can be used to extract health-related causal causal mentions (CDRs) from biomedical-related tweets, and we propose to apply the Mearns automatization (MAG) method. MAG is a generalization of an earlier method used to extract FDRs from handwritten biomedical-related journal articles and conferences. The positive and negative variables we consider are those identified as being causal, either by textual evidence or an existing machine learning model. Our findings indicate that utilizing the Mearns automatization to extract CDRs outperforms MAG by an average of approximately 3.1% in terms of accuracy.
57. **Independent and automatic evaluation of acoustic-to-articulatory inversion models**
   We demonstrate the feasibility and practicality of acquiring ground truth acoustic-to-articulatory (A2A) speech from clinical practice to improve monotonic speech synthesis systems. We refer to our new data collection effort as "DRE - DNNs for Dissambulation". In the first part of our investigation, we introduce an evaluation protocol to collect ground truth from speech therapists, that relies on development and validation of synthetic A2A mappings, both aurally and through facial expressions. We study factors related to success in establishing the sound mapping between the real and synthesized articulatory data. In the second part of our investigation, we implement a direct A2A-to-ground truth (DEA-GTA) model for monotonic speech synthesis, to evaluate the performance of A2A-to-articulatory mapping with state-of-the-art autoregressive sequence models. The derived DI-GTA model shows improvements on speech quality, producing more natural speech and maintaining phonemes. However, it also shows an increased modeling burden with respect to the original DEA-GTA model, which is an efficient method for modeling long-term correlation with respect to utterance level.
   **Bootstrapping NLU Models with Multi-task Learning**
58. Neural machine translation (NMT) is one of the most prominent research fields in the Natural Language Processing field. Most existing models are based on the encoder-decoder framework, which has been widely used to learn an NMT model. Most of the neural machine translation methods also use multi-task learning for translation model training. However, the number of parameters in current multi-task neural machine translation models is too large to be easily transferred over other languages to learn a new language, which severely hampers the translation quality. In this paper, we propose a new framework to transfer multi-task learning models into different NMT architectures. In the proposed model, the source sentence is first aligned with a ground truth alignment, and then a distance based alignment method is utilized to align a source sentence with a target sentence and the source model is also trained with the aligned target sentence. Experimental results show that our proposed model has comparable performance with the models trained by multi-task learning.
   **Experiments in Detecting Persuasion Techniques in the News**
   We focus on persuasion techniques in the news to provide evidence for different notions of effectiveness. We do this by applying techniques that have been shown to be effective for influence campaigns in the text-only domain. We then analyze the sources of persuasion campaign persuasion techniques to investigate which sources carry the most influence. We compute influence based on the perceived similarity to the target (on a two-level scale) as compared to the comparison to other documents. We argue that different sources have different influence strengths, and often for different domains. We find that channel diversity has the greatest influence. We provide strong evidence for the hypothesis that channel diversity is a more important factor for influence than document similarity. Finally, we investigate whether certain types of persuasion techniques influence more than others and whether we can take advantage of this knowledge to develop more effective persuasion campaigns. We show that effectiveness on persuasion campaigns can be predicted with a 0.7% improvement in relative accuracy over using all persuasion techniques.
59. **Evaluating robustness of language models for chief complaint extraction from patient-generated text**
   Patient-generated text is widely used in health-related applications and has become a new data source that can be used to analyze the behavior of hospitals. However, detecting chief complaints (CPDs) in this data is challenging. Existing models for CPD detection fall into the usual classification paradigm. This is not a desirable approach for medical texts, since there is no "base classifier" such that a text can be correctly classified if it was classified correctly on any other data source. In this paper, we propose to incorporate a system-level regression technique based on principal component analysis (PCA) into an existing CPD detection model, in order to enable it to handle noisy data such as patient-generated text. Our experiments on a medical text dataset from the University of Kentucky show that the new model (RNN-PCA) can outperform previous models that do not use PCA for classification. In addition, we compare two simple methods to mitigate the effects of noisy data. Our results indicate that they are not effective.
   **An Accuracy-Enhanced Stemming Algorithm for Arabic Information Retrieval**
60. Information retrieval (IR) has seen remarkable growth over the last decade. To achieve this, researchers have adopted various methods to improve the performance of IR systems. This paper aims to overcome the deficiency of existing systems by utilizing information from relevant websites and the capabilities of statistical machine learning (SML). We propose a novel, accurate, and fast stemming algorithm which utilizes two sets of features to enhance the overall performance of a retrieval system. Experimental results using several publicly available IR datasets show that, our proposed approach achieves superior performance compared to other system performance metrics.
   **Towards Automated Sexual Violence Report Tracking**
   Given the prevalence and severity of sexual violence and its effects on women and children around the world, the question of how to capture and analyze a comprehensive dataset for the analysis of such crimes is of high relevance. In this paper, we propose a data annotation scheme that enables us to collect a large corpus of full text description of sexual violence and create an online framework for the systematic analysis of this crime. The training of the system is driven by the manual labelling of textual descriptions. With a simple and language-independent feature extraction module, the resulting system achieves 90% accuracy of identifying reports of sexual violence in the UK annual official statistics of crime in 2011-2012.
61. **Contribution au Niveau de l'Approche Indirecte \`a Base de Transfert dans la Traduction Automatique**
   This paper introduces a fundamental paradigm shift from the direct to the indirect speech generation, at a time when both are heavily focused on. We present a corpus for indirect speech generation which contains over 10K sentences and compares them to the direct ones in six different evaluation datasets. Since there is no direct parallel corpus, we use one from a real-world text corpus and one based on a sentiment analysis dataset. We aim to bridge the gap between natural language and spoken language by developing a transfer-learning based approach for these noisy indirect tasks. We show that our approach leads to significant improvement in the current direct indirect tasks. We also provide an in-depth analysis of our models' efficacy for the two tasks. Finally, we report experimental results from the direct indirect tasks with different architectures and models trained on natural language and sentiment data.
   **Quick and (not so) Dirty: Unsupervised Selection of Justification Sentences for Multi-hop Question Answering**
62. Multi-hop Question Answering (QA) has gained attention recently due to its significant real world applicability in real world domains. Most previous methods that search for justification sentences for multi-hop QA require manually constructing justification sentences and searching them for justification candidates. In this paper, we propose a novel algorithm, AIS-QA, to automatically find justification sentences for multi-hop QA with an end-to-end learning strategy. Unlike previous methods that use manually constructed justification sentences for QA, AIS-QA is able to learn justification sentences automatically from multi-hop QA datasets with just few labeled data. We employ the LSTM and GRU recurrent neural networks to learn the multi-hop QA evaluation metric by using the labeled data only. Additionally, we implement a short handcrafted justification sentence to identify justification candidates efficiently by searching for them in the text space of QA datasets. Experimental results on multi-hop QA datasets demonstrate that AIS-QA outperforms the state-of-the-art QA methods.
   **Multi-Zone Unit for Recurrent Neural Networks**
   In recurrent neural networks (RNNs), a single recurrent neuron is trained to learn various components of sequences. However, it is difficult for RNNs to learn sub-sequences over time that are arbitrary in nature. A way to handle arbitrary sub-sequences would be by choosing one of the components as the start or end of a chain. In this paper, we propose a multi-zone unit for RNNs. Multi-zone units measure the region of a time sequence in which the neurons take different actions. We describe the computational complexity of the proposed unit, and show that the proposed unit satisfies some properties and facilitates the training of RNNs. Experimental results on two datasets show that the proposed unit performs better than single-zone units and much better than traditional zone units. We also show that the proposed unit can capture very fine temporal structures in sequences, such as individual events and characters in a sequence, and better than the layer units in a conventional RNN.
63. **Mining Unfollow Behavior in Large-Scale Online Social Networks via Spatial-Temporal Interaction**
   This paper investigates the following question: how can one discover that an online social network (OSN) user follows a subset of topics by observing their interaction patterns? If the goal is to reach a global understanding of user behavior patterns, one could get a global clustering by clustering a large set of users. However, such a framework leads to an unreachable goal because the number of topics or users in the OSN is not necessarily known in advance. In this paper, we propose a novel deep learning method to identify user sub-groups which are small in size but follow a subset of topics. In our model, the topics are represented as the nodes of an interconnected graph with communities that are formed by the interaction patterns of the sub-groups. An interaction graph is proposed by inducing community structure on the nodes of the graph, and each community learns a specific type of node type to represent the interactions in the graph. After learning the node types, we decompose the input interaction graph into a series of "layer-wise" functions that encode the different relations between nodes. To select the nodes which are "presumed" to follow a subset of topics, we employ the alternating direction method of multipliers method and select the "kernel" nodes from the selected layer-wise functions to represent the interaction patterns of the nodes. Our algorithm then estimates the subsets of topics that the users follow using a hybrid graphical model. Moreover, the algorithm uses the observed interactions to update the surface-topic distribution of the graphs. Experimental results on two real-world OSN datasets show the superiority of the proposed method over existing methods that predict the number of topics a user follows or is oblivious to.
   **Rumor Detection on Social Media: Datasets, Methods and Opportunities**
64. Rumor detection in social media has attracted much attention in the recent years. Due to its ability to discover highly relevant and context-aware rumours, it is of critical importance to take advantage of the efforts to create reliable methods to detect rumours from social media. To this end, this paper provides the first comparative analysis of nine different rumour detection methods on four popular social media datasets. These datasets, including Twitter, Facebook, Reddit and Flickr, are collected from real-world sources, with outliers considered. All datasets contain texts from four languages: English, Chinese, Hindi and Tamil. The methods are categorized into four groups: traditional, non-traditional, non-unsupervised, and supervised. Traditional methods are algorithms based on text mining and are manually designed; the rest are machine learning based. Although our studies focus on rumour detection on social media, the methods and types of data are applicable to other related problems such as sentiment analysis. We also compare various decision criteria and visualize different features to understand the effectiveness of the different methods. We provide comprehensive examples for different types of data and discuss their pros and cons. We provide a large set of hyperlinks to further explore the code and data on GitHub.
   **DualVD: An Adaptive Dual Encoding Model for Deep Visual Understanding in Visual Dialogue**
   In this paper, we propose a novel architecture, DualVD, that explicitly encodes the visual and textual modalities into two encoders. One encoder provides both visual and textual modalities to the decoder, which generates a low dimensional vector representation of the dialogue context. The second encoder independently learns a representation that shares visual and textual features with the first encoder. We propose a dual encoder model that exploits dual similarity, a semi-supervised, deep architecture, to jointly train the decoder and encoders. The proposed dual encoder model, which we refer to as DualVD, is built end-to-end on top of the FastText model. Extensive experiments on two dialog datasets demonstrate that our DualVD model outperforms existing methods.
65. **Universal and non-universal text statistics: Clustering coefficient for language identification**
   We discuss two ways of estimating language co-occurrence using the available data. The first is based on specific groups of data but requires distributional assumptions about human language. The second is universal data that avoids distributional assumptions and approaches probability distributions of words by using the statistical properties of words (scores and dissimilarities) to build a low dimensional feature space. The results we obtain for text are, first, on a par with recently obtained results for phonetic similarity, and second, higher in quality for the word vector representation than those obtained for deep learning models of language. They also suggest that the best standard for estimating word co-occurrence is uncertainty sampling.
   **Short Text Language Identification for Under Resourced Languages**
66. Social media in low-resource settings, like Twitter, is an important medium for people in developing countries to communicate with their neighbours in these languages and more importantly, to promote development through shared learning and innovation. With this in mind, the idea of developing a text language identification (TLI) system for these languages was recently introduced. The premise of the proposed approach is to develop a learning algorithm for the task of text language identification by looking at a small set of tweets, such as tweets in English, in these languages. We propose a specific solution for this problem as compared to other approaches, which generally look at the entire corpus and use the Universal Dependencies (UD) dependency tree as an intermediate representation. As a result, we are able to better match English tweets to their corresponding UD sentences and can further combine this result with a phrase or two from the UD tree to build up the current model. In our experiments, we use English Twitter for our target task and find the proposed model outperforms other state-of-the-art methods by a significant margin. We also show that the model generalises across languages by adapting to and using UD sentence trees not only from different languages but also from different languages that have not been used for training.
   **Deep and Dense Sarcasm Detection**
   Automated sarcasm detection is a complex task which requires sentiment analysis and emotion models. These are computationally expensive and subjective processes which greatly hinders their deployment on the general public, and consequently, developing their use on the Internet. For these reasons, much research has focused on shallow and shallow systems. In this paper, we propose to use deep and dense models in a double-blind experimental setup which avoids the need of user annotations. We compare both approaches and evaluate their accuracy and robustness on both a public and a private dataset. Results show that the deep model is highly robust but only the dense model is able to achieve good performance on the private dataset.
67. **Deep Spiking Neural Networks for Large Vocabulary Automatic Speech Recognition**
   Spiking neural networks have become popular for improving speech recognition performance in the context of deep learning architectures. Based on the training of these networks, a new kind of Spiking Neural Network (SNN) is proposed for large vocabulary automatic speech recognition (LVASR). At the core of the SNN is the new SpikCNN, a fully connected convolutional network with spiking weights and dropout connections. We apply the SNN to the Charades test set and investigate the performance of the new network. To show the robustness of the new network, we use the full 20 hours Charades test set and verify the performance of the proposed network. In addition to the findings on the above-mentioned tasks, we find that spiking neural networks are more efficient in terms of memory usage than traditional feedforward neural networks for SVASR tasks. As the performance of the proposed SNN for LVASR is still far from that of other state-of-the-art methods, the proposed network is found to be a good candidate for learning meaningful features of speech for LVASR tasks.
   **Deep Poetry: A Chinese Classical Poetry Generation System**
68. In this paper, we present a poetry generation system called \textbf{Deep Poetry} that generates poetry from Chinese classical poetry. The system is composed of two parts: the classical poem generation part and the poems generation part. In the classical poem generation part, two deep convolutional neural network-based models are trained to output poem based on the output of an intermediate decoder. In the poems generation part, a collaborative filtering is applied to select the poems from the candidate poem generated by the classical poem generation part. In the experiments, we validate the effectiveness of the proposed system using the output of the first model and the output of the second model. Experimental results show that the proposed system can generate better poems than the baseline system. It also shows that the poems generated by the proposed system are of higher quality than the previous generation systems.
   **Event detection in Colombian security Twitter news using fine-grained latent topic analysis**
   As the information age is nowadays ubiquitous, with people being exposed to more, more information, cyber threats and exploits are also proliferating. In this context, information security is seen as an important cybersecurity topic with a wide range of security issues for any information infrastructure. This chapter discusses the topic of cyber threats and exploits in security news articles, and provides an overview of various classification methods based on lexical and contextual features, as well as entity-centric features. We evaluate the performance of these methods through both extracted features and the proposed model performance. Our study has been performed on the CIC-CSIC2018 dataset and its outcomes have demonstrated the effectiveness of the proposed system against different types of cyber threats and exploits.
69. **EmpGAN: Multi-resolution Interactive Empathetic Dialogue Generation**
   Multi-resolution dialogue systems can bring new range of useful applications and can effectively solve the dialogue-refinement problem. We propose an end-to-end multi-resolution Dialogue-Reinforcement Learning framework called EmpGAN to effectively learn empathetic dialogue generation. Dialogue-Reinforcement Learning (DRL) is one of the leading paradigms in Dialogue-Aware Neural Systems (DAS) to train end-to-end multi-resolution dialogue systems. Generally, human-human dialogue is composed of non-standard sequences that can differ in terms of time and relative scale. It is also often the case that humans prefer different settings. Existing works show that the task of dialogue refinement is always beneficial in the multi-resolution dialogue models. In this paper, we propose a multi-resolution approach to generate empathetic dialogues by forcing dialogue-reinforcement learning to effectively leverage a dialogue-to-model structure, i.e., from a high resolution dialogue model to a low resolution one. Furthermore, the target reward of the dialogue-reinforcement learning can be expressed in terms of a reward distribution of the original dialogue and a low resolution dialogue model and adapted automatically to both dialogue-reinforcement learning and the original dialogue. Experimental results show that our model can generate more fluent, empathetic and conversationally informative responses than two baselines: existing multi-resolution methods and recent neural-networks methods.
   **Global Greedy Dependency Parsing**
70. In this paper we present a new greedy dependency parser, the first of its kind, that achieves competitive accuracy and parse speed in both full dependency and partial dependency languages. The key insight is that given the choice of parsing algorithm, we could always get back a representation of the tree structure that is complete but has a worst case complexity comparable to a solver that would have been used. We also present an efficient way of exploiting this representation to reduce the search space, thereby permitting the use of efficient greedy solvers. Finally, we demonstrate that the proposed parser can be successfully applied to the problem of tree disambiguation in a partially observed model of parsing.
   **Co-Attention Hierarchical Network: Generating Coherent Long Distractors for Reading Comprehension**
   Reading comprehension systems have shown improvement in the recent years with deep neural networks (DNNs). However, obtaining the ground truth answer to complex questions is computationally intensive for large text corpora, while existing DNN-based approaches generate distractors (`tricks') to help the model improve the performance. Although the quality of the generated distractors has been proposed as an evaluation metric for deep reading comprehension models, no existing approach has evaluated such metric against humans. We propose a novel Co-Attention Hierarchical Network (CHA-Net) to address this problem. Unlike conventional DNN-based approaches, the difficulty of this task is artificially constrained by using two additional attention networks, which are trained jointly. One of them, co-attention, simultaneously attends to the salient parts of the question and the answer pair, which can assist human comprehension systems to identify the appropriate distractors for reducing the computational burden on them. Experimental results on three large-scale datasets show that our proposed approach outperforms the state-of-the-art deep learning-based models in terms of the Coherence-Aware Information Retrieval (C-AIR) metric. We also show that our approach performs better than a standard DNN model, with only 15% of the computational cost, by solving one of the largest question-answering datasets ever compiled, which is still computationally expensive by nature. Furthermore, ourCHA-Net can produce novel distractors in a scalable manner, and can be used for extending the content of existing DNN-based models.
71. **Hunting for Troll Comments in News Community Forums**
   How do we generate meaningful and informative replies to comments on web forums? Do human-based methods provide a reliable means to explore the distributed nature of such discussions? In this paper, we explore the different approaches that people adopt to deal with online threads in news and discussion forums. In addition to generating replies, many users use `game-theoretic' methods to gain an advantage over the opponent. While most of the existing works on this topic focus on unique communities, we analyze a news forum that is very popular in the public internet. We manually label more than 100k comments in the forum, which contain information about programs that are available for download and which communities are popular. We find that users in the forum tend to discuss programs that are highly interrelated. With the help of various linguistic characteristics, our method can generate a highly significant and well-structured subset of comments for the news forum. Moreover, we find that in some communities, the level of recency and similarity between the top comments can be quite high.
   **End-to-end ASR: from Supervised to Semi-Supervised Learning with Modern Architectures**
72. This paper presents an investigation of a novel architecture called End-to-end ASR (E2E ASR), which is capable of reconstructing speaker utterances and speech recognition transcript in an end-to-end manner. E2E ASR leverages state-of-the-art voice conversion techniques that also have been successfully employed in deep learning. Such approaches can learn a model with a high degree of representational compactness in order to directly integrate in the single-stage decoding (S2D) process. E2E ASR applies a language model trained for both the target language and the source language. During the decoding process, it additionally computes a lexicon of source words for each input utterance that are used to predict the marginal probabilities of the target language labels. A neural network is trained to predict such probabilities at different stages of the S2D process. The proposed system is evaluated on LibriSpeech 1.0 and 1.1. Extensive experiments are conducted on English, German and Italian datasets, as well as on other ASR tasks, where E2E ASR improves upon competitive baselines. Furthermore, it has been shown to be more efficient than single-speaker models.
   **Controlling Neural Machine Translation Formality with Synthetic Supervision**
   As part of efforts to bring machine translation closer to human level performance, efforts have recently been made to "drop out" translations in training. This may result in the "formality" of the machine translation output being different from human produced text, and would result in "soft" attention to certain word positions. These efforts are limited to little effect on human evaluations and have low impact on automatic evaluations. In this work we show that, in an external evaluation environment, maintaining a synthetically generated syntactic structure can be beneficial. We use neural machine translation models trained on a synthetic bilingual evaluation corpus to evaluate syntactic forms, in addition to baselines. While the loss in performance due to syntactic supervision, relative to non-syntactic input, is negligible we find that it can be in the region of 5.75% to 16.87%, depending on the synthetic syntactic form. We use existing cross-lingual transfer mechanisms to create artificial auxiliary data that (1) is closer to real, with lower number of bilinguals, (2) increases stability by reducing the regularization effect of syntactic supervision. Finally, we show that these results can be directly used to improve the quality of machine translation, by identifying an essential set of words (such as business entities and items) which, under normal conditions, should not be included in the translation output.
73. **Real-Time Emotion Recognition via Attention Gated Hierarchical Memory Network**
   Emotion recognition is a fundamental but still a challenging task in computer vision field. Since a human observer tends to recall the information related to the emotions from past experiences, a suitable emotion recognizer can help human to gain emotional insights in a long-term-memory preserving fashion. Emotion recognition is also complicated due to a variety of factors such as irrelevant and irrelevant-relevant factors, noisy data, background clutter, occlusions, viewpoint and light etc. Therefore, the available emotion recognizers often need to trade off the prediction accuracy with processing efficiency. In this paper, we propose a novel holistic model, which comprises a cascade of hierarchical recurrent neural network architectures to accurately learn the mapping from pixel space to the emotion representation space at run-time. Our proposed attention gated memory network (AGMNet) exploits the complementary nature of attention mechanism which contains two-stage attention mechanism. Specifically, the first stage generates a representation at the level of granularity, while the second stage exploits the local and global contextual information to adaptively fuse the representations at the finer scale. Therefore, AGMNet can adaptively select informative spatial and temporal regions for accurate emotion representation, and meanwhile improves the recognition accuracy of the intermediate layer. Extensive experiments on three publicly available emotion recognition datasets (i.e. MegaCeleb, MEC-2 and Oulu-CASIA) demonstrate the effectiveness of the proposed approach.
   **CAIL2019-SCM: A Dataset of Similar Case Matching in Legal Domain**
74. We introduce the CAS Litigation Benchmark 2018 (CLB2018) dataset that we made publicly available. The dataset contains 1,876 cases, representing contracts in the domain of commercial mediation. We describe the dataset and argue why this dataset is useful in the research of learning to match cases in the legal domain. We also discuss the issues that we found in the dataset. To the best of our knowledge, this is the first dataset of this kind that is publicly available.
   **How to Ask Better Questions? A Large-Scale Multi-Domain Dataset for Rewriting Ill-Formed Questions**
   Recently, neural models have demonstrated that it is possible to read, compose, and answer coherent natural language questions from textual information. In this paper, we present a large-scale multi-domain dataset containing almost 2 million ill-formed questions that a human is not equipped to answer. Our dataset is composed of ill-formed questions labeled with descriptions of their semantic relationships, which are generated from either human comments or automatically generated content (such as forum posts). We demonstrate that it is possible to do visual question-answering tasks on a larger scale than has previously been possible and show that this can improve a state-of-the-art strong passage-level neural model, RCNN, by 5.7% F1 score on the LastFM test set.
75. **Assessing the Benchmarking Capacity of Machine Reading Comprehension Datasets**
   Machine reading comprehension (MRC) involves acquiring a complex question from natural language text. It remains a challenging task and, as a result, it is rarely investigated in multiple datasets. This problem is even more difficult when the problems are well known. In this paper, we investigate the benchmarking capacity of MRC datasets. We focus on question-word pairs in text; thus, we do not use rich language features or knowledge bases. We examine the performance of automatic question-answer generation models, which are machine reading algorithms that rely on the input question and the output answer. We identify limitations of existing benchmarking datasets and present two questions: 1) which questions should not be used for the evaluation of existing benchmarks? and 2) which datasets should not be used for the evaluation of existing benchmarks? Our first question identifies an absence of data-dependent and interpretable evaluation methods for existing benchmarks. Our second question questions whether existing benchmarking datasets may be improved by collecting more data.
   **Cantonese Automatic Speech Recognition Using Transfer Learning from Mandarin**
76. Automatic speech recognition is an essential task in Computer Science. With the increasing computing power, it has become a challenge to build automatic speech recognition system from Mandarin speech corpus. Transfer learning from Mandarin is a new method of improving the performance of spoken language recognition and it has been successful in improving the accuracy of Mandarin Automatic Speech Recognition (ASR). This paper proposes a transfer learning approach from Mandarin ASR model and evaluates the performance of Mandarin ASR system using CTC based approach. This paper contains the experimental evaluation of a Mandarin ASR system using transfer learning. Using Mandarin ASR model, we evaluate the performance of the Mandarin ASR system and the results of CTC based approach are used to evaluate the performance of Mandarin ASR system. Experiments with the Mandarin ASR model and the CTC approach were done and the performance of the Mandarin ASR system using transfer learning was measured. It is noted that the performance of the Mandarin ASR system using the CTC approach are better than the performance of Mandarin ASR system using Mandarin based approach. In the experiment, the performance of Mandarin ASR system using the CTC approach were compared with that of Mandarin ASR system using Mandarin based approach.
   **Characterizing Scalability of Sparse Matrix-Vector Multiplications on Phytium FT-2000+ Many-cores**
   Sparse matrix-vector multiplications (SVMs) are widely applied in the field of natural language processing, among which they play a key role in the parser and generation models of TreeBank (OB), a widely-used statistical machine translation. The computational complexity of kernel SVMMs is of special interest and has been discussed in a broad range of papers in the recent years, but here we focus on a more practical type of SVMs, sparsity-guided SVMs. Our investigation shows that under certain conditions, kernel SVMs can achieve an optimal approximation to kernel SVMMs, i.e., they can be learned efficiently. However, due to its sparsity-driven nature, kernel SVMs cannot achieve the worst-case approximation for the majority of popular SVM training algorithms in the literature. On the other hand, kernel SVMMs have a fairly high scalability in comparison with the algorithms, e.g., the nearest neighbor method. This result motivates us to develop kernel-guided SVMs to further address the above scalability issue. Our proposed approach is particularly well-suited to the training of tree-structured neural models, e.g., Bayesian neural networks, whose training processes naturally include tree-structured SVMs. We study various kernel-guided SVMs and show how these kernels can be tailored to improve the scalability of the models by incorporating new relevant information. Our experimental results show that kernels from the sparse matrix-vector multiplications family perform well on a range of text classification tasks, showing that the feature-sparsity property of kernel SVMMs is a crucial enabler for their scalability.
77. **Natural Language Generation Challenges for Explainable AI**
   Deep learning techniques are achieving unprecedented success on a wide range of NLP tasks. However, there is no reliable automated way to interpret these predictions. This makes it challenging to analyze their performance. Here, we discuss four challenges in explanation in NLP, specifically: 1) why do the trained models have low perplexity, 2) what explains an output, 3) what causes a model to predict an incorrect output, and 4) what role does prior knowledge play in explanation. We propose a deep learning model that supports explanation by quantifying perplexity as the difference between a predicted answer and the one that would be given by a human reader. We then discuss how recent natural language generation datasets like METEOR and Penn Treebank help to tackle these challenges. Finally, we experiment with our model on METEOR and on Penn Treebank datasets and show that the model is more effective in generating human-readable explanations.
   **Joint Emotion Label Space Modelling for Affect Lexica**
78. Emotion labels are informative factors for affective computing and have shown to be effective for various tasks. However, most existing emotion models ignore label information which can improve performance for downstream applications. In this paper, we propose a joint emotion model for affect lexica using a new learnable model space of emotion labels. Emotion labels are embedded into a reduced dimensional feature space, which is interpretable by learning two different neural models: one for label representation and the other for emotion representation. We evaluate our approach on two affective data sets and show significant improvements over traditional baselines.
   **Table-Of-Contents generation on contemporary documents**
   Recent trends in electronic book content publishing, such as digital access to scanned books, digital publication of encyclopedic collections, and the unprecedented number of book formats and editions, have motivated the use of machine-assisted reading systems (MARS). Users produce text in MOOCs, and MARS help them find relevant sections of the document that they need to answer a question or find answers to other related queries. Unlike traditional machine reading systems, which analyze documents individually or in isolation, we present a framework for providing a unified analysis of the document by exploiting the structure of a document's code. Given the code of a document and the comments of relevant documents, our approach outputs the text to be read by the system, with a simple statistical model over the language of the document, its coding style, and how often the name of a section occurs within the text. Our experiments show that we are able to recognize any textual content within the document using our model, while previous approaches can only recognize either the section name or the first sentence.
79. **Demystifying TasNet: A Dissecting Approach**
   Despite tremendous success in the area of classification, it is well-known that training deep network models is also an expensive process. Training standard classification models requires at least several hours, while developing or tuning new architectures is often tedious and relies on accurate and consistent estimates of model parameters. Another major challenge is that the number of samples and their uncertainty are extremely high, making it even more difficult to establish a baseline without resorting to expensive and time-consuming hyper-parameter tuning. In order to address these issues, in this paper we propose a technique that disentangles the original decision forest model into smaller sub-models that are easier to train. The technique only requires a baseline decision forest model, a vector of network statistics, and the stochastic gradient descent algorithm. We show that the state-of-the-art classifier produced by this approach, ditched only inputs with high input ambiguity (e.g. color, height, and materials) from the training set, can be trained to perform significantly better than the original model, while keeping all other input variables similar. We also show that even with a tiny training dataset, with only $100\%$ of all possible inputs, ditching such a component leads to even better performance than the original classification network model.
   **Casting a Wide Net: Robust Extraction of Potentially Idiomatic Expressions**
80. We study how a trained machine learning classifier may be misused to extract potentially idiomatic expressions (PUEs). Specifically, we look at learning approaches to extract PUEs from text that have been trained for other NLP tasks. We propose several methods for detecting PUEs based on statistical properties of text corpora. We find that strong learning classifiers (consequently, strong PUE extraction systems) may also be misused to extract PUEs. We study in detail the types of misuses to which such approaches can be applied and characterize the properties that give rise to such misuses. Using two datasets of about 1 million Wikipedia statements from the Unite the Right 2000 and 2000 conferences, we show that the input to strong classifiers, whether they be statistical or neural networks, is mostly a simple CWA; moreover, such inputs often contain the PUE. Despite the fact that different language and network architectures are capable of correctly identifying potentially idiomatic expressions, we find that relatively large neural networks can use simple heuristics to extract PUEs, but cannot distinguish between the PUEs and normal sentences. Our results indicate that neural classifiers are not at the same level of quality as strong classifiers. Moreover, our results suggest that two common feature representations used by strong classifiers and neural network classifiers are different, and may be complementary in other NLP tasks.
   **Reinforcing an Image Caption Generator Using Off-Line Human Feedback**
   In recent years, machine learning-based caption generation has achieved promising results. However, existing methods suffer from the problem of catastrophic forgetting during the training phase, a consequence of the trivial conditional distribution of the semantic labels and the lack of effective feedback. To address this issue, we propose a new model based on a generative adversarial network. In our model, we use off-line human feedback to influence the generation process of the caption. Specifically, we show that it is possible to exploit the semantic knowledge as a powerful selective regularization. We experimentally demonstrate that the new model can significantly improve the generated captions.
81. **Learning Multi-level Dependencies for Robust Word Recognition**
   Research on word recognition is primarily focused on acquiring discriminative word representations to extract discriminating features from word strings. In contrast, most conventional approaches for measuring similarity between words are based on examining short-term correlations of word sequences (context words), without explicitly taking into account long-term dependency information between words. We propose a non-parametric similarity measure based on a discrete classification technique, which measures similarities between contextual and long-term relationships. The model is highly discriminative, and robust to various length and noise levels of word string. The model is extended to incorporate contextual word dependencies, which is further verified on the construction of a large dataset of contextual word sequences, and its application for identification of term co-occurrences. As demonstrated by experimental results, our model improves both word sequence identification and relationship classification accuracy, when compared to several recent baseline methods.
   **Global Health Monitor: A Web-based System for Detecting and Mapping Infectious Diseases**
82. The rapidly increasing costs of healthcare operations and administrative complexity of the spread of infectious diseases has led to a push for improved management of disease containment. Combining surveillance data with expert knowledge is a vital component of any disease containment effort. However, traditional computer-aided disease containment systems rely on manually labeling data. This work proposes a system to generate disease descriptions and associated descriptive data for monitoring of infectious diseases. The infectious disease community has developed formal definitions of disease descriptions (such as VKID), but they have been previously ignored. A new class of diseases, termed as Global Health Monitor, has been introduced as a mechanism to define global health monitors. Based on these definitions, a database of global health monitors was developed. We demonstrate how a technique developed for the classification of medical images can be applied to this database.
   **Weakly-Supervised Opinion Summarization by Leveraging External Information**
   We describe a method for automatically generating summarization of reviews written by high-quality authors. Unlike prior work that relies on keyword search, we make use of a large, annotated set of reviews (Reddit) to automatically rank them according to the quality of the content they describe. Such a high-quality set is difficult to obtain for many domains and languages. We train on lists of 250-250k reviews from Reddit to estimate the quality of a review set using an explicit, joint distribution of the product and the author features. Our method uses a stochastic hierarchical ensemble model to predict quality scores from this simple joint distribution. When trained only on Amazon reviews, our method yields state-of-the-art performance on product quality prediction. We demonstrate its generalization to other domains and languages.
83. **A Discrete CVAE for Response Generation on Short-Text Conversation**
   Discrete Variational Autoencoders (D-VAEs) have been previously proposed for image and audio generation, however, it remains unclear whether they can generalize to synthesizing text responses. In this paper, we apply the Discrete CVAE (DCVAE) model to response generation on short-text conversation, where a long short-term memory network is trained with conversation-level variational inference and a novel variational attention mechanism is developed to address the requirement for generating coherent responses. Experiments on two representative short-text conversational datasets demonstrate the efficacy of the proposed approach.
   **Joint Learning of Answer Selection and Answer Summary Generation in Community Question Answering**
84. Community Question Answering (CQA) has become a popular topic of research and popularized in recent years. However, due to the growing popularity of CQA, existing systems either process answer selections from CQA data or summarize answers for CQA based on CQA understanding. In this work, we propose a novel joint learning framework which simultaneously predicts answers, their confidence scores and their length in the answer summaries. To enable our proposed model, we integrate an autoencoder-based sequential model for answer selection into a CQA framework. We also introduce several metrics that can be used to evaluate and compare proposed method against state-of-the-art methods. Experimental results on five real-world datasets show that our method is able to produce an answer summary that is comparable to state-of-the-art methods in both accuracy and interpretability. In addition, our model also shows the ability to generate multiple answers for a question.
   **Generating Diverse Translation by Manipulating Multi-Head Attention**
   Multi-head attention on sequences plays an essential role in natural language generation (NLG), a task with wide applications and challenging conditions. It has been shown that to be able to control the attention distribution in different heads within a long-term attention network, it is important to disentangle the languages being modeled and the attention distributions in the same heads. However, most previous works apply conventional softmax crossentropy loss, which consists of a single adversarial loss in a long-term network. In this paper, we propose a novel approach to directly control the attention distribution through a divide and conquer strategy of a multi-head attention. Unlike previous methods, our method is able to control the attention distribution of the entire output sequence from the head to the tail, without modifying the neural structures of the long-term attention network. Moreover, to apply this strategy to individual heads of an attention network, we propose a robust splitting technique. We conduct experiments on the tasks of English-German and English-French translation, and demonstrate that our method achieves state-of-the-art performance.
85. **An Empirical Study of Sections in Classifying Disease Outbreak Reports**
   Predicting outbreaks of disease in new areas is a labor-intensive task that requires significant data collection. When we have high volume and high velocity of data, forecasting outbreaks is extremely challenging. We have presented a method to identify regions of a country with high and low health outbreaks through statistical analysis using data gathered from the national reporting portal of each country. We applied our model to a number of time series forecasting problems, including disease outbreak data from the Centers for Disease Control and Prevention (CDC) and air pollution data from the Environmental Protection Agency (EPA). We have compared our results with a statistical model that is based on regression, showing that the proposed model outperforms the statistical model in all of the considered time series forecasting problems.
   **Emotion Recognition for Vietnamese Social Media Text**
86. Social media data, in particular, emoji and mood words are key elements for detecting the emotional states of people, and they could play a crucial role in behavioral analytics. In this paper, we propose an emotion recognition approach for Vietnamese Twitter text. Emotion recognition is related to the emotion analysis of texts, which shows the emotional states of the people expressed in the text. Most existing methods are adapted to English text, because of their large amount of emotion-bearing characters and syntactic similarity with English. In this paper, we try to learn from the rich emotion information of Vietnamese text. We use neural networks to construct an emotion language model based on the word embedding and bag-of-words techniques to model the emotions of tweets. Then we use the character embedding to learn an emotion word embedding to represent each tweet. We evaluate the performance of our model on two publicly available datasets, and the experimental results show that our model is able to recognize the emotions of tweets well.
   **Chemical-protein Interaction Extraction via Gaussian Probability Distribution and External Biomedical Knowledge**
   Automatic extraction of relevant chemical-protein interactions (CPIs) plays an important role in drug discovery. However, an important challenge in CPI extraction is the difficulty of obtaining a sufficient number of training examples. In this paper, we propose a deep learning-based approach to extract CPs from biomedical text using variational auto-encoders. The proposed approach includes three key components: (i) a supervised variational auto-encoder (VAE) and a self-attention mechanism to encode sentence-aligned, context-boundary protein sequences into a high-dimensional feature vector, and then an unsupervised Siamese auto-encoder (SAE) to learn unsupervised feature embedding on a large unlabeled test set. (ii) a modified variational auto-encoder, which exploits an auxiliary network to learn high-level features and allow the VAE to learn to recognize morphological similarities between protein sequences, and then a conditional VAE (cVAE) that learns the affinity of different protein sequences to the same motif from multiple domains. (iii) a text classification task to extract relevant CPs from the biomedical texts, which is validated on the publicly available TACRED and publicly available CharliXR8 datasets. We evaluated our approach on three different tasks, namely, protein-protein interaction prediction, protein-protein binding affinity prediction, and protein-protein interaction prediction, on these three publicly available datasets and achieved promising performance.
87. **A Cluster Ranking Model for Full Anaphora Resolution**
   Anaphora resolution can be divided into two stages: utterance selection and resolution. Selecting the correct noun phrase for anaphora resolution is critical to resolve anaphora correctly, but it is non-trivial due to varying types of argument and partial knowledge in the knowledge base. A high-quality anaphora resolution model is essential for full anaphora resolution. In this paper, we propose a novel cluster ranking model for full anaphora resolution. First, we perform anaphora selection through clustering the arguments in the knowledge base to form the top-k candidates. Based on the candidate cluster, a quality score is obtained for each possible candidate phrase. Based on the quality score, cluster ranking is performed to select the top-k candidate phrase. Extensive experiments on Twitter data show that our method achieves state-of-the-art performance.
   **An analysis of observation length requirements for machine understanding of human behaviors in spoken language**
88. In this paper, we present a statistical analysis of the "one shot" capability of neural network models of human behaviors in spoken language. This means that we learn all "hidden" models of the observed behavior, e.g., models of internal states, a cause, and a goal. Our analysis is based on a new dataset consisting of audio recordings from full conversations in order to demonstrate the kinds of phenomena that neural network models can predict and may be able to support. Our analysis includes properties of the behavior that allow us to derive new insight into this form of modeling.
   **A Data Set of Internet Claims and Comparison of their Sentiments with Credibility**
   The continuous growth of internet controversies brings with it the need to know what users think, especially those who issue them. However, most of the research on controversies has focused on authorship and public reactions. In this paper, we develop a statistical language for analyzing the opinions of internet users regarding the media, in order to build a gold standard for measuring the credibility of the claims that are being made on the Internet. Furthermore, we evaluate the proposed approach to measure the credibility of media news using a natural language processing methodology and report on our experiments for the evaluation of the proposed method. We hope that the results of our study will provide new insights for the development of a systematic methodology for measuring the credibility of news on the Internet.
89. **Topical Phrase Extraction from Clinical Reports by Incorporating both Local and Global Context**
   The goal of this paper is to classify the content of clinical text and extract topical phrases of the type "Drug Name". We attempt to tackle this task by exploiting both local and global contextual information to perform the task in two different ways: extracting topical phrases by means of recurrent neural networks (RNN) and by utilizing a hierarchical random walk embedding model, able to explicitly capture relations between terms in both local and global contexts. We report a large empirical evaluation that clearly shows the potential of RNNs for identifying topical phrases, i.e. phrases whose content is specific to a specific clinical disease. Moreover, the results show that exploiting both local and global contexts, namely representations based on a hierarchical random walk embedding model and information from the current document, improve the prediction performance of RNNs. Furthermore, we have developed a conceptual analysis of the efficacy of our model and have performed experiments that illustrate the success of our approach, comparing our approach with another representative phrase extractor based on a strong nearest neighbor information extracted from the topic hierarchy.
   **CRUR: Coupled-Recurrent Unit for Unification, Conceptualization and Context Capture for Language Representation -- A Generalization of Bi Directional LSTM**
90. The proposed model named CRUR is a generalization of the LSTM model for automatic data representation and understanding. It embeds concepts, how they are encoded in words or what context they are deployed in, into a neural network layer. Meanwhile, it increases the training stability to allow the model to focus on capturing the most important contextual features for the given concepts. The network is extended by a bidirectional long short-term memory, which makes possible to capture long-term dependencies. CRUR is a principled model that achieves the state-of-the-art performance for both sentence tagging and document classification on the Wall Street Journal and Quora datasets. On the WSJ dataset, it achieves a BLEU-3 of 39.1. On the Quora dataset, it achieves a BLEU-1 of 22.4.
   **Go From the General to the Particular: Multi-Domain Translation with Domain Transformation Networks**
   In this work, we aim to boost text translation accuracy by expanding the scope of text corpora by translating texts in different domains. Our approach starts with the usual assumption that one cannot utilize large-scale corpora to get bilingual sentences. Instead, we propose to add random words of different domains into the target domain and then transfer the knowledge of those words in the source domain to the target domain, a combination of domain transformation and machine translation. Domain transformation networks (DNTN) are used as the new translation target model and trained on target domain sentences generated by a language model, and the DNTN sentences are translated into the source domain. The source-domain sentences are further translated to the target domain in the conditional manner. Experiments on two English-French translation tasks (translation of academic article article titles and English-language TV show reels) show that our approach can achieve significant improvements over the standard translation models.
91. **Classifying Vietnamese Disease Outbreak Reports with Important Sentences and Rich Features**
   We propose an approach to classify disease outbreak reports in Vietnamese using rich biomedical features. We use a wide variety of feature combinations and extend widely used Word2vec features with biomedical text and activation features. Our system achieves about 89.9% of F1 score with 10 features.
   **Continual adaptation for efficient machine communication**
92. In this paper, we propose a new type of iterative approach for automatic machine learning that operates in an alternating-time paradigm. The idea is based on the adaptation of a machine learning algorithm to the data obtained using a different algorithm. We consider both offline and online settings. The idea is to observe the learning trajectory of the learning algorithm over a time period of several weeks and to adapt the algorithm based on that trajectory. This paper investigates two different adaptation strategies. The first adapts the algorithm to the data that are released during the learning period and the second adapts it to the same data after a period of stopping the learning algorithm. In both cases, the time that elapses between the release of the new data and the first communication of the data is used to determine the effectiveness of the adaptive approach. Numerical experiments show that, despite having a relatively large number of parameters, the proposed approach is more robust to the error of the learning algorithm than alternative approaches.
   **SWift -- A SignWriting editor to bridge between deaf world and e-learning**
   The World Wide Web is a major source of multilingual knowledge, especially for natural languages. SignWriting is an emerging research area to perform sign writing for a language other than English. It enables signers to maintain an active presence on the World Wide Web, where e-learning is being developed to enable signing at scale, through a variety of communication tools. This paper presents SWIFT, a general purpose signwriting editor. SWIFT is an easy-to-use tool. It includes features to overcome common stumbling blocks faced by deaf people, and it has been developed to support development of e-learning sign writing applications.
93. **Resource production of written forms of Sign Languages by a user-centered editor, SWift (SignWriting improved fast transcriber)**
   In this paper we describe the development of a sign language translation system with an interactive version of an online writing program for signed languages. The interface of the system is based on the latest version of the open source SWIFT (SignWriting improved fast transcriber) toolkit. The objective of this study is to investigate the use of sign language as a method of communication. We investigate to what extent using sign language as a form of communication can be enhanced and how this could be achieved by means of an interactive, user-centered approach, based on written forms of Sign Languages. The goal of this study is to demonstrate the feasibility of an interactive sign language translation system, able to convey messages and make decisions about what is said during the signed communication. The idea of the system is to replace the traditional system-centric approach of interpreting texts in sign languages with a user-centered one which ensures that the system's decisions are based on a closer look at the issues and constraints that lead to an effective expression of what is being said and what is being done. The system's interfaces are presented and compared. The system is tested and compared with an interactive speech communicator. Results show the superiority of the user-centered approach.
   **Learning to Caption Images with Two-Stream Attention and Sentence Auto-Encoder**
94. As the deep learning community continues to achieve impressive progress, one of the remaining fundamental challenges for sentence understanding is to predict an accurate captions. Although methods such as deep recurrent neural network (DRNN) and self-attention have been widely applied to image captioning, they have not been studied for language understanding. In this paper, we present a deep learning architecture that can be used for both image captioning and sentence understanding. Our method uses two encoders: the encoder for image captioning and the sentence encoder for sentence understanding. In addition, we propose a novel sentence-aware auto-encoder (SAAE) to learn sentence embeddings by learning both visual and semantic information from sentence descriptions. Experimental results show that our model can achieve significantly better performance for sentence captioning than existing methods.
   **Multilingual Culture-Independent Word Analogy Datasets**
   This paper describes a set of multi-lingual word analogy datasets (MIDA) as well as the resource that produces the data. These datasets are in English and Spanish. MADAs were generated through lexicalizations of each language's word analogy corpus (LIWC).LIWC is one of the largest similarity corpora currently available. This paper describes how these datasets were produced, their different aspects and characteristics, and the methods used to mine the corpora. These methods and outputs also address the challenges of (a) setting up effective automated methods to mine the corpora; and (b) creating efficient models to rank the similarity of words within a target language. The new datasets are publicly available for research purposes.
95. **Controlling the Amount of Verbatim Copying in Abstractive Summarization**
   Automated abstractive summarization uses copy-movement to accommodate a length limitation of the output and to save space for the input summary. However, the method often includes a lot of copying, rendering the output ineffective as most of the sentences contain content unrelated to the input text. In this work, we present a comprehensive study on automatic abstractive summarization using the insertion of copy-movement. We have identified several conditions that lead to automatic summarization with increased copying. These include: length constraints on the summary length, preprocessing the output with pre-processing methods, and the inclusion of extrinsic information. To evaluate these conditions, we conduct a series of experiments to evaluate the copying condition that uses common systems and an embedding of the sentence(s) in the external knowledge base. For the experimental evaluation, we have also produced two datasets consisting of real-world summaries. Our experiments show that copy-movement can increase the overall efficiency of the human-generated summary and also provide an effective way to control the amount of copying for automated abstractive summarization.
   **Using the Web as an Implicit Training Set: Application to Noun Compound Syntax and Semantics**
96. There are several large-scale works on Noun Compound Syntax and Semantics. One of the useful tools to process the information is the content of a web-based bibliography. The sense of an noun compound can be obtained by viewing its occurrences in a bibliography (dictionary), which allows one to infer the meaning of a compound from its senses. This method has been used by Winiarski (1988), who presented an algorithm for the construction of the sense of a compound. Two years later the USENIX workshop (Winiarski, 1989) considered the problem of automated construction of the sense of a compound, and the NIST research project (Brady and Fish, 1992) offers an example of a sense construction based on Wikipedia. This work examines the use of a web-based bibliography as a model to construct the sense of a compound. The resulting algorithm is implemented and evaluated on three examples, where it performs nearly as well as the classical construction of a sense in Winiarski (1988).
   **Joint Parsing and Generation for Abstractive Summarization**
   In this paper, we propose a joint parser and generator for abstractive summarization that can generate the summary conditioned on the main body of the document. Our method generates a proposal in a high-quality, non-tokenized representation of the original document, which can then be used as a soft annotation for the ground-truth and as a target for learning to generate the summary. This set of features are combined with a neural grammar and generated by exploiting the whole document, which is passed through a character-based recurrent neural network. We evaluate our system on the TAC-KITCHENS dataset, and demonstrate substantial improvement in terms of human evaluation metrics.
97. **ScienceExamCER: A High-Density Fine-Grained Science-Domain Corpus for Common Entity Recognition**
   In recent years, text understanding technologies have become increasingly capable of handling the complex natural language phenomena we encounter in our daily lives. Many of these technologies were developed with the goal of improving the natural language understanding (NLU) task at the document level. However, as text is used to provide additional context to explain the output of NLU systems, it is critical to capture necessary contextual entities in a large-scale document dataset for accurate NLI. This paper introduces a new dataset of 50M sentence pairs annotated with contextual entities across 521 fine-grained science-domain articles, collected during two public events. Our extensive experiments show the importance of the contextual entities identified in the context for the task of fine-grained NLI.
   **Discourse Level Factors for Sentence Deletion in Text Simplification**
98. Automatic text simplification is a challenging problem in natural language processing. Recent studies have shown that models have reached impressive accuracies on this task. However, the absence of well-defined semantic conditions has severely limited the success of this task. In this paper, we aim to uncover semantic factors affecting text simplification. We propose to evaluate the semantic changes for a simplified sentence in an annotated corpus. These are based on meaningful clues in the modified sentence, but not when the sentence is deleted. We use state-of-the-art linear dimensionality reduction methods and hand-crafted features for this task. Experiments on a Chinese simplified news dataset show that the proposed method can improve simplification models' accuracies by 1% and 4.5%. By using the same simplification models trained on a German news dataset, we achieve an error reduction of 1.8% and 4.9% on the training set, respectively. Our code and datasets are publicly available.
   **A Transformer-based approach to Irony and Sarcasm detection**
   Ironies and sarcasm are polar opposites of intelligence, and both concepts are very prone to exaggeration in informal situations. Although a variety of sentiment analysis algorithms exist, most of them fall short on distinguishing between irony and sarcasm expressed in the text. In this paper, we propose a sentiment-labeled and sentiment-corrected multilayer transformer model with attention-based encoding to classify whether two words convey the same sentiment. We employ the content of sentential expressions to associate the sentiment in each sentence and fuse these two information vectors to classify the sentiment in the sentence. We applied our model to the task of ironies and sarcasm detection on the Twitter corpus, and our experiments show the effectiveness of our model.
99. **When is ACL's Deadline? A Scientific Conversational Agent**
   ACL, the largest conference organization in Computing, is an annual four-day event that attracts over 12,000 participants to Barcelona in June and July. The full conference proceedings are only released after every four-day conference. In practice, ACL holds a number of workshops and workshopshops over several years. In this article, we propose an approach to producing scientific software of substantial academic interest using an agent-based AI-based framework. This research product is a methodology that is consistent with the scientific context and intended to facilitate the academic conversation around ACL events. In particular, the work product is intended to be a friendly conversational agent that can be used for ongoing and upcoming ACL workshops, talks, special events, as well as facilitating academic and methodological research in the appropriate venues.
   **Enhancing Out-Of-Domain Utterance Detection with Data Augmentation Based on Word Embeddings**
100. One common method for improving neural utterance detection (NER) is to augment the training data. This paper explores the effectiveness of word embeddings as a content prior for embedding-based NER models and presents experiments on the WN18 RSR Evaluation dataset and the public WMT 2018 English to German Speech Recognition Evaluation dataset. Experimental results show that augmenting the training data can significantly improve the performance of models trained with word embeddings and that the contextual information contained in the word embeddings can provide additional context to improve NER performance.
   **Who did They Respond to? Conversation Structure Modeling using Masked Hierarchical Transformer**
   In this paper we introduce Masked Hierarchical Transformer (MHT), a simple, efficient and generalizable approach for conversation modeling. MHT is motivated by the assumption that most conversations are characterized by limited conversation length, and hence one could define a general latent structure model by modeling the conversation as a sequence of hidden variable or structure states. By optimizing for both length and latent variable in the latent states, we show a highly nonlinear transformation of the latent states can produce a strong latent feature representation. It also brings another advantage, since for the first time we show it is possible to estimate latent variable from conversational sequence even when it is non-linear. We call this the "hanace" property. Additionally, in order to improve model interpretability and generalization, we introduce a novel translation mechanism called "self-attention masking" which keeps the mask of the hidden variable but is self-attentive to non-contextual parts of the conversation. Experiments on two public datasets show that our method outperforms the state-of-the-art methods by large margins. We also show that the model learns to capture human intentions and opinions on multiple real-world datasets.
101. **Learning to Retrieve Reasoning Paths over Wikipedia Graph for Question Answering**
    Learning to link sentences or paragraphs from Wikipedia articles is one of the most efficient ways to build high-quality logical representations. In this paper, we propose a method to retrieve reasoning paths on a graph from individual Wikipedia articles. To learn inference paths, we leverage existing content representations on a global graph (such as Wikidata). By treating each graph as a node in a directed acyclic graph, we can directly retrieve the most relevant paths and representations. We present experimental results showing that our approach can significantly improve ranking performance on various Wikipedia link prediction tasks. We show that our approach also helps answer the question-answering task by retrieving the second-best answer. Our approach could help unsupervised natural language understanding in which reasoning paths are of major importance for the comprehension of external knowledge.
    **Conversational implicatures in English dialogue: Annotated dataset**
102. The ability to translate a conversation is a natural and important aspect of a successful human-computer interaction. This ability has been achieved mostly through approaches that use discourse relations, including information exchange and passage comparison, to search for useful linguistic clues. However, it is not at all clear how these discourse relations compare to the more elaborate semantic links found in natural language. We present here an annotated dataset, Conversational Implicatures (CIM), for evaluation of the ability to translate a conversation, to provide baseline research for this task. We describe its organization and illustrate its contents through a series of cases from which we extract the most insightful implicatures. Finally, we propose several guidelines for future research.
    **JParaCrawl: A Large Scale Web-Based English-Japanese Parallel Corpus**
    The success of NMT systems depends on high quality of parallel corpora. However, creating these corpora is extremely labor-intensive. In this paper, we create a large-scale English-Japanese parallel corpus from web. We explicitly create a set of linguistic annotation guidelines and corresponding metrics to track the quality of the parallel corpus. This corpus thus constitutes a benchmark for English-Japanese machine translation. Additionally, we propose two novel datasets that are designed for different tasks, e.g., e-commerce sentiment classification. We use these data to train various models on various translation tasks, including attention-based encoder-decoder models and the CRF with BERT embeddings. Results show that the developed models for the task outperform the baselines by large margins. For example, using the proposed low-resource low-resource hybrid setup for sentiment classification, the performance of our models achieves 42.9% F1-score, which is the best in the benchmark.
103. **Two Causal Principles for Improving Visual Dialog**
    Visual dialog has been shown to be a valuable tool in multi-turn information gathering, where the goal is to explain the reasoning behind user's decisions by synthesizing the specific observations about the user's previous answers. Although some research has been done on improving visual dialog, most of these methods are inspired by approaches that take some form of training data, which can be difficult to obtain in practice. In this paper, we propose a novel method, called as DropCose, to improve visual dialog through jointly optimizing for two complementary objectives, i.e. 1) the success of the dialog (e.g., replicating the user's queries) and 2) the degree to which the user is able to recover the correct answer. In order to do so, we propose a novel hierarchical visual encoder-decoder model to enhance the visual dialog by retaining and distilling information from the informative parts of the dialog structure. During the decoder, we introduce DropCose method which ensures a similar visual disambiguation to that of the language models. Experimental results on two publicly available datasets show that DropCose is able to generate visually compelling dialogs that can outperform or at least match the performance of existing methods.
    **Examining the Role of Clickbait Headlines to Engage Readers with Reliable Health-related Information**
104. Clickbait headlines represent a rapidly growing area of online media. Increasing online use of social media enables Internet users to receive a constant stream of social updates that may contain textual prurience, thus making them susceptible to clickbait. Despite recent work on automatically detecting clickbait headlines, automatic text feature extraction from news articles remains a challenge. To facilitate research in this area, we created a large-scale real-world dataset for real-world clickbait detection. Our dataset contains clickbait headlines as well as news articles with full text from over 500,000 news publications from two distinct domains, USA and UK. We introduce a novel system for text feature extraction for clickbait headlines. Our system achieves state-of-the-art results on the clickbait detection task, which indicates the need for automatic text feature extraction from news articles.
    **Visual Summarization of Scholarly Videos using Word Embeddings and Keyphrase Extraction**
    The increase in scientific publications provides opportunities to build image or video summarization systems that can be used for summarizing these publications. The goal of these systems is to obtain a representative summary of the primary document (or portion thereof), while avoiding irrelevant (non-documents) content or other noises. At the same time, these systems should be able to produce an interactive version for assistance in summarization. The former is achieved by using word embedding methods, where the output is represented by a set of vector representations of words. The latter can be achieved by using an attention mechanism to project the entire document or video to a low-dimensional, stable space. As no standardised task exists to measure performance, the automatic, interactive summarization system will need to assess the strengths and weaknesses of both approaches. The abstract describes a methodology to use only the word embedding method to automatically build a summary using a chosen evaluation metric.
105. **Importance-Aware Learning for Neural Headline Editing**
    Automatic headline generation, as the name suggests, attempts to generate the headline text directly from the raw text of the article. However, manually rewriting text is time-consuming and prone to subjectivity. To tackle this issue, in this paper, we propose a novel topic-aware learning model that facilitates neural headline editors to pay attention to the desired topic of the article. With attention-based topic modeling, our proposed neural headline editor enables human experts to answer a broad range of most desirable and undesirable questions (from both article editors and readers) while keeping the text-to- headline ratio at around 0.5, which is a significant improvement from existing methods. Moreover, a topic modeling module allows our method to automatically discover the relevant topic of a given article and thus avoid answering its commonly-asked questions (from both human experts and readers) during the learning process. Through extensive experiments with real-world articles, we show that the proposed model not only outperforms existing neural headline editors but also compares favorably with human experts.
    **Independent language modeling architecture for end-to-end ASR**
106. In this paper, we investigate end-to-end automatic speech recognition (ASR) from speech from a phonetically annotated source. The source speaker speech is processed using an independent decoding architecture, which makes it possible to benefit from monolingual speech that has been generated in another language. For this purpose, the separate decoding networks (DNNs) are merged in a multi-layered network with a novel adaptive cross-lingual attention mechanism. Moreover, a corresponding de-noising RNN is used to generate language specific features for ASR. Experimental results show that the proposed architecture achieves state-of-the-art performance on the English-Chinese (ICDAR-2013) dataset, which is far better than previous monolingual ASR systems.
    **Tracing State-Level Obesity Prevalence from Sentence Embeddings of Tweets: A Feasibility Study**
    One of the main problems in the current study of computational linguistics is the measurement of the prevalence of obesity on social media. Existing obesity prevalence methods are related to the use of preprocessing and embedding, however, they are not specifically designed to characterize state-level obesity. This paper proposes a new method that addresses this problem, and it is based on the use of word embeddings. Results of experiments performed on a benchmark Twitter corpus and clinical data of four thousand patients show that the proposed method for obesity prevalence estimation can produce a result comparable to the current state-of-the-art approaches, providing tools for quantifying state-level obesity.
107. **Emotional Neural Language Generation Grounded in Situational Contexts**
    In this paper, we propose a novel neural model for automatic generation of emotional sentences in dialogue. Our approach can not only capture emotional inflectionality and personal sentiment, but also identify semantically-related emotional relations between expressions, which is complementary to existing sentiment analysis based methods. We introduce an emotion-emotion embedding based on recently developed word embeddings. This embedding effectively encodes emotional information for a dialogue context, such that it can be integrated with standard natural language processing systems. We conduct experiments to validate the effectiveness of our model and show that our proposed model can generate higher-quality emotional sentences compared with a state-of-the-art neural model.
    **Towards robust word embeddings for noisy texts**
108. A major challenge in text mining research is to model rich and meaningful patterns across texts. In this paper we explore the use of word embeddings as features for such tasks. Word embeddings are high-dimensional representations, which lack intrinsic meaning, and thus are unsuitable for text mining tasks such as document classification. We present an alternative set of embeddings called probabilistic word vectors that capture richer, meaningful patterns. We study the robustness of these embeddings to syntactic and semantic noise, as well as to size and content variations in the texts, and show that they match or outperform all existing sentence-level and document-level benchmarks. We discuss several design choices that help make the probabilistic word vectors well suited for natural language processing tasks.
    **Learning to Reuse Translations: Guiding Neural Machine Translation with Examples**
    Many successful neural machine translation (NMT) systems have a single attention-based encoder and decoder, which aims to capture the global semantic meaning of the input sentences. However, we observe that this structure leads to global catastrophic forgetting when applied to rare words that contain repeated meanings or when translating sentences of different lengths. This can lead to a serious drop in translation quality in the long run, and lead to unclear semantic patterns between translation instances, which makes finding the correct translation for the context unclear. We propose a novel framework to prevent catastrophic forgetting by using a high-capacity lexicon to provide guidance and updating the word vectors via the lexicon, and to use these vectors as examples to guide the low-capacity encoder. The purpose of these lexicons is twofold: (1) to preserve the local semantic semantics and (2) to guide the decoding process, as illustrated by the French-German translation in this paper. Experimental results demonstrate that our approach is effective in preventing catastrophic forgetting and increasing the translation quality. In particular, the proposed model significantly outperforms the state-of-the-art NMT systems.
109. **Unsupervised Domain Adaptation of Language Models for Reading Comprehension**
    This paper studies the general problem of unsupervised domain adaptation of language models for question answering (QA) on the newswire dataset. We focus on modeling fine-grained entities such as titles and author bio-attributes as part of an abstractive QA model. We explore the use of character-level embeddings as an alternative representation for the document to mitigate the source domain shift. We apply neural networks and tree-structured shallow-neural-network language models (TNNs) to both English and German and show that the performance of the models significantly increases. The use of character embeddings is more appropriate for the abstractive QA setting as the usage of deep features also requires a huge amount of labeled training data. Our experiments show that no supervised domain adaptation mechanism can effectively learn word representations on the abstractive QA dataset. To avoid the excessive amount of labeled data we propose to incorporate fine-grained entity information into the embedding space and perform fine-grained pre-training on individual entities in each domain. Our results indicate that using TNNs for fine-grained pre-training is an effective strategy for mitigating the language-domain mismatch on abstractive QA tasks.
    **Chinese Spelling Error Detection Using a Fusion Lattice LSTM**
110. Text error detection plays a very important role in many NLP tasks. Compared with traditional word error detection, error detection in Chinese language is also more difficult due to many spelling variations in Chinese and the lack of computational resources to build word error dictionary. In this paper, we propose a spelling error detection model that uses a neural network to predict errors in Chinese characters based on word error detection scores. We also develop a lattice lattice structure to avoid the curse of dimensionality and model possible errors by leveraging the relationship between the lattices. In the experiments, we apply our model on two datasets. The experiments show that our model achieves the state-of-the-art results on both datasets.
    **Financial Event Extraction Using Wikipedia-Based Weak Supervision**
    We consider the task of classifying financial events from unstructured documents including newswire and technical documents. A popular approach is based on using supervised machine learning methods to predict a subset of classes for training classifiers. We focus on an extractive semantic event extraction framework where only a small subset of the events from the documents are used to train the classifiers. Wikipedia is a free knowledge resource that describes not only named entities but also non-named entities. Wikipedia is one of the most popular information sources that many people utilize as a source of information. There are many potential applications in which Wikipedia can be used to provide expertise to individuals and to assist organizations and institutions in collecting and publishing large amounts of event data. We present a framework that we have developed to improve the performance of the extractive semantic event classifiers. The framework improves upon previous work by incorporating Wikipedia knowledge in a supervised learning process and also uses a weak supervision technique for reducing the dependence on a large amount of event-specific training data.
111. **FLATM: A Fuzzy Logic Approach Topic Model for Medical Documents**
    Medical topic modeling has become an important research topic for extracting meaningful information from text data. Existing studies usually treat medical topics as a set of discrete documents and, then, apply traditional methods like bag of words (BoW) classification to classify these documents. This paper proposes a novel topic modeling model, FLATM, where each document is represented as a probability distribution over a finite set of topics. To preserve the interpretability of our model, we add soft (i.e., fuzzy) assignments of topics to the documents to make them more interpretable. This is achieved by performing topic assignment in a fuzzy way. Furthermore, a novel proposed representation of medical documents is proposed, which is a map from raw text to a set of topics using fuzzy logic. These topics are self-expressive, and can be represented using a pre-defined compact notation. Experimental results show that our model outperforms several state-of-the-art topic modeling methods.
    **Korean-to-Chinese Machine Translation using Chinese Character as Pivot Clue**
112. This paper describes a technique for Korean-to-Chinese machine translation which uses Chinese characters to position the bilingual dictionary to enhance the accuracy. We use the cost matrix of the bilingual dictionary as pivot. As we have not seen previous Chinese-Korean machine translation technique, we have tried to use the heuristic knowledge to improve the translation accuracy. The experiments on the Japanese-to-Chinese and English-to-Chinese machine translation datasets show that our method can be further improved when using character as pivot.
    **Outbound Translation User Interface Ptakopet: A Pilot Study**
    In this article, we describe our first experimental results on translating the 'outbound' financial-service interface in Thailand into Bangla (Moken). This task is based on the Bangla karana - the basic unit of Thai language. It is performed by translating the text into a language that is well-suited for the Bangla karana. In order to define the correct index for the Bangla karana, a statistical index was used to find the best translation. We have implemented our text-to-rank model on a small study of the Bangla karana. We perform our pilot experiments using a set of 4 popular systems to translate the text into Bangla. After our results are discussed, we suggest improvements for our system for other Thai language systems.
113. **SWift -- A SignWriting improved fast transcriber**
    Sign writing, i.e., reading and writing an English word by watching a script, has a long history in thecient world and still remains as a man-made invention.The sign writing used in daily life is transcription and consists of formants, words and lines. It's the main hurdle to automatic speech recognition (ASR).So far, ASR systems use a separate transcription engine, which is used before the ASR system. Nevertheless, the type of sound that the human makes when he signs a word, i.e., velocity and stridulation is almost the same with ASR systems. So, we try to convert this speech of the human into a waveform and send it directly to the ASR system. In this research, the non-linear waveform transform (short-time Fourier transform) is proposed to save time, speed and accuracy. To verify the proposed method, a popular sign writing method called SWIFT is used. Furthermore, different types of strings (newspaper, salt-and-pepper, sign, colored, silhouettes, Morse code, handwritten lines and handwritten words) are used for a target language such as English. With the proposed method, ASR system can use these text strings and perform a language-independent word-classification. Experimental results show that with this method, the ASR system can achieve a certain 90.67% rate in word-classifying written English text, i.e., up to 98.67% for a set of 30,000 word-classifying ASR utterances. This method can be used as the new type of ASR system.
    **Recency predicts bursts in the evolution of author citations**
114. We predict abrupt drops in author citations in online language, compared to normal spans of time. We define these drops as punctuated clicks, with a natural cutoff value. Existing methods use similar features to predict these short-duration clicks, including normalized cross-entropy, normalization, and a new attention based on conditional random fields. By taking into account recent statistics of the data, we show that authors tend to self-select for the online nature of their texts, and utilize this in establishing these interruptions in author citations. To overcome the misspecification of cross-entropy, we present novel attention on text units (words), and develop new models to measure their impact on the prediction of author clicks.
    **A Vietnamese Question Answering System**
    This paper presents a Vietnamese question answering system. The system was developed based on simple sentence and word embedding methods. Besides, the system also uses semantic relation analysis. This approach ensures the performance of the system with a better matching precision and has proven effective in selecting and matching the most relevant information with low cost. Besides, this approach also works well in generating the personalized answer based on the previous question-answering context. In addition, this approach is achieved in the fast manner, requiring less memory and takes less time compared with the most of the systems which are developed by human experts. We tested our system using the publicly available test data from SemEval 2014 Task 3 (Personality and Influential Question Answering) challenge. The evaluation is based on questions which were already answered in the course of the evaluation. Compared to other systems, our system has proven to be more effective and efficient.
115. **Taking a Stance on Fake News: Towards Automatic Disinformation Assessment via Deep Bidirectional Transformer Language Models for Stance Detection**
    Distributed Machine Learning methods have gained a lot of momentum in this year, which allows researchers to distribute their computations across multiple data centers in a privacy-preserving way. Deep learning is a particularly promising and effective approach to apply Machine Learning methods for fake news detection. However, there is no one-size-fits-all solution for automatic fake news assessment, because many fake news articles are often poorly written. In this paper, we propose Deep Bidirectional Transformer Language Models (DBTDMLM) that learn to predict fake news stance from the text of a news article. In the proposed approach, we employ the popular Transformer model to generate a language model from an input text and a biased opinion from a target word. After that, we use the bias model as the feature representation of a Bilingual Language Model, and then we utilize word embeddings to classify the fake news stance. The experiments on the SemEval-2010 Task 1 Fake News Detection dataset show that our approach significantly outperforms the baseline methods, and the combination of our approach and the baseline systems gives a significant improvement in performance.
    **AIPNet: Generative Adversarial Pre-training of Accent-invariant Networks for End-to-end Speech Recognition**
116. Automatic speech recognition (ASR) has evolved with an increasing level of granularity and complexity in recent years, with many conventional deep learning (DL) architectures incorporated in an end-to-end fashion, and also with residual-based approaches explored for dealing with system variation in temporal dimensions. In addition, utterances in a streaming ASR system will naturally change over time due to ambient factors. These additional dynamics poses new challenges in end-to-end ASR, and deep learning architectures that are conditioned on a static and static training corpus, while being able to handle any current and future changes, become suboptimal. In this paper, we introduce AIPNet, a generative adversarial pre-training approach for end-to-end ASR. The main challenge in our approach lies in adapting the language model, language and utterance features in a robust manner. We investigate different variants of the language and utterance representation, and use generative adversarial learning to jointly train these components, with and without a language model, in a joint training setup. The proposed approach demonstrates superior performance in non-temporal conditions to the most recent conventional ASR systems in terms of both word error rate (WER) and word recognition accuracy (WER@10). Further, we show that this method also helps speech recognition systems to cope with temporal dynamic.
    **Simultaneous Neural Machine Translation using Connectionist Temporal Classification**
    In the current work, we develop a technique for learning synthetic parallel corpora with a single classifier. We call this method Connectionist Temporal Classification (CTC). Unlike previous approaches that construct synthetic parallel corpora through capturing dependencies in the output distributions of two distinct classes, CTC uses the network states of an arbitrarily coupled and approximately stationary neural classifier to learn the connectionist temporal rules needed to perform machine translation on unlabeled input. The proposed technique is evaluated on machine translation for Arabic-English and English-Persian pairings. A set of experiments demonstrate that the CTC approach yields a translation system that is both less noisy and grammatically correct than the corresponding system where the inputs are in turn trained on only pairs from a baseline corpus.
117. **Self-Attention Enhanced Selective Gate with Entity-Aware Embedding for Distantly Supervised Relation Extraction**
    Distantly supervised relation extraction aims at learning a mapping from textual triplets to entity-hypotheses in a relation database. Most of the existing work focus on automatically learning a relation relation instance representation from triplets and show promising results. However, they rely on noisy triplets with noisy triplets annotations. On the other hand, entity-aware relation representations still lack reliable entity labels for relation instances and achieve poor performance. To address these issues, this paper focuses on jointly learning relation relation instance embeddings with entity-aware relation features and jointly optimizing the embedding and relation instance representations via self-attention. The relation relation instance embeddings are learned by optimizing the relation relation instance label consistency, while the entity-aware relation embeddings are learned from triplets and entity-aware triplets in a latent space by leveraging the relation relation information. Experiments show that the proposed method significantly improves both the quality and the precision of relation relation extraction.
    **A Vietnamese Text-Based Conversational Agent**
118. This paper introduces the goal of building an agent for Vietnamese conversations, a language that has a multitude of pragmatic dimensions. We introduce the system of a new project called Contemporaneously translated conversational agent (CVC), which is working towards building an intelligent system for Vietnamese conversations. To develop such a system, we utilized large-scale data from Vietnamese wiki, along with the services of a professional Vietnamese chatbot, which are used to optimize the design of the system, including sentence detection and syntactic analysis. The development of the dialogue system was initially driven by a case study, which consists of about 45 sentences that we collected on a daily basis in a Vietnamese web chatroom and conversational platforms. The findings of our investigation allow us to sketch the outline of a prototype for the CVC system. This prototype consists of three modules, namely dialogue system, backend unit and manager. The dialogue system performs the task of generating the responses of each dialogue turn. The backend unit is used for analyzing the utterances and generating responses with the possibility of separating out the discourse segments from the linguistic items. The manager is used to control the dialogue turn based on an attention control mechanism. We present a prototype for the CVC system that will be publicly available for download.
    **ATCSpeech: a multilingual pilot-controller speech corpus from real Air Traffic Control environment**
    Knowledge-based controllers' communication policies are critical for ensuring the coherence, reliability and productivity of Air Traffic Management (ATM) systems. These policies contain various user-defined rules that require training from controllers' peers. Manual annotation for these rules are time consuming, expensive and infeasible, while automated methods have proved to be highly effective at this task. In this paper, we propose a novel text-based framework for labeling operational procedures. ATCSpeech is an Amazon Web Services (AWS)-based text-based corpus that includes English (English-CAN) and Spanish (Spanish-ES) recorded text. Each recording is accompanied by short analytical notes with analyses on the transcribed text. These analytical notes are followed by a taxonomy-based hierarchical labeling of each recording. We discuss the annotation process of the corpus, identify key mistakes that speakers make during recording and the key challenges that arise from the multilingual nature of the corpus. The texts of the corpus have been successfully used to train speakers in English (CAN) and Spanish (ES) language to be translators. However, to fully leverage the positive effect of this data set, further experiments are needed to improve the system, in particular to improve the quality of the generated translations.
119. **Natural Language Generation Using Reinforcement Learning with External Rewards**
    Natural language generation (NLG) is an inherently sequential task that can be effectively automated through reinforcement learning (RL). An RL algorithm generates continuous representations (training sets) of the target sentence in a way that directly maximizes the level of semantic similarity between the target and the training texts. However, the existing methods for NLG, including policies of reinforcement learning (RL), is typically difficult to generalize to task specifications that are not part of the training set. In this work, we propose a novel way of using supervised information, namely extractive sentence embedding, to support a NLG algorithm to learn to optimize its own reward function. Our experimental results show that our approach (i) allows the RL algorithm to learn a function that directly maximizes the similarity of the target sentence to the training texts, and (ii) significantly outperforms the existing RL algorithms for NLG.
    **A Large-scale Dataset for Argument Quality Ranking: Construction and Analysis**
120. Argument quality has been one of the focus areas of opinion mining research, with a multitude of tasks performed and datasets proposed. Argument quality ranking (AQR) is a promising task aiming to measure the quality of the arguments. Compared with other kinds of ranking, AQR is based on pairwise comparisons of argument pairs rather than on pairwise comparisons of argument utterances. In this paper, we introduce a large-scale AQR dataset, built on top of various corpora such as OPML, CRF, etc. Besides semantics of the arguments, it contains not only linguistic features but also two different kinds of algorithmic measures that are shown to outperform traditional machine learning-based approaches: term frequency, and relation length. Finally, we propose several AQR-related tasks, and report experimental results on the AQR tasks.
    **Integrating Relation Constraints with Neural Relation Extractors**
    Many recent works have proposed methods for ranking textual documents. However, these systems are often viewed as a black box, and are not easily interpreted or explained. In this work, we present a novel method for improving Neural Relation Extractor (NER) using existing knowledge from Semantic Web. We refer to our method as Natural Language Extraction with Symbolic Knowledge. In contrast to previous work on Natural Language Extraction, the proposed method does not focus on solving an encoding task, but instead focuses on learning a neural relation extractor from a large corpus. We evaluate our method on SemEval2015 Task 8, and show that our model performs on par or even better than previous state-of-the-art NER systems.
121. **Relevance-Promoting Language Model for Short-Text Conversation**
    Recurrence-based language models have gained popularity for their powerful modeling ability and better performance. However, models which do not have inherent relevance constraints, such as gender or location, generally perform poorly. In this paper, we study relevance-promoting language models, where a context model is implicitly built to encourage model performance in realistic conversation scenarios where the subject is important to the conversation. We introduce the scope model, which is built by enforcing the relevance of the current sentence and the discourse contexts of the subject. We evaluate relevance-promoting models in a short-text conversation setting and find that they significantly outperform word-based baselines. Additionally, relevance-promoting models can be easily integrated with existing language models to further improve their performance.
    **Neural Machine Translation with Explicit Phrase Alignment**
122. Neural machine translation (NMT) is a popular method for machine translation that relies on an encoder-decoder network. The decoder consists of a long-short-term memory (LSTM) network and a long short-term memory (LSTM-LSTM) network. LSTMs are extremely long-lasting units and can be trained end-to-end, improving accuracy of NMT. However, LSTM models suffer from poor decoding accuracy since the input to the decoder is extracted from the entire sequence regardless of syntactic structure. In this paper, we propose the explicit phrase alignment (EPAL) loss to remedy this problem. As the output from the decoder is usually the alignment between the phrase and the target sequence, we require the alignments of the phrase and the target sequence. To this end, we make use of phrase alignment resources such as the conventional phrase-based translation models (PBM), which are used in phrase-based machine translation (PBM-MT) and phrase-based statistical machine translation (PBM-ST). Experimental results demonstrate that the proposed approach significantly outperforms LSTM, PBM and ST with small experiments on 7 datasets, and achieves the state-of-the-art result on WMT14 English-German translation task with a relative word error rate (WER) of 5.4%.
    **Semi-supervised Bootstrapping of Dialogue State Trackers for Task Oriented Modelling**
    Speech-based social interaction systems are very popular in the context of planning human-robot collaboration. However, these systems require a large amount of social interaction data which is expensive to acquire. Also, the data quality is often found to be unsatisfactory, leading to unsatisfactory user experience. In this paper, we propose a novel model for conversational speech-based task-oriented modelling called semi-supervised bootstrapping, which leverages information on the social relationships to pre-train dialogue state trackers for various tasks. More specifically, we develop a novel network architecture for decoding multimodal streams of utterances through sharing and embedding weights for a set of language encoders. Through training these encoders using semi-supervised learning, a dialog system is automatically trained to predict different task-oriented labels given the state information of the dialogues. We validate our model on a set of benchmark human-robot collaborative planning tasks with very varied linguistic inputs such as speech transcripts, natural language question and answer texts, and raw-dialog voice inputs. Our experiments show that we can achieve significant improvement in accuracy over standard semi-supervised state tracking methods such as Gated Recurrent Unit (GRU) for the dialogue response generation task. We also evaluate the proposed model on a set of more general dialogue planning tasks using a mixture of unsupervised task-oriented models, i.e. systems without ground truth dialogues. Our results show that our proposed model can effectively utilize the power of deep learning to learn task-oriented dialogues, effectively augment dialogue state trackers, and learn a robust and accurate dialogue engine.
123. **Doc2Vec on the PubMed corpus: study of a new approach to generate related articles**
    Having the automatic extraction of text features useful for biomedical information processing is a worthwhile aim in medicine. While this aspect can be considered in its own right, another aspect, which has the potential of bearing important impact on knowledge extraction, is document similarity. In this paper, we are focusing on the question how much the extraction of data and model embeddings for related articles work. Towards this end, we study the impact of the proposed method on the authorship of retrieved articles and on the number of retrieved articles as a function of the coverage of an article. We find, that the extraction of model embeddings is helpful in the comparison of retrieved articles by their potential of being related articles, even when the articles are written by different authors.
    **A Time Series Analysis of Emotional Loading in Central Bank Statements**
124. Statements produced by central banks regarding interest rates, quantitative easing, and other monetary policies are generally quite short. To understand why these statements are macroeconomic forecasts, one must understand the relationship between central bank decisions and macroeconomic movements in other financial markets. Specifically, financial-press reports generally correspond to closing stock market spreads. While the associated time series of closing spreads (a.k.a. the "fear index") is not only irrelevant for the analysis of these macroeconomic forecasts, but may actually be misleading (to central banks), the extent of such misleading affect depends on how one interprets the false implications of closing spreads on the macroeconomic situation. Indeed, the empirical literature shows that many trading strategies are difficult to accurately model when the closing spread is used as a tool for model-based trading decisions. In this paper we examine different time series representations that make use of closing spreads, provide an assessment of these representations' ability to capture misleading effects, and explain why it is appropriate to view closing spreads in a context where other variables (such as market volumes or macroeconomic indicators) have the same predictive power as closing spreads.
    **A concrete example of inclusive design: deaf-oriented accessibility**
    Building and running an accessible computer assisted system are at the core of the reason why accessibility is necessary for those who have no capacity for understanding language. The implementation of such systems can be approached as a kind of design problem. In this paper, we define a concrete accessibility design problem, namely deaf-oriented. We illustrate this problem with a few examples of designed systems. Then we discuss its associated challenges and how to solve them.
125. **Automatic Generation of Headlines for Online Math Questions**
    We describe a simple approach to automatically generating headlines for math questions on the MathExchange forum. We leveraged machine learning and web search for similarities between the texts of articles and the labels of questions. We also describe the factors that influence the similarity between question and article text, and how this information is used to automatically rank questions. We hope that by sharing our efforts with other research groups and math educators, this approach will contribute to better learning materials for math learners.
    **Towards improving the e-learning experience for deaf students: e-LUX**
126. The e-learning experience is affected by its learner and the environment it interacts with. It is well known that individuals with hearing disabilities can benefit from e-learning and that e-learning experiences of deaf students are more positive than those of hearing students. However, most existing research has focused on one aspect of this interaction: e-learning experiences of students. In this paper, we propose to investigate the effects of e-learning experiences on individual students from different backgrounds and ages. We conducted a large-scale experiment in two locations: Abbotsford, BC, and Cairns, Qld, Australia. We collected surveys from 663 deaf students (Leavesmart 2012), 564 hearing students (Leavesmart 2013), and 535 hearing students and hearing students (Bao and Pollard 2013). Our goal is to understand the characteristics of these students and find whether students who interacted with e-learning were significantly more positive or negative or similar in the way they interacted with e-learning. We used logistic regression to predict e-learning experiences, and Pearson's correlation coefficient to evaluate the performance of the predictor. Based on this, we found that students in the second location were significantly more positive and able to improve their e-learning experience compared to students in the first location.
    **Zero-shot Chinese Discourse Dependency Parsing via Cross-lingual Mapping**
    This paper describes a multi-lingual computational framework for zero-shot discourse dependency parsing. The core component of the framework is a multilingual sentence encoder which maps sentences in different languages into a shared embedding space. This sentence encoder is also used to map discourse dependency proposals to sets of triples, which are used for reasoning over. This multi-lingual framework is used to learn discourse dependency as well as discourse triples. These discourse triples are used to train a zero-shot discourse dependency parser and at test time, zero-shot triples are retrieved and fed into the parser. Our experiments on the Chinese-English multi-lingual data set show that this framework outperforms state-of-the-art zero-shot models on zero-shot discourse parsing.
127. **Do Attention Heads in BERT Track Syntactic Dependencies?**
    We experimentally investigate the potential use of attention heads in BERT (Bidirectional Encoder Representations from Transformers) for sequence modeling. We find that attention heads of different contexts behave differently, and thus the model needs to learn different types of head representations. Based on these findings, we propose an attention shaping layer that can adaptively select which heads should be read at each time step in order to improve the perplexity, which can be added to existing BERT-based models.
    **SAMSum Corpus: A Human-annotated Dialogue Dataset for Abstractive Summarization**
128. One of the core issues in automatic summarization is to choose the most informative sentences. Nevertheless, building an effective extractive summarizer is still an open challenge. To provide new insights into how human beings compose abstractive summaries, we contribute a new dialogue dataset called SAMSum (for Summarization of Automatically-generated Argumentation Mention sentences), which consists of 1000 conversation dialogues between 110 people discussing basic aspects of a variety of areas. In our dialogue datasets, we select discourse-level argument pairs and randomly combine them into one argument. Experimental results show that the data enables novel insights into the ability of humans to summarize abstractive conversations.
    **NorNE: Annotating Named Entities for Norwegian**
    In this work, we introduce NorNE, a complete unsupervised annotation scheme for Norwegian named entities (NE). For each named entity in the text, NorNE generates a set of all possible direct and indirect named entity names using a hybrid system that combines a Chinese-based named entity and a named entity recognition system. This mapping can be obtained from knowledge bases to improve the accuracy of named entity recognition. Moreover, the resulting NE entities are used to enrich the semantic representation of the text in a simple way. The classification and parsing of NE entities is evaluated in a standard real world test scenario on the corresponding standard task of named entity recognition, which demonstrates the advantage of the proposed method compared to the state-of-the-art.
129. **SimpleBooks: Long-term dependency book dataset with simplified English vocabulary for word-level language modeling**
    Modeling Long-term dependencies of words is the main challenge of Natural Language Processing (NLP). In this paper, we propose a very simple two-layered neural network model (Simple Books) for language modeling task. We provide a word-level pre-trained embedding, named Simple Books, for English. And we also construct a vocabulary of Chinese words that can describe both long and short term dependencies in English. To train our model, we collect a very short-term labeled dataset (Short-Term Dependencies in English) in Simple Books and introduce a very simple set of concepts for word-level language modeling. With the help of these concepts, we introduce a simple RNN and LSTM layer for Long-term Dependencies of English words. We evaluate the performance of the Simple Books-generated pre-trained embedding of English words using different machine learning tasks. In general, we show that the Simple Books-generated embedding shows a better performance than the embedding generated by the traditional machine learning methods.
    **Jejueo Datasets for Machine Translation and Speech Synthesis**
130. This paper describes our ongoing work to create a corpus for natural language processing that we call Jejueo. Jejueo is a collection of around 9k fragments of English text, separated into idiomatic, semantical, and non-idiomatic sentences. Jejueo is intended to serve as a benchmark for cross-lingual machine translation and synthesis as well as a source for benchmarking acoustic models. This paper is under consideration for acceptance in TPLP.
    **Sideways Transliteration: How to Transliterate Multicultural Person Names?**
    Multilingual names are similar in more ways than one. This paper attempts to quantitatively compare the utterances of different languages in a name transliteration task. To this end, we present a method to create a large amount of transliterated name pairs by combining bilingual lexical databases. After generating these pairs, we study their similarity to the English translation. We find that (a) all languages produce translations similar to the English, and (b) when two languages have a similar amount of transliterated name pairs, they will be more similar to the English translation when transliteration is done in the two languages.
131. **word2word: A Collection of Bilingual Lexicons for 3,564 Language Pairs**
    We present word2word, a large collection of bilingual lexicons for English, Italian, and Spanish, consisting of 1,154,766 words in 6, scraping pairs of languages. This is much larger than comparable data for other languages. To make our collection more human-readable, we have crafted statistical correlations between the distributions of the words in the lexicons. This information will help the reader quickly get a sense of which word types are similar to which in each pair, as well as which types are more similar to each other than to others. We further show that word2word serves as a valuable resource for training and evaluating NMT, as well as for the development of better NMT architectures. Finally, we use it to provide a step-by-step method for obtaining bilingual speech recognition software for training and evaluation of pre-trained language models.
    **JEC-QA: A Legal-Domain Question Answering Dataset**
132. The web is the richest source of information available, yet questions can elicit more insights than traditional answers. Questions also allow for large quantities of data to be collected and processed, but to what extent is unclear. Given the popularity of legal questions and the difficulty of answering them, it is essential to evaluate the performance of existing QA methods and identify those that are better suited for this task. To this end, we created the largest QA dataset, JEC-QA, to study QA performance in both the academic setting and the real-world legal domain. We collected questions from the New York Supreme Court decisions of 1980-2015, then created a baseline model that runs on a specially-trained neural model. We then expanded this model to handle questions of the kind routinely seen in practice and created a QA dataset with real-world questions. We then evaluated two baseline models on this dataset, one of which is a convolutional neural network and one of which is a semi-supervised learning model, and show that the semi-supervised model achieves a BLEU score of 9.3 on the test set, compared to a baseline of 7.4. In the academic setting, we obtain a very similar result (7.4) that has held consistently for all models over time and across all models. We also analyze which kinds of information the different QA models capture, and find that the question type is a better predictor than the answer type.
    **GitHub Typo Corpus: A Large-Scale Multilingual Dataset of Misspellings and Grammatical Errors**
    In this paper we describe the GitHub Typo Corpus (GTC) containing 250, Riftrecht-Greule-style, German-English, graph-structured texts annotated with correct vs. misspelled and grammatical errors, as well as WordNet-oriented pairs annotated with misspellings. We provide baselines on the task of predicting the fixations, inflections, and paradigmatic verbs of graphs with and without perfect grammatical correctness. We introduce the challenges that arise in our work and provide a baseline solution.
133. **Poq: Projection-based Runtime Assertions for Debugging on a Quantum Computer**
    It is becoming more and more important to test whether a quantum computer could help to improve the speed of algorithms. On this topic, we propose a framework Poq which is very close to the framework of Desai et al. (2010) to overcome the limitations of the existing framework, while maintaining the state-of-the-art accuracy of O(1/k) for one-step confidence. It allows for compiling all operations of a quantum algorithm, including analogies and minibatch quantum updates, to a standard program with at most k pairs of point-wise quantum operations per iteration, guaranteeing consistency with sublinear runtime runtime (for the same model). It also allows the creation of quantum optimisations, as well as a new evaluation metric (called stability) of quantum correctness, improving the performances of classical machine learning models on quantum hardware. Our approach could have a huge impact on machine learning community and furthermore, opens up a new way to construct quantum-enhanced learning algorithms.
    **Multimodal Machine Translation through Visuals and Speech**
134. In this paper, we propose a multimodal machine translation system that leverages both visual and audio inputs. Our system transfers representations between different modalities by using a 2-step process: by aligning audio and visual signals using a fusion network, and by utilizing a joint cross-modal attention module. On the English-to-German dataset, our system achieved a BLEU@4 score of 30.1 on an unseen test set, which outperforms previous multimodal methods and our human baseline by 10.1 points. Moreover, our model also obtains a significant performance gain on a large English-to-French dataset, demonstrating the applicability of this method to multimodal machine translation tasks.
    **A Fine-grained Sentiment Dataset for Norwegian**
    When building a sentiment analysis system, it is necessary to analyze the texts along with the sentiment. However, the sentiment data in Norwegian is of a very fine-grained level, where fine-grained sentiment does not exist yet. To address this, we propose a fine-grained sentiment dataset named NORFASTE for Norwegian with POS tags and opinion labels. The collected information includes dialect, gender and first-person pronouns. The sentiment data for Norwegian are from ICVN, an Opinionated Corpus from CONVENTOR (2015). To deal with the complex data in fine-grained sentiment, we propose the fully connected neural network, FCN for Norwegian POS tags, to perform the sentiment classification and emotion analysis, and we also use LSTM to capture the relation between emotion and word. Our experimental results on the new dataset show that the performance of the proposed methods are not far from the high level of the POS tags.
135. **A New Corpus for Low-Resourced Sindhi Language with Word Embeddings**
    This paper presents a new Low-resource Sindhi Corpus. It contains entries for five classes: rule, context, indication, verb, and head. These classes are important for low-resource NLP tasks including machine translation, named entity recognition, emotion detection, morphological disambiguation and dependency parsing. This corpus is made publicly available under the open-source license. This paper presents the corpus in detail and discusses the features and challenges of this language.
    **Emotion helps Sentiment: A Multi-task Model for Sentiment and Emotion Analysis**
136. A plethora of models for sentiment analysis and emotion analysis have been proposed in the past two decades. While deep neural network models for sentiment analysis have shown a clear improvement in many cases, there are still two open questions. First, how does sentiment help emotion? Second, how can we improve the performance of existing sentiment and emotion analysis models? In this paper, we propose an ensemble-based neural network model that jointly learns both sentiment and emotion. The first task is to learn multi-modal embedding space by exploring the content of sentences. The second task is to apply local sentiment analysis to improve the performance of the emotion lexicon. Experimental results show that the proposed model outperforms the state-of-the-art emotion-level sentiment analysis methods in terms of accuracy.
    **KPTimes: A Large-Scale Dataset for Keyphrase Generation on News Documents**
    Keyphrases provide key information for a given document. Building large-scale datasets for keyphrase generation on news documents is challenging. However, the availability of news documents on the web was supposed to promote research in keyphrase generation. Unfortunately, existing news documents do not contain enough news keyphrases in order to build large-scale datasets for keyphrase generation. To address this issue, we propose KPTimes, a large-scale news keyphrase dataset. KPTimes contains 5 million news articles and 6 million keyphrases with a total of more than 1 million news instances per document. We have selected the keyphrases of news articles that are most similar to the news keyphrases generated by external sources. KPTimes is freely available for research purposes, and has the potential to be a good starting point for future research on keyphrase generation on news documents. We demonstrate the effectiveness of KPTimes on the development of several keyphrase generation models.
137. **Improving Neural Relation Extraction with Positive and Unlabeled Learning**
    In this paper, we address the task of relation extraction from text. We investigate two different methods of leveraging unlabeled data for relation extraction, each to separate sentences in the same paragraph. The experimental results show that, compared to a strong baseline, both of these approaches significantly outperform the baseline, indicating the advantages of unsupervised learning on relation extraction. The proposed methods can be applied to large-scale corpora to learn a latent factor model, which is potentially used for downstream tasks, such as summarization and question answering.
    **A probabilistic assessment of the Indo-Aryan Inner-Outer Hypothesis**
138. The Indo-Aryan Inner-Outer Hypothesis (IAP) asserts that Indo-European languages belong to a single Indo-Aryan language family. This paper investigates the capabilities of recently-introduced Bags-of-Words models (BoWs) on the IAP task. We show that an implementation of the same model can recover Indo-Aryan word boundaries from a single grapheme or a few cursive marks in the wrong place, even when there are small amounts of additional annotated text (as little as 8K tokens) to be analyzed. This knowledge improves the detection of Indo-Aryan languages that are outside the IAP.
    **Sentiment Analysis of German Twitter**
    In the last decade, social networks have become highly important media platforms, in order to express and share opinions and opinion sentiments. As a general rule, conversations are allowed and public opinions are displayed as the outcome of a conversation in a public space, in such as a restaurant restaurant or in a market. Sentiment Analysis (SA) is an important branch of Natural Language Processing, and it aims to estimate, estimate, estimate sentiment of a text. The main contribution of this paper is to apply these techniques to German social media, namely Twitter. This has been made possible through access to three private data sets, as these were released as part of research activities in order to support researchers working in sentiment analysis. This work is made possible through the use of the developed tools available in TUM and the cooperation of the operators of the projects for which they developed. The quantitative results of the described research are presented, such as the evaluation measures and the results of the estimation methods. All these are illustrated in the following tables:**
139. Twitter social media datasets: Tables and Methods Twitter social media datasets: Tables and Methods
    **A Multi-cascaded Deep Model for Bilingual SMS Classification**
    This paper presents a multi-cascade deep model for automatic SMS classification (Multi-Deep Classifier-MC). The three models of Multi-Deep Classifier-MC are: 1) the Sentence Decoding Network (SDN), which converts the sentence into a weighted number of phrases in the form of Sentence Attention Recurrent Neural Network (SARNN) and a Bag of Ngrams (BoN), 2) the Recurrent Attention Network (RAN) that uses text comprehension as its recurrent top-down attention mechanism to classify the sentence, and 3) the Phrase Encoder (PE) that converts the sentences into phrases and applies the same decoder network to the encoder output and the Sentence Attention Recurrent Neural Network (SARNN) to the decoder output of the sentence. Experiments are performed over the well-known IAM dataset, showing that the three models achieve consistent improvements over the existing method (i.e., Multi-Sentence Markov Model, Multi-Sentence Sequence Model, and Sentence Decoding Network) as well as previous state-of-the-art methods (i.e., Sentence Decoding Network and Sentence Decoding Network).
140. **Kurdish (Sorani) Speech to Text: Presenting an Experimental Dataset**
    Sentiment analysis is a crucial research problem for many applications such as marketing, sentiment analysis of reviews, and customer support. In general, lexical and semantic information of online user-generated content is always helpful for detecting sentiments of users. One popular way to obtain such information is by reading texts such as news articles and discussion forums. However, there are two main challenges when using such online documents: First, the selection and extraction of relevant data is very time consuming. Second, the systems developed in these researches usually don't take opinion towards the new content into account, which is a critical issue for social media users. In this paper, we present a novel dataset for analyzing the opinion of people towards content of Turkish language online. The dataset contains news articles related to news related topics (links are collected on Twitter and Meitelli blog. These articles have been collected and published for three semesters in 2015 and 2016, and they contain 3.2 million and 53.6 million users. These users generate 1.7 billion and 1.3 billion sentences over the three semesters of the dataset. All datasets are made publicly available to encourage further research.
    **Neural Chinese Word Segmentation as Sequence to Sequence Translation**
141. We propose a neural Chinese word segmentation model to exploit word-level statistics of Chinese characters. Our model is different from previous word segmentation methods in the following two aspects. First, our model leverages character-level statistics instead of word-level statistics, which can capture the long-term effects of contextual information and long-range correlations, respectively. Second, we design an adaptive sequence to sequence model to capture both word-level and character-level statistics simultaneously, as it significantly increases the probability of word-level statistics. Based on these two properties, we design an end-to-end model that can generate word segmentation results by sampling character level distributions. Experiments demonstrate that our model outperforms previous word segmentation methods on our training set.
    **Tag Recommendation by Word-Level Tag Sequence Modeling**
    This paper presents a system to recommend tags that could be added to an arbitrary tag list in order to improve the tagging accuracy. The proposed system utilizes the temporal information encoded in Word-Level tag sequences. Moreover, the tags proposed by this system are free of extraneous tags. A single recommendation system, called Tag Recommendation by Word-Level Tag Sequence Modeling, can recommend tags that could be added to an arbitrary tag list as well as to any other tag list of any tag list-based system. Experiments conducted with different experiments show that our system can recommend tags with more accuracy than the recommended tags from any other tag recommendation system.
142. **Automatic Creation of Text Corpora for Low-Resource Languages from the Internet: The Case of Swiss German**
    In this paper we present an automated text generation system for creating low-resource corpora for Swiss German, a low-resource language. Our system involves three steps: 1) automatic word segmentation, 2) category-dependent language modeling, and 3) decoding based on compositional embeddings. The initial results suggest that category-dependent language modeling, rather than word segmentation, could be more efficient for generating low-resource corpora for Swiss German. Furthermore, we show that multi-lingual word embeddings learned on synthetic data are more robust for Swiss German than single-lingual word embeddings learned on real-world data. Thus, given a pair of synthetic and real data, we show that our translation system is more robust to the fact that Swiss German is a very language-dependent language.
    **Modeling German Verb Argument Structures: LSTMs vs. Humans**
143. We consider the task of modeling verb-argument structures from German text. Verb-argument structures appear in many natural language processing applications, including in computational semantics, logical positivists and syntactic parsing, among others. We present a neural network model of verb-argument structures, which is trained using supervision provided by humans, and tested on these structures. The model outperforms human performance on some corpora, although the underlying structure of the data sets is different, both in dimensions of structure type and context. The model also demonstrates the ability to describe the structures in a human-understandable way, making it a useful approach for the design of computational models of verb-argument structures.
    **Assessing the Robustness of Visual Question Answering**
    Visual question answering (VQA) has been a hot research topic for several years, in which artificial intelligence has shown its competence on various tasks. Previous studies have shown that the robustness of VQA systems is mainly determined by the similarity between the questions and answers. In this paper, we examine the robustness of VQA systems on three question sets (Question-Answer-Fiction) of the CLEVR dataset, in which there are multiple correct answers. In contrast with previous work, we introduce a set of non-blind answer set techniques to create pseudo-answer sets for evaluation, in which the answers are more likely to have no incorrect answers than to be correct. To cope with the inconsistency of our pseudo-answer sets, we employ a new measure to estimate the uncertainty of answers. To assess the robustness of VQA systems on these three question sets, we report the improvement of the systems' performance for each question set, using two different sets of performance metrics. The results indicate that VQA systems are significantly more robust than previously reported, and that all question sets are not equally reliable. The same answer set methods can also be used to measure robustness of VQA systems for multiple answer sets.
144. **Generalizable prediction of academic performance from short texts on social media**
    We consider the problem of predicting a student's overall academic performance in a given university. We consider a supervised prediction task, in which the prediction model is trained only on a set of short texts labeled as academic news reports, written in English by students themselves. As part of the learning process, we design a bi-attention sequence-to-sequence model, to learn the temporal correlation between academic topics and important responses by students. We also propose a novel feature, named "Focused Attention", which can be explicitly represented as a student's attention on her/his chosen topic. Experimental results on the largest news dataset available for this task demonstrate the performance of our model and a number of other competitive baseline models.
    **Merging External Bilingual Pairs into Neural Machine Translation**
145. Language acquisition is the process by which a machine learns to translate a natural language text into another language. This paper investigates the task of multimodal language acquisition, in which a machine learns to translate one or more languages into another language, such that all the output sentences are generated from the same source language. We focus on the task of translating from English to Chinese (or other languages), in the setting where these languages have their own pair of bilingual machine translation corpora. To solve this problem, we introduce a crosslingual pairing-aware attention mechanism that selects source sentences that are most relevant to the target language pair. Experiments demonstrate that the proposed crosslingual pairing-aware attention leads to significant improvements in the translation performance of neural machine translation, especially when the target language pairs have little bilingual training data.
    **HSCJN: A Holistic Semantic Constraint Joint Network for Diverse Response Generation**
    In many real world applications, a conventional dialogue system requires diverse response generation, which aims at producing diverse response conditioned on previous responses. In this paper, we present HSCJN, a novel holistic semantic constraint joint network (HSCJN) framework for addressing this problem. HSCJN can effectively and efficiently learn the mapping between the predicate-argument structure and the natural language of current dialogue history, i.e., the history of previous generated responses, to address the problem of diverse response generation. Specifically, we introduce a highly-capacitated sequence of language units (NU), namely nU_i, to model the semantic and syntactic similarity between words in a sentence. For a given NU_i, HSCJN jointly utilizes its context and latent embeddings to predict both the probability and probability distribution of each individual sentence. In addition, we formulate a Dense Relation Propagation algorithm to deal with noisy and incomplete data by reusing the context and nU_i, and further extract and encode the continuous vector representations of the discourse relations to provide an overall sentiment model for generating diverse responses. The proposed framework is evaluated on the multi-domain collaborative filtering (MCF) dataset and experimental results demonstrate that the proposed framework produces better quality diverse responses than baseline models.
146. **Machines Getting with the Program: Understanding Intent Arguments of Non-Canonical Directives**
    Statements in canonical (i.e. informal) directives have strong implications, which can justify their grammaticality if they follow from the expected meaning of the directive and its context. However, such statements have very little, if any, explanation in the sense of formal semantics, because they are non-canonical. Previous works attempt to solve the problem by creating (roughly) grammatical proofs for all possible interpretations of the directives (which might be incorrect). For non-canonical directives it is impossible to get a justified form of proof for all possible interpretations of the directives. We present a method for obtaining valid explanations for non-canonical directives from their context using linguistic parsing. We also extend this method to help explain why non-canonical directives can be interpreted well.
    **Integrate Image Representation to Text Model on Sentence Level: a Semi-supervised Framework**
147. Recent research shows that convolutional neural networks (CNNs) can be successfully applied to text classification tasks. In particular, it is well-known that the classification accuracy of these methods is critically dependent on the data size. In this paper, we aim to capture information from the sentence text by integrating a CNN model into a hybrid text classifier which has been proven to be effective in previous research. Further, we apply the semi-supervised framework, in which the classifier shares part of its parameters with the CNN model. Based on the learned relationship, we further build a local region-based recognition model, which combines region-based classifier and classifier model to maximize the correlation between the texts. Experimental results on four large-scale image datasets show that the proposed method outperforms the traditional methods by a large margin.
    **Semantic Enrichment of Streaming Healthcare Data**
    Patient data streaming is increasing rapidly with big data as the main focus. In order to extract useful information from these patients, an essential task is to enrich it with entities known and unseen in the stream. There is a need for machine learning approaches to analyze such streams which can be applied to discover important patient information. For applications like clinical decision support, where only patients are of interest, it is especially important to detect hidden patterns in patient data. As for the historical patient data which is collected in clinical hospitals, it has been possible to mine data insights using machine learning methods, however, the problem is challenging when an already collected patient data stream includes both temporal and spatial information. In this work, we propose a novel dataset based on medical diagnosis data which we use to measure the performance of two machine learning models: recurrent neural network (RNN) and convolutional neural network (CNN). Then we compare both the extracted features and features of unseen data in the stream. In particular, we define a novel concept called entity matrix (EM), a property that is defined over all the observations of the stream and is used to extract information from all the observations in order to build a semantic matrix. The experimental results show that our method performs better in the tasks of triple classification and label prediction while also detecting hidden patterns.
148. **Neural Academic Paper Generation**
    To efficiently generate academic papers, researchers typically need a large pool of paper proposals. For instance, once a year, members of a publishing committee at a major university typically review and possibly reject over 10,000 manuscripts, with only a fraction of these being accepted. To accelerate this process, we develop a neural neural network architecture (DQN) for automatically generating abstractive, high-quality academic papers. To learn the abstractive abstractive characteristics of academic papers, we introduce DQN-conditional Residual Networks, trained on academic papers with gold stylistic labels. Unlike existing neural abstractive models, DQN-conditional Residual Networks have three main advantages: 1) they can capture fine-grained features from academic papers; 2) they learn to generate coherent abstractive papers, and 3) they have fewer parameters and can scale to large datasets. We show that DQN-conditional Residual Networks are comparable to, and in many cases outperform, the best existing academic paper generation models. More importantly, DQN-conditional Residual Networks automatically generate abstractive, high-quality academic papers, thus demonstrating the practical benefits of neural abstractive models for academic paper generation.
    **Unsupervised Inflection Generation Using Neural Language Modeling**
149. Unsupervised word-level morphology induction remains a challenging problem. Most previous work only deals with one or two types of inflections, which are either ungrammatical or not morphologically proper. In this work, we propose a novel method to induce ungrammatical inflections by exploiting the shared latent word-level representations in a multilayer autoencoder. Our method achieves competitive results on four languages, outperforming the previous state of the art ungrammatical induction methods.
    **An Annotated Dataset of Coreference in English Literature**
    In this work, we present an annotated corpus of coreference in English literature. The corpus is a collection of 5,1993 books published in English between 1500 and 1860 and was manually compiled by academic experts with a specific interest in coreference resolution. The corpus was processed using standard POS taggers and was manually annotated using a term frequency-based technique. The corpus is available for public access at: https://github.com/Skrm23/coreference_test_cat
150. **See and Read: Detecting Depression Symptoms in Higher Education Students Using Multimodal Social Media Data**
    Research has identified that a substantial portion of people diagnosed with mental illness seek treatment for their mental health disorders from school. With the growth in the number of adults diagnosed with mental illness, the need for these individuals to use smartphones for social media communication is increasing. Additionally, social media platforms such as Twitter and Facebook provide unprecedented opportunities for mental health diagnosis. This paper presents a study of two major features of mental health from social media data. The first feature is the number of users' tweets and Facebook posts that contains symptoms of depression. We take two sets of this feature - Twitter and Facebook posts - and train classifiers to detect the presence or absence of depression in this data set. The second feature is the number of years a person has been in school. We use text mining methods to extract text-based features from student social media posts, and present these features to a machine learning classifier for text classification. We present a comparative analysis of these features for training and testing text classification models and show that a simple word vector is able to outperform other state-of-the-art features.
    **Deep Bayesian Active Learning for Multiple Correct Outputs**
151. We consider the problem of active learning to select the number of queries for an active learning algorithm that is built to identify multiple correct outputs, in the model where the statistical dependencies are known in advance. We develop a general formulation of this active learning problem in terms of multiple output estimation, and derive a corresponding batch active learning algorithm. We prove that this algorithm, named Active Bayesian Active Set Selection (ABAS), is sample-optimal for the observed complexity, where the total number of queries, sampling algorithm, and update rule are revealed by the statistical dependencies between the sets of outputs and the target variables. We demonstrate the efficacy of the proposed ABAS algorithm on a set of real-world data sets.
    **Fiction Sentence Expansion and Enhancement via Focused Objective and Novelty Curve Sampling**
    We propose the Focused Objective and Novelty Curve Sampling (FO-NER), an algorithm for automatically generating new relevant texts using a set of sentences from an unknown text corpus. By focusing on the different aspects of different text genres, the proposed method generates sentences that are complementary to existing text genres, and at the same time is guaranteed to preserve the metadata of the original texts. We apply FO-NER to a fiction domain dataset, and illustrate the effectiveness of the approach. We show that the novel text generated by the proposed method is significantly more accurate and better suited for generating further text genres, and in doing so, we gain new perspectives on knowledge discovery from massive text corpora.
152. **Leveraging Contextual Embeddings for Detecting Diachronic Semantic Shift**
    Diachronic semantic shifts of text are one of the most prevalent challenges of natural language processing. While powerful multi-dimensional embedding learning methods are studied, their training and inference are computationally expensive for large data and deep learning models. In this paper, we propose a novel and scalable model to detect changes of semantic relatedness with respect to historical context. Our model improves the recall performance from 0.64 to 0.86 in detecting the semantic shift from 9 text collections with the original weights to 9 text collections with the contextual weights. With contextual embeddings, our model is also robust to the context of a document's content. Experimental results indicate that the contextual embeddings are useful for semantic shift prediction and the proposed contextual embedding-based multi-dimensional embedding model is applicable for large scale natural language processing tasks.
    **Learning Word Ratings for Empathy and Distress from Document-Level User Responses**
153. One of the key characteristics of Artificial Intelligence is its ability to learn from, interpret, and improve upon the abilities of humans. To have successful Empathetic Intelligence systems, one crucial aspect is to correctly identify and describe emotions from texts. Although many sentiment analysis tasks have been studied, including sentiment classification from Facebook posts, most of them have focused on papers with big and noisy corpora. However, since online social media platforms have been introduced, the amount of public user written texts has been increasing rapidly. In this paper, we propose a novel text-based Empathetic Emotion Analysis dataset, named FB30k-ESPE. Based on the original annotations, we conduct experiments to classify each sentiment sentiment from FB comments into three sentiment groups based on person's emotions: 'neutral', 'angry', and 'affect'. In order to successfully identify emotions, we collect a large number of annotated text-based ratings from real users. Based on the ratings, we propose three classifiers to predict the emotions of a person from the annotated text-based ratings. The experimental results and word ratings from real users demonstrate that our dataset is effective in classifying emotions and emotion polarity from text data.
    **BLiMP: A Benchmark of Linguistic Minimal Pairs for English**
    Similarity between language pairs is an important aspect of computational research in natural language processing. One of the important subtasks in this area is the computation of minimal pairs of words. In this paper we describe an ongoing project which seeks to develop a benchmark of such pairs, using the language pairs from the freely available TextUnpacker v1.9.0 dataset. In this paper we describe two evaluation protocols which we use to assess the similarity of pairs, and show that in both cases, the proposed methodology performs very well.
154. **ZuCo 2.0: A Dataset of Physiological Recordings During Natural Reading and Annotation**
    We present a new dataset (ZuCo 2.0) designed for physiological studies of reading. The ZuCo 2.0 dataset consists of 5 minutes of recording of 59 participants reading a passage (English) and an annotation of each question with 14 performance measures (translation accuracy, logical reasoning, reading comprehension, readability, and formality). A detailed analysis of the dataset, together with a study of results with non-parallel corpora, demonstrates that the presence of natural reading can help understand passages. Furthermore, we show how a full-scale speech analysis approach can be used to improve reading comprehension, showing that audio annotation is a practical alternative to large-scale corpora.
    **Morphological Tagging and Lemmatization of Albanian: A Manually Annotated Corpus and Neural Models**
155. We present a new morphological tagging task and a new Lemmatization Dataset (LLE) of Albanian, available as a part of the Ello 2018 evaluation campaign. We also present and analyze two new, limited-vocabulary neural models for this task, namely JGG+ and SCL-Nets.
    **TutorialVQA: Question Answering Dataset for Tutorial Videos**
    With the rapid progress of image and video recognition in recent years, the usage of video content has become common for student learning. However, the traditional methods to achieve the goal of video question answering are limited due to the difficulty in producing a large video dataset. To help this goal, we present a novel Video Question Answering dataset of 10,468 human-human videos from UvA study website. The problem of video question answering is to accurately answer a series of questions about an image. Our dataset can be used to evaluate the quality of image features, identify the content that can be recognized, and decide how to improve the quality of video features. We introduce a benchmark method to answer the question based on automatic question answering. To the best of our knowledge, this is the first large-scale video question answering dataset. Moreover, we introduce a method that combines frame selection, information retrieval and video resolution for learning the visual information. In this paper, we extensively describe the dataset and the method to train and evaluate the model. The question answering results show that we can obtain a reasonable score from our benchmark method. Finally, we show the qualitative and quantitative results of the videos.
156. **Dynamic Prosody Generation for Speech Synthesis using Linguistics-Driven Acoustic Embedding Selection**
    In this paper, we present the Linguistic-Driven Acoustic Embedding Selection (LDE-ASE) algorithm for automatically generating acoustic embeddings for spoken language synthesis. LDE-ASE effectively selects a set of statistical model parameters which suitably combine the acoustic features of speech in a way that leads to a more accurate and realistic generated speech. The experimental results show that LDE-ASE performs well on the CoNLL-2003 data set and generates better and more natural speech in comparison to several baselines. Moreover, LDE-ASE successfully produces phoneme based acoustic embeddings which can be used for speech recognition applications.
    **SemEval-2016 Task 4: Sentiment Analysis in Twitter**
157. Social media are essential for political opinion expression and have been used by pro- and anti-government forces for generating counterfactual narratives, or narratives that suit the public mood and perception of the government. In this paper, we describe our participation in SemEval-2016 Task 4 - Sentiment Analysis in Twitter. We represented a system consisting of several components, namely, sentiment classification, anti-emotion classification, and counterfactual-prediction models, and proposed a comprehensive set of evaluation methods for them. Our system won the first and second places, for anti-emotion classification and counterfactual-prediction respectively, in Subtask A.
    **SemEval-2016 Task 3: Community Question Answering**
    Many questions and answers can be answered with the help of a summary provided by the majority of the community. We propose the task of inferring the community of a text article as a testbed for such understanding. This task was inspired by the work of Communities of Smart People (CoSP). We use a large dataset of English articles on a popular Web forum, and train a multi-layer neural network based on document and community features, and community entities. These features are then used in an ensemble method, where the ensemble weights are shared among the three parts of the model. Our models outperform three competitive baseline models in the community question answering (CQA) task. Further analysis and discussion reveals differences in community composition and specific topics that affect performance.
158. **HAMNER: Headword Amplified Multi-span Distantly Supervised Method for Domain Specific Named Entity Recognition**
    Entity recognition is the task of identifying and classifying named entities in text. The recently introduced Deep Named Entity Recognition (DNER) method makes significant progress by leveraging the rich source of information extracted from documents and labeling them with automatically constructed DNER features. While DNER does not suffer from model misspecification as it is built on top of the well-established WordNet ontology, recent work has shown that word sense disambiguation (WSD) is a viable alternative to DNER. WSD entails learning the functions to identify different word senses, and then using these functions to build the shared contextual vector representation that is used for matching across domains. Although existing methods are mainly designed for a single language, we show that pre-training on documents of multiple languages can improve the performance of DNER on languages that do not share a common shared space. Specifically, we show how a bidirectional autoencoder based method built on multi-tagging methods can be used for pre-training in a multi-lingual setting and how domain adaptation can be used to reduce the performance gap between DNER and WSD. Experiments conducted on two different language sets show that pre-training on documents of multiple languages can lead to significant improvements in performance compared to the state-of-the-art.
    **Integrating Whole Context to Sequence-to-sequence Speech Recognition**
159. This paper introduces a framework, which integrates a new sequence-to-sequence model for automatic speech recognition. A proposed decoding framework, which is inspired by the context-encoding scheme in NMT, exploits the dependency relations between input tokens and recurrent state feature maps, while taking dependency relations into consideration as well as reconstructing input words by considering recurrent state feature maps at each time-step. The motivation for this framework stems from an observation that any word following a context token should have the same semantics, but the dependencies among token contexts are not consistent with this consistent semantics. Therefore, our proposed decoding framework exploits the dependency relations between input tokens and recurrent state feature maps and reconstructs each input token by considering a recurrent state feature map. This paper also introduces a new vocabulary size distribution for efficient decoding of NMT systems. Experimental results show that our proposed framework outperforms a competitive baseline NMT system by a large margin.
    **Acquiring Knowledge from Pre-trained Model to Neural Machine Translation**
    Pre-trained neural machine translation (NMT) has shown state-of-the-art performance on many language pairs in recent years. However, the cross-lingual transfer between NMT systems, i.e., learning a pair of NMT models from parallel corpora with different languages, is still a topic of research. Most of existing approaches employ back-translation (BPT) and leverage pre-trained NMT models for cross-lingual transfer learning. However, BPT introduces a new problem of generating translation probability distributions in all-words of source sentence, which affects the quality of BPT. As a result, BPT-based approaches only change the embedding vectors for source sentence and the target sentence. In this paper, we propose a novel approach for cross-lingual transfer learning by leveraging a set of pre-trained NMT models and decoding their outputs directly in the target language pair. Our method adopts a framework in which pre-trained NMT models are firstly employed to transform source sentence into a target language embedding space, and then a character-based bilingual dictionary is introduced to automatically learn a set of bilingual dictionaries for both source and target language pairs. Experiments on multiple language pairs (e.g., English-German and English-French) show that our approach significantly improves the quality of both BPT-based and back-translation methods, and outperforms previous approaches.
160. **A Resource for Computational Experiments on Mapudungun**
    In this paper we provide a new source of (html, pdf) articles and more general texts that we hope will help the researcher and researcher friends to understand the efforts made for better understanding Mapudungun's map (introduction, mapmaking techniques, state of the art of map, map implementation, etc.). Since many types of resources are available in different language, it is important to understand which is the best type for comparing the solutions with other work (map) by using the resources. We provide resources for the mapfication evaluation.
    **Fast Intent Classification for Spoken Language Understanding**
161. We propose an approach for Spoken Language Understanding (SLU) that learns from an offline labeled dataset of spoken utterances to estimate semantic intent (e.g., "where are you going", "what do you want to do"). Specifically, we consider learning an embedding of utterances into an embedding space by using data-driven vectors. We first use off-the-shelf methods to initialize the embedding for each utterance, and then use a binary classifier to identify the exact word(s) to embed. In order to evaluate our model, we built two real-world datasets. The first dataset is a labeled dataset of spoken utterances and the second dataset is an unlabeled dataset of written text (e.g., questions) and shows the necessary building blocks of our SLU framework. We used the same state-of-the-art neural network architecture and achieved the best results on both datasets. We also compared the results of our model against a baseline model using similar architecture and achieved competitive results.
    **Writing Across the World's Languages: Deep Internationalization for Gboard, the Google Keyboard**
    The Internationalization of Neural Machine Translation has experienced a significant leap forward recently, with recent models capable of producing close to zero errors at about 70% BLEU, and very high quality translations of previously out-of-domain texts. Despite this success, however, there has been little progress in building neural translation systems that are cross-lingual. The purpose of this paper is to demonstrate that it is possible to train a Neural Machine Translation system in a domain where there is no parallel data, and the initial training data is consequently either generated by modeling low-resource bilingual corpora, or by carefully cropping parallel data to increase coverage in the target language. We show that a good model, when trained in a cross-lingual setup, can achieve the same BLEU score as a model trained on a strong parallel corpus, in the target language. To the best of our knowledge, this is the first use of cross-lingual training for neural machine translation. We also show how to easily achieve the same level of BLEU as a model trained in a parallel setting, without expensive cropping. We have used both English and Chinese data to evaluate the proposed approach, and our experimental results show that cross-lingual training with micro-targeted generation can achieve state-of-the-art BLEU results for most language pairs, while keeping model size very low. Code is at https://github.com/daniiliev/cross_lingual_research.
162. **AMR-to-Text Generation with Cache Transition Systems**
    Autoregressive models are popular data models for natural language generation. The large amount of training data required for the training phase of the autoregressive models is often prohibitively expensive. In order to cope with the limitation of the training data, we propose to use data caching. We introduce a novel type of cache transition systems, called cache transition systems, that cache transitions. The cache transition systems can be used as a preprocessing step for autoregressive models for enabling rapid transition in order to reduce memory footprint. We apply the proposed caching transition systems to a state-of-the-art autoregressive model and observe consistent improvements in both BLEU scores and chunked-word error rates compared to baseline models.
    **Cross-lingual Pre-training Based Transfer for Zero-shot Neural Machine Translation**
163. Recently, some zero-shot neural machine translation models have been proposed to utilize pre-trained sentence embedding space for English-to-French translation, which can help in capturing syntactic similarity of words from the target language. However, such models are not scalable in handling cross-lingual pre-training due to the large amount of parallel data in foreign language. Therefore, we propose Cross-lingual Pre-training (ClPe) as a way to implement cross-lingual pre-training without the large amount of parallel data in the target language. Our framework is two folds: (i) a Cross-lingual Pre-training Generative Adversarial Network (cPeGAN) based classifier to transfer pre-trained sentence embeddings from English to French and vice versa, and (ii) a ClPe adversarial learning module that is trained jointly with the classifier and the adversarial learning module, so as to avoid the degradation of the generalized translation. Extensive experiments on the Chinese-English translation task have demonstrated the superior performance of our model compared with the state-of-the-art methods.
    **Facilitating on-line opinion dynamics by mining expressions of causation. The case of climate change debates on The Guardian**
    This article investigates how online opinions spread through conversation. To address this, we gather and annotate images and videos from 2015 to 2017 for four debates that take place on The Guardian, a British newspaper that regularly covers news about climate change. We create a dataset of 482, Joey's tennis match at Wimbledon and calculate the average amount of entropy that these videos feature, and then evaluate the impact of a variety of factors, including crowd influence and users' emotional states. We found that emotions were important in both conversations and discussion, and expressed by both participants and non-participants. Distinguishing between these two types of discussion is crucial in helping the scientific community to understand the role of both dialogue and opinion in scientific discourse.
164. **TU Wien @ TREC Deep Learning '19 -- Simple Contextualization for Re-ranking**
    Contextual models are not only useful in ranking; they have also a number of new potential applications. However, despite their popularity, it is hard to understand the intuition behind them. In this paper, we analyze two contextual models: the extrinsic and the intrinsic context models. We focus on their simple contextualization models, i.e. which can learn the marginal weights automatically, e.g. deep neural networks. While these simple models do not always outperform their complex counterparts, we can show that for a certain range of contextualization models, the simple models have a lower prediction error than complex models. At the same time, we show that the simple models have a negative impact on the generalization power. Besides that, we show that a simple contextualization model can be boosted to produce a competitive accuracy on the TREC datasets. The performance on these datasets can be explained by the fact that the intrinsic context models need much more memory, and use much more context from the start, whereas the extrinsic context models rely on much less context. To study this phenomenon, we benchmark the properties of two popular models on all three datasets. Experimental results confirm that they are useful in ranking, but also highlight the negative impact on generalization power of simple models.
    **Knowledge-Enriched Visual Storytelling**
165. We introduce a new task, called Knowledge-Enriched Visual Storytelling, in which a system is asked to generate natural language descriptions of real world events and objects. Knowledge-Enriched visual storytelling presents several challenges, including a lack of multiple types of information about the same events. We design a framework for creating an agent capable of visual storytelling that is conditioned on external knowledge. We use a critic module to evaluate an agent's knowledge of the world and the underlying story and the key ideas to be described, as well as a sequential planner for the system to generate the story description. We evaluate our method on a new dataset of human generated story descriptions and demonstrate our ability to describe real world events and objects accurately and fluently.
    **Reading the Manual: Event Extraction as Definition Comprehension**
    This paper describes a new task, Reading the Manual (RoH), in which the task of automatizing a highly precise natural language task - that is, automatically understanding the meaning of text - is combined with the task of definition comprehension, that is, extracting definitions of expressions. Both tasks are linked through a kind of planning: a human does one task, the automatic parser sees the result of that task and outputs the definition. We introduce a new dataset, Read the Manual (RoMH), which is a large and rich version of WikiTableQuestions, the widely popular Question Answering dataset. To our knowledge, this is the first paper that explores automatic definition understanding, and that is paired with the explicit definition comprehension task. RoMH contains a considerable amount of text, even for trivial questions, and consists of automatic, well-researched resources (e.g. Wikidata and Wikipedia) and hand-crafted resources (e.g. the ontology of the domain). The new dataset has two rules for encouraging the exploration of unstructured text: one allows all images to be used for training, and the other makes all task examples perfectly match each other. We apply different neural architectures for solving both tasks. We conduct experiments to demonstrate the high quality and robustness of the models.
166. **COSTRA 1.0: A Dataset of Complex Sentence Transformations**
    In recent years, word2vec and N-gram embeddings have gained ground in the NLP field for generating synthetic sentence representations. As a matter of fact, these encodings can be applied to many complex NLP tasks. However, only few works used complex sentence transformations in NLP model training process. To overcome this, we construct a large-scale NLP pipeline with the publicly available COSTRA (Constant Operating Resource) dataset. The dataset includes 14,46,542 sampled English sentences as it has the high-coverage of the English language. As part of this study, we propose a novel transformer model which is composed of a two-level sequential tokenization and a conditional long short-term memory (CLSTM) network, which is trained with the phrase based system GRU. The proposed model can handle complex sentence transformations. Experiments show that our model outperforms previous state-of-the-art state-of-the-art NLP models in three tasks: sentiment analysis, named entity recognition and sentence rewriting.
    **Easy-to-Hard: Leveraging Simple Questions for Complex Question Generation**
167. While current question answering (QA) systems are mostly shallow (ie. QA systems are mostly about extracting answers to simple questions), the new approach of maximizing simplicity through question generation is quite novel. In this paper, we propose a QG system that is simple in concept and not only in concept. We focus on answer extraction as a task, and make a hypothesis that easy-to-hard QG is easier to solve. We show that the human evaluation of QG systems is a weak indicator of whether they can generate high-quality answers. We then introduce several simple questions, which clearly show that simple questions are not easy to answer. We investigate the validity of this hypothesis in the context of a question generation model. We focus on the types of complex questions that generate no good answers. Our results demonstrate that the new method called Easy-to-Hard (E2H) can easily handle difficult questions. We demonstrate the quality of E2H in the context of the REBOL 7 QA challenge.
    **PhoneBit: Efficient GPU-Accelerated Binary Neural Network Inference Engine for Mobile Phones**
    With the rapidly growing computing power and memory bandwidth, deep neural networks have been widely applied in mobile applications. However, it remains challenging to scale inference in the mobile devices because of severe energy and storage constraints. Existing GPU-accelerated neural network inference engine, including MobileNet, requires a significant amount of data transfers, which are expensive, impractical, and bottleneck the parallel processing speed. This paper aims to simplify MobileNet and resolve the existing bottleneck problem by leveraging the GPU. We observe that MobileNet, when trained using Deep-Q-Network (DQN) method, can provide similar accuracy as the DenseNet, yet it is much faster and much smaller in weight. Hence, we propose a lightweight MobileNet framework that can support efficient computation to train MobileNet model by following the DQN procedure. Specifically, we introduce Compressed Deep Agglomeration (CDDA) to aggregate existing deep architecture blocks of MobileNet together with the weights and activations. We also design a novel GPU-accelerated variational inference algorithm (VIRA) that accelerates the inference in MobileNet by learning the weights and activations in a compressed form. We conduct extensive experiments on the ImageNet-1K, ImageNet-5K, COCO-Birds-200, COCO-Birds-5K and COCO-Vehicle datasets, and our approach achieves competitive accuracy to MobileNet, reducing both the inference time and the energy consumption while achieving similar accuracy compared to MobileNet. In addition, our approach can run multiple times faster than MobileNet.
168. **Implicit Knowledge in Argumentative Texts: An Annotated Corpus**
    The thesis presented in this article is a presentation of the Principles of Argumentation, and its theoretical implications on argumentation. Using argumentation as an example, the thesis illustrates the use of implicit knowledge in argumentative text, and presents data on its occurrence and usage. The data is acquired from a well-known argumentation database containing discussion discussions in thousands of news articles. An annotated corpus of argumentative text is presented, with annotation for argumentative knowledge and argumentative utterances. The outcome of the presented research is a dataset, which can be used for the purpose of training and evaluating models for argumentation.
    **Keyword Aware Influential Community Search in Large Attributed Graphs**
169. Social network analysis techniques generally use the correlation coefficient for feature ranking. Due to the limited number of documents on the Web, traditional importance-based approaches may over-estimate the correlation coefficient of some terms when large attributed graphs are used. This is because large graph weights are often not revealed. We suggest to use attention to identify the most influential terms and combine it with an attention mechanism to find high-confidence large attributes. The experimental results on five real-world datasets show that using word or term-level features can significantly reduce the error rate, while using an attention mechanism can increase the confidence of the top-N terms in a big attributed graph.
    **An Exploration of Data Augmentation and Sampling Techniques for Domain-Agnostic Question Answering**
    We evaluate cross-domain question answering over existing labeled and unlabeled text corpora on the SemEval-2010 Task 8 dataset. Our model uses a document source to obtain word embeddings and then leverages these embeddings to perform a set of joint reasoning tasks. We propose three training strategies: (1) typical joint reasoning, (2) intra-document document embeddings transfer, and (3) inter-document document embeddings transfer. Our models demonstrate very promising results across all tasks for all of the domains. Our code, pre-trained models and the data used for all experiments are available at https://github.com/amirzadeh/dataset8.
170. **PDC -- a probabilistic distributional clustering algorithm: a case study on suicide articles in PubMed**
    We propose a new family of probabilistic distributionsal clustering (PDC) algorithms based on the notion of similarity. These algorithms are robust and fast, particularly in comparison with naive log-linear regression methods. We apply these new algorithms to a series of popular heterogeneous medical papers. An analysis of the resultant representations reveals intriguing structures in the data which seem to be indicative of the psychological development of each paper. We conclude that the data often adequately captures the underlying population distribution of mental health professionals. Moreover, similar sentiments are expressed across articles containing similar content. This suggests that such data capture a hidden pattern of mental health development across different psychiatric journals.
    **Neural Machine Translation: A Review**
171. In this paper, we review and summarize recent advances in neural machine translation. We discuss the general approaches and applications of neural machine translation and distinguish them according to the tasks they address: phrase-based, sequence-based, and hybrid. Based on these distinctions, we then discuss the various training methods for neural machine translation, highlight the capabilities of different neural architectures, highlight the drawbacks of recent improvements to neural machine translation, and provide insight into the future of neural machine translation. Finally, we suggest research directions for neural machine translation and review open issues.
    **Enhancing Relation Extraction Using Syntactic Indicators and Sentential Contexts**
    People tend to be more verbose with respect to their relationships with others when they express those relationships in text. Extracting this information from text enables a system to understand relationships in a more detailed manner. It can thus be useful for decision support and socialization applications. In this paper we address this problem by proposing a new approach for relation extraction based on syntactic indicators. The proposed method creates an interval tree from two components of a sentence: syntactic dependencies between its constituent sentences and their relations; and semantic and syntactic inferences. These new dependencies are then filtered to achieve a final relation tree. The advantages of the proposed method are the small number of trees and the uniformity of the relations computed at the root. We experiment with an evaluation corpus that consists of small numbers of text from Wikipedia and in particular context texts and show the advantages of the proposed method compared to recent state-of-the-art methods on the sentiment analysis of abstracts, news headlines and tweets.
172. **PitchNet: Unsupervised Singing Voice Conversion with Pitch Adversarial Network**
    Currently, voice conversion (VC) is becoming a common research area in speech recognition and speech processing, especially in performing accurate voice conversion (VC) for automatic speech recognition. In this paper, we propose a novel approach, called pitch network (PN) for VC, which achieves VC by exploiting the pitch noise and interference in a single deep neural network. Firstly, we devise an effective pitch generator to synthesize the pitch noise. Secondly, a new pitch adversarial network (PAN) is designed to encode the pitch noise into the data by imposing an adversarial discriminator, which captures the manifold structure of the pitch noise, and then utilizes the PN to encode it into the converted voice. Experiments on three benchmark datasets (i.e., i-VT, i-SRE and QMIX) show the superiority of the proposed approach against various state-of-the-art VC methods.
    **Cooperative Reasoning on Knowledge Graph and Corpus: A Multi-agentReinforcement Learning Approach**
173. The use of commonsense knowledge to assist robots in navigation tasks is becoming increasingly popular, due to the accessibility of such knowledge to unsupervised machine learning techniques. However, the acquisition of such knowledge is an open question that is becoming increasingly relevant with the increasing availability of new information sources, such as crowdsourced knowledge graphs. These knowledge graphs often represent complex social relationships, that make the acquisition of commonsense knowledge particularly difficult and demanding. In this paper, we address the problem of collective commonsense reasoning and suggest a method to dynamically decide what commonsense knowledge to leverage for collective navigation. The current case study consists of a carefully chosen robotic task. Our preliminary results indicate that the proposed method could significantly reduce the time required for the robot to acquire commonsense knowledge.
    **Classifying Diagrams and Their Parts using Graph Neural Networks: A Comparison of Crowd-Sourced and Expert Annotations**
    Diagrammatic reasoning and pattern recognition tasks can be divided into two main categories: (i) data-driven inference, where the goal is to find correspondences between symbols in an input diagram and the corresponding symbols in a database, and (ii) representation-based reasoning, where the goal is to represent symbols in the input diagram as a set of features for some downstream task, in particular to perform classification or retrieval. In this work we consider the former category and attempt to answer questions such as: Given a diagram of size n, identify whether this diagram contains $n$ symbols in total (including non-occurring ones). To this end, we propose a Graph Neural Network (GNN) based system that achieves a true positive rate (TPR) of $99.61\%$ when automatically extracting symbols and associating them with concepts from a graph and passes human annotation to achieve a TPR of $95.35\%$. This success indicates that the power of GNNs is mainly in their ability to localize unseen symbols.
174. **Complete Variable-Length Codes: An Excursion into Word Edit Operations**
    Text editors are typically incapable of creating variable-length representations for words. This leads to multiple critical limitations. First, variable-length representations contain additional information that is not captured by the current standard character set. Second, this additional information causes a practical bottleneck for many applications. In this paper, we present a formal model of variable-length representations and their encoding, in which different editing behaviors generate data sequences which lead to diverse representations. We show that editing has an effect on the encoding of variable-length representations. We analyze two variable-length representations: {\em d}-character and {\em poly}-character. We demonstrate that adding an edit to a character sequence removes the special character before the edit is applied. Additionally, we show that if an edit is allowed for a word, then the encoding of a character sequence will remain constant. To further show that the language and the encoding are not independent, we study the character set representations of English words that are the most common in natural language text. We found that the poly-character representations lose most of the information that is encoded in the standard character set. Furthermore, we found that edits can occur to a character sequence at almost any position, and the encoding will remain fixed. This allows the optimal poly-character encoding to be obtained with the least amount of effort.
    **Fine-Grained Emotion Classification of Chinese Microblogs Based on Graph Convolution Networks**
175. Emotion-related questions and comments play a crucial role in driving the community conversation, making social media platforms a prime platform for users to express their subjective emotions. One of the main issues that affect the efficacy of emotion-related question and comment classification methods is the scale discrepancy in the language expression and the emotion of a particular user. In order to address these issues, we apply graph convolution networks (GCNs) for emotion classification of Chinese microblogs. In our experiments, we used two standard datasets and two novel datasets that were generated from microblog posts. The performance of GCNs with graph convolutional layers was evaluated. The results showed that the network was able to achieve a classification accuracy of 91.35\% and 84.65\% on the two experimental datasets.
    **Massive vs. Curated Word Embeddings for Low-Resourced Languages. The Case of Yor\`ub\'a and Twi**
    We study the performance of local embedding algorithms for low-resourced languages on word analogy and in-context inference tasks, with focus on Yor\`ub\'a and Twi. The main reason for this special low-resource case is that these languages are closely related. In addition to our interest in comparative evaluation, we expect that our findings may be useful for bridging the gap between local and highly specialized word embeddings and the community on similar words.
176. **Effective Data Augmentation Approaches to End-to-End Task-Oriented Dialogue**
    In recent years, end-to-end task-oriented dialogue systems have attracted much attention due to its high potential for increased human-machine interaction. However, this promising direction of dialogue is still in its infancy. In order to learn more effectively and quickly from data, we propose several data augmentation strategies to enlarge the dialogue experience and effectively train a task-oriented model. These are tested on an open-domain scenario from scratch and obtained the results as shown in Figure 1.
    **Love Me, Love Me, Say (and Write!) that You Love Me: Enriching the WASABI Song Corpus with Lyrics Annotations**
177. Annotation data is expensive and time-consuming. Various methods exist, but in this paper we propose a novel annotation scheme which enables rapid learning of song lyrics. We apply this scheme to lyrics for five popular bands - Radiohead, Enema of the State, Mumford and Sons, Kaleo, and the Big Easy - and obtain an annotated corpus of 152K songs in total. The results show that this approach yields considerable gains over a generic support vector machine baseline for the task of automatic metadata generation.
    **Learning to Predict Explainable Plots for Neural Story Generation**
    In neural story generation, existing methods are mostly limited to presenting natural language descriptions for generated stories. However, most existing neural stories are short, simple, lacking in complex explanation, and do not contain content in external databases. To address these limitations, in this paper we propose a novel neural story generation model to infer the natural language description, which is capable of generating long, complex, and detailed natural language descriptions with coherent content for generated stories. Specifically, we propose a Novel Characters Recurrent Network (NCRN) to explain the generated story based on input characters, which is based on characters similarity information. Specifically, the inter-character relationships and the dependencies among the characters are analyzed to infer the natural language description. Based on the information of characters, we also present a Character-Word Co-occurrence Network (CWCN) to retrieve the most relevant words from characters and words. Furthermore, an external story database with both in-domain and out-of-domain story data are constructed to evaluate the model and evaluate the explanatory power of our model. Experimental results on an automatic evaluation dataset (i.e., ACE-Story) show that the proposed model improves the baseline models significantly, and reaches much higher F1-measure with less training samples and lower run-time than other baselines.
178. **Measuring Social Bias in Knowledge Graph Embeddings**
    A social network can be formed by hundreds of thousands of people, where only a small portion of them will share an opinion on another person's popularity. Such a small number of shared opinions on each person's public profile could lead to various forms of bias, such as creating virtual twins that share the same opinion, or favoring certain community members by excluding others. We consider a standard knowledge graph embedding and embed such a small subset of the nodes into it. How does the graph embedding reflect social bias, in terms of co-attention across the individual nodes? To answer this question, we quantitatively study the embedding in a large collection of data and find that it often creates a virtual twin or shows stronger interaction bias. Moreover, we establish several conditions that guarantee that the embedding makes accurate predictions and can still retain social bias.
    **Decomposing predictability: Semantic feature overlap between words and the dynamics of reading for meaning**
179. There is considerable progress in the understanding of the semantic characteristics of words from their patterns of use in a particular domain. However, we lack a detailed analysis of how semantic information is transferred across concepts and sentences. We hypothesize that transferring semantic information relies on a shared representation, by allowing the semantic dynamics of words to be partially captured using simple logical rule sets. Therefore, we decompose the semantics of words according to their set of logical rules, subject to the specific dynamics of reading. This decomposition makes the decomposition of semantic rules more transparent to end-users, by allowing to evaluate them against natural texts. We propose an approach to find semantic overlap by comparing the semantic representations learned by a recurrent network with a standard statistical model, aiming at matching how the two learned representations can be explained using deterministic models. Our experimental results on small-scale datasets show that the approach gives accurate predictions of semantic representations between words and sentences.
    **Explaining Sequence-Level Knowledge Distillation as Data-Augmentation for Neural Machine Translation**
    We consider sequence-level knowledge distillation for neural machine translation (NMT). The knowledge distillation is achieved by applying the neural network to the source sentence and then to the distillation target sentence. The performance of NMT system is measured by the reconstruction error. In order to quantify the quality of the translated sequence, a bootstrapping strategy is adopted, where the target sequence is automatically generated from the source sentence. The idea of this research is to combine the role of regularization and data augmentation of attention models with the sequence level knowledge distillation method. Our experiments show that the proposed method is superior to the state-of-the-art knowledge distillation methods and produces much better translation quality.
180. **Semantic Mask for Transformer based End-to-End Speech Recognition**
    End-to-end (E2E) neural networks are increasingly being applied to automatic speech recognition (ASR), which is a method that makes significant use of the structure of the neural network and relies heavily on hand-crafted features. During training, E2E networks are typically trained on synthetic data, and subsequently fine-tuned using large amounts of natural audio data. The challenge in E2E training has been addressed in prior work which can achieve comparable ASR performance with a smaller amount of data. In this paper, we seek to overcome this limitation with a novel architecture for E2E end-to-end speech recognition. Our model takes the unlabeled natural audio as an input and produces a mask (masked feature vector), as the mask value is independently chosen for each word. We adopt an alternating training process, which in turn alternates between predicting the mask value and training the discriminator to find the mask value. Our experimental evaluation on Wall Street Journal, ASRA-B, NIST 2000 evaluation sets and the recently released Microsoft mixed reality test set demonstrate state-of-the-art performance on all metrics, and significantly outperform prior methods. In particular, the proposed model is about one hundred times smaller than previous E2E architectures, and achieves similar or higher performance.
    **Adversarial Analysis of Natural Language Inference Systems**
181. This paper introduces a novel framework for exploiting adversarial perturbations in natural language inference systems to probe their internal reasoning processes. The framework addresses three open questions regarding how an inference system makes and reasons with such perturbations. Specifically, we study the perturbations in two important contexts: background knowledge inference and machine comprehension. Specifically, we find that perturbations are especially effective for general knowledge inference systems and particularly effective for the neural systems that use intermediate representations. We also observe the existence of systematic perturbations, i.e., adversarial perturbations, that are effective for only a subset of inference systems. Finally, we show the empirical validity of the framework by evaluating the applicability of existing models on datasets of natural language inference in two relevant applications: (i) Question answering and (ii) Machine comprehension.
    **Unsung Challenges of Building and Deploying Language Technologies for Low Resource Language Communities**
    Under-resourced language communities can be a challenging environment for developing language technologies. The language community of a particular language is a proxy for the broader linguistic development of the community. However, existing data-driven model-driven language technology deployment approaches have not fully explored how language technology can be leveraged for low resource language communities, particularly for new languages with scarce training corpus. The newly released 7 Languages in 7 Languages (LA7L) benchmark presents a novel testbed for this, along with numerous simulation studies and language technology evaluations. These experiments, based on a fully bilingual 5-layer LSTM, present a representative scope of language technology deployment challenges for language communities, as well as the language technology that can be successfully deployed by low resource communities. By adopting the LSTM language technology deployed in LA7L and comparing the results to seven competitive baseline language technologies, we identify the remaining challenges and opportunities for building and deploying language technologies in low resource language communities.
182. **Personalized Patent Claim Generation and Measurement**
    In this paper, we consider the task of automatically generating patents as well as generating claims, using deep neural networks. In particular, we focus on three key ingredients of a patent claim generation task: (i) patent-specific input texts, (ii) background knowledge of the claim and (iii) context information of the input. We develop two different neural network architectures: (i) a single network, which generates sentences containing patent text and background knowledge, and (ii) a multi-stage network, which simultaneously generates individual claims and text. We evaluate our methods on two publicly available patent datasets, extracting claims from a text generator and then matching these claims to patents. The experiments show that (a) our single-stage method improves the quality of the generated sentences, and (b) the multi-stage approach improves the quality of the generated claims. We also report results of experiments on measuring the compatibility of the generated claims and the claimed patents.
    **Women in ISIS Propaganda: A Natural Language Processing Analysis of Topics and Emotions in a Comparison with Mainstream Religious Group**
183. One of the greatest challenges for research in understanding the ways in which ISIS propaganda disseminates its ideas has been the lack of a neutral or comprehensive corpora. In this study, we utilized the hashtags, memes and other phrases that appear in ISIS-related Twitter data to extract information about various topics such as women and ISIS-related events. The social media posts containing these keywords were processed to yield a summary of the topics in the ISIS-related tweets. We applied Natural Language Processing (NLP) techniques to extract characteristics from the ISIS-related tweets and compared the results with the opinions expressed in mainstream religious Twitter that were produced from official ISIS accounts.
    **A Multi Purpose and Large Scale Speech Corpus in Persian and English for Speaker and Speech Recognition: the DeepMine Database**
    In this article we present the DeepMine database, consisting of more than three hundred hours of speech utterances of more than sixteen thousand speakers of ten languages. This is the largest available multi-talker Persian and English speech corpus to the best of our knowledge. We present the methodology used in creating and curating this database. We also present the strategies we use to train and evaluate the system using these recordings. Furthermore, we describe the process we have used to train and evaluate a speech recognition model using these recordings.
184. **Open-domain Event Extraction and Embedding for Natural Gas Market Prediction**
    Market activity forecasting has been one of the major challenges of the natural gas industry. Traditionally, experts obtain data on individual customers in neighborhoods, and use it to predict the distribution of each customer's market. However, in many cases, customers have very different behavior patterns, and hence, prior data cannot be directly used to model demand. Instead, we need to build a model to predict demand from an unsupervised way using predictive models learned from self-generated text data such as social media posts and event streams. In this paper, we present a prediction model for the gas market on an open-domain dataset of new California Gas Network customers. Our experiments demonstrate that our model outperforms many state-of-the-art market models.
    **Analysis of the Ethiopic Twitter Dataset for Abusive Speech in Amharic**
185. Using social media, individual, and collective voices to expose abusive behaviors on the Internet is an interesting challenge. The vast amount of Arabic social media data in Amharic language, which is mostly collected by communities in the region, is a great resource for analysis of the abusive behaviors. In this paper, we analyze the datasets for abusive speech and the different types of abusive behaviors using a machine learning model that generates the descriptive words of the abusive behaviors. The hierarchical approach of Hierarchical Linear Support Vector Machine (HL-SVM) was used to classify abusive behaviors using different features. The results of the approach showed that the dominant abusive behaviors can be the racial abuse (Gehi or mar- Rahim meaning white, racist, racist, Arab, or Arabic) and gender abuse (Arab).
    **AI2D-RST: A multimodal corpus of 1000 primary school science diagrams**
    We describe our crowdsourced definition of analogies in order to classify school diagrams into five prototypical conceptual types: equivalence relations (equivalence relations are defined to correspond to relationships between elementary elements), analogy relations (imilarity relations are defined to correspond to relationships between elementary elements), functional relations (functional relations are defined to correspond to relations between elementary elements), interpretation relations (interpretation relations are defined to correspond to relations between elementary elements). We hope that these definitions will provide an important tool in the construction of an interpretable diagram representation and in the development of high-level reasoning tasks for the intelligent diagram systems. Finally, we use these definitions to analyze the description of life diagrams and show that the twelve biological constraints are associated with two notable diagram typologies: living diagrams and diagrams with incomplete information.
186. **Homograph Disambiguation Through Selective Diacritic Restoration**
    We propose a robust graph convolutional network (GCN) framework which is capable of identifying the close-contrast homograph. The key idea of this paper is to model a GCN as a joint filtering of two inter-related sources of local information (i.e., visual and contextual information). A set of selective diacritics is first designed for a proposed joint filtering strategy, which maintains more distinct visual and contextual features. Then the detected homographs are incorporated into the network training in a number of ways. To further improve the top-down node encoding for real-world use cases, an attention mechanism is incorporated into the network structure. Our method is evaluated on three large-scale public benchmark datasets, i.e., the SemEval-2010 Task 10 dataset, the MS COCO dataset and the University of Sheffield Flickr30K dataset. A comparative evaluation on the three datasets show that our method not only achieves state-of-the-art performance, but also offers useful insights for researchers and practitioners.
    **Zero-shot Text Classification With Generative Language Models**
187. In this paper we address the problem of zero-shot text classification. Unlike conventional text classification which assumes that examples of the classes of interest are labeled with a predefined set of predefined features, this paper proposes a more challenging problem where the text class is not predefined, but rather learned during the training stage. This model is a generative model where the generated samples of the text class are expected to be the same as the examples of the class. Our approach is based on training the classifier through self-supervised learning of a disentangled latent representation that captures the semantic content of text words. We demonstrate the effectiveness of our proposed model on three popular zero-shot text classification benchmark datasets.
    **GeBioToolkit: Automatic Extraction of Gender-Balanced Multilingual Corpus of Wikipedia Biographies**
    This paper presents a Gendered Bio-Text Toolkit (GeBioToolkit) that presents itself as a simplification of a large, but sophisticated, and potentially complex toolkit that provides geolocation and multilingual text categorization, stemming from the BioMedTreebank. We propose to use the toolkit to automatically extract text that relates to a particular gender from Wikipedia biographies, by matching the sections that mention a gender to a few full articles that mention the same gender, within a paragraph. We demonstrate the utility of our approach by presenting a large-scale gender-balanced multilingual collection of nearly one million Wikipedia articles, created using the toolkit. We conduct experiments using standard text categorization and named entity recognition benchmarks on this collection. Our results show that using gender as an information source for task-oriented text extraction is a promising approach to bringing linguistic resources to underserved populations.
188. **Advances in Online Audio-Visual Meeting Transcription**
    Video-based meeting transcription has been considered a subject of active research over the last several decades. Many successful methods have been introduced in the literature; however, manual meeting transcription remains the standard, for the reasons of resource limitations and language difficulties. In this paper, we propose an online speech-to-text transcription system that has the advantages of being able to transcribe all meetings from an input audio clip in seconds, and of being able to incorporate metadata to improve the overall quality of the transcription. We evaluate our system on the GRID corpus, a large meeting corpus that has been annotated with these same attributes. We are the first to generate the NIST ASR evaluation labels for the GRID corpus. We report state-of-the-art, on-par, and better quality than a baseline, and our results are consistent with previous work.
    **Neural Module Networks for Reasoning over Text**
189. We present a new type of deep recurrent neural network that is designed to learn a textual description of the world. The model can answer a series of questions about the text, and is able to synthesize the description for new knowledge. On the basis of this kind of text generation capability, we propose a framework, MLMNet, which can make use of general-purpose reasoning capabilities and may be used as a basic building block for a neural machine translation (NMT) system. The framework relies on two encoders: a Module Network that generates parameters, and a Sequence Network that performs a neural attention control. We evaluate MLMNet on three NMT tasks. The first task is information retrieval, and our approach outperforms a number of strong baseline methods on a large gold standard data set. The second task is multi-hop information extraction, where the parameters generated by the Module Network are used to parameterize a semantic graph for the multi-hop information extraction model. This approach obtains competitive performance on the benchmark data set, and performs significantly better than the competitive baseline methods, while still being capable of generating text that generates an adequate description for a query. In this last task, the Parameter Network is used to generate a graph that encodes semantics, and MLMNet performs competitively against the other baseline methods.
    **An Ensemble Method for Producing Word Representations for the Greek Language**
    This paper presents an approach to acquiring an adaptation for word representation that can be used for the extraction of Greek words from text corpora. For this purpose, the proposed method consists of an ensemble of three modules, which are processed by means of a maximum entropy reinforcement learning approach and a tree based clustering technique.
190. **Medication Regimen Extraction From Clinical Conversations**
    The ability to represent and extract generic electronic health record (EHR) data is expected to contribute significantly to the evaluation of health resource utilization, which can aid in the automation of the process of data analysis and decision making. Recent research efforts have shown significant progress towards this goal by exploring various medical topic representations, classifiers, and annotation frameworks. However, current medical topic extraction systems typically consist of (i) a generic representation learning module, and (ii) the generation of annotated medical Q/A corpora. While the generality of generic topics serves them well for general document summarization, they do not provide any additional features for Medication Regimen Extraction. In this study, we introduce a novel framework that leverages the above concepts to (i) automatically learn a generic knowledge graph (KG), and (ii) develop an integrated document and Q/A extraction system. The proposed framework is evaluated on a large publicly available corpus of Medication Regimen Extraction conversations. A machine learning model outperforms the traditional handcrafted features for KG learning, and the proposed framework is used to generate an annotated corpus of Medication Regimen Extraction conversations. Results demonstrate that the proposed framework can extract effective summary topics, and the system is able to learn generic document representations for various EHR corpora.
    **How to Evaluate the Next System: Automatic Dialogue Evaluation from the Perspective of Continual Learning**
191. Continuous learning (CL) is one of the promising research directions that allow automatic system reasoning in the presence of new data. However, the existing CL frameworks mostly deal with raw utterances and are therefore unable to evaluate utterances which were generated through carefully designed CRF (constrained regularized finite automaton). To solve this problem, in this paper we propose a novel, data-driven CL framework called CFACTOR that builds upon existing RL (i.e. sequence to sequence) frameworks. We generalize the algorithm under current system specifications to a multi-agent setting where multiple dialogues are available for evaluation. We design a curriculum learning strategy to provide a good balance between answering repetitive questions and providing rich responses to new dialogues. We evaluate CFACTOR with six standard dialogues under the framework of CALLHOME. Experimental results on the dataset demonstrate that CFACTOR outperforms existing CL frameworks.
    **A Collaborative Ecosystem for Digital Coptic Studies**
    In our community, digital coptic studies are increasingly being implemented, a process that involves digital data manipulation, visualization, annotation, and analysis. The challenge is that each individual in this collaborative effort is also a coptic; the resulting digital repositories are scarce and difficult to study. Moreover, the coordination of all the activities are challenging, due to the large number of participants, the many different tasks (including data manipulation), and the practical challenges of analyzing all these information. We believe that digital coptic studies would benefit from collaboration to advance the state of the art in terms of high-quality and fast-paced analysis. To achieve that, we propose to organize the digital coptic studies work as a collaborative ecosystem, and to formally recognize the shared requirements and technical challenges among the participants. In addition, we also discuss how to support the sharing of results in digital coptic studies, in a transparent, controlled, and reproducible way.
192. **Quality of syntactic implication of RL-based sentence summarization**
    As part of a larger corpus-based sentence summarization system (SSRM), we demonstrate that summarization is feasible by training a sequence-to-sequence-based summarization model using a dataset of 100k fragments (100k candidate sentences) with associated evaluative scores. We discuss issues and shortcomings of the proposed approach, and propose modifications to improve the quality of generated summaries.
    **Unsupervised Neural Dialect Translation with Commonality and Diversity Modeling**
193. An unsupervised neural machine translation model learns source-side information through subword alignment and a transducer, and then predicts target-side information via a two-step forward machine translation system. While this approach has been used successfully in neural machine translation, translation of non-native languages has been extremely challenging due to their mixed vocabulary, morphology, and dialects. In this paper, we propose a novel architecture to address this problem: namely, a conditional generative adversarial autoencoder (cGAN) that learns to map each source word to the shared word representation of the target. The encoder and decoder of the cGAN are independent and hence can be trained end-to-end. The input for the cGAN, a target word, is generated from a standard language model trained on the source-side information. The cGAN learns to map both source and target word representations to the same representation space and effectively uses it to model non-native language aspects of a language. We further show that our approach can be used to recover corruptions made during machine translation. Experiment results on four different non-native languages show significant improvements in both fluency and content over several baseline models.
    **Automatic Spanish Translation of the SQuAD Dataset for Multilingual Question Answering**
    We present an automated Spanish translation of the SQuAD benchmark dataset for multilingual question answering. Previous studies on multilingual text summarization use gold translation output from a tool such as RUMF. However, manual labeling is tedious, costly and time-consuming. Inspired by recent advances in neural language models, we propose to model each sentence pair in the SQuAD corpus into a vector of words, so as to capture high-level textual information. The re-weighted cross entropy loss is then used to measure the quality of each translation output. We evaluate our approach in the form of two quality measures, based on $G$-mean and word overlap. Both of these measures are computed from the pre-trained language model. Our approach outperforms the standard translation methods, in terms of evaluation metrics. Our approach also yields high quality translations of each of the manually labeled gold data pairs in the SQuAD benchmark, showing that it is feasible to build a baseline on this dataset for future research.
194. **Voice Conversion for Whispered Speech Synthesis**
    In this paper, we propose to exploit deep learning for synthesizing spoken words for several speech enhancement tasks. As one important step, we developed a language-independent model based on a fully-connected neural network that can translate different languages into speech, and a language-dependent model which can further improve the quality of the generated speech. To evaluate the proposed models, we designed a protocol using a three-task objective for acoustic stylometry, voice conversion, and voice enhancement, and experimented with one speaker-independent model and three speaker-dependent models. The experimental results show that the deep models can achieve high quality phonetic speech for speakers from different languages, and are comparable with the proposed language-dependent model. This paper also presents a phonetic error-correcting generator based on the language-dependent model.
    **CoSimLex: A Resource for Evaluating Graded Word Similarity in Context**
195. We introduce CoSimLex, an online corpus for evaluating graded word similarity. Our corpus consists of pairs of words that have been co-registered to a given document to show that they are similar, but their semantics differ. We manually annotate each pair and report its closeness using a fixed-point model. We describe a principled way to use this result to evaluate an NLP system for spoken text and evaluate models that use word similarity to perform direct speech recognition. We also describe several evaluations of neural models based on the co-registered sets. We open the corpus and will share it with the community.
    **Improving Neural Protein-Protein Interaction Extraction with Knowledge Selection**
    Extraction of the optimal joint-effect model (JEM) for assessing the biological significance of protein-protein interactions is an essential task for bioinformatics applications. To capture the dependencies among various protein-protein interactions, we can utilize a set of learned partial model structures in a powerful way. In this paper, we propose an embedding framework to define knowledge based conditional joint-effect models, where the knowledge is defined based on latent variables extracted from an input protein sequence. We call this model \textbf{KnowE} because it is based on a set of \textbf{Knowledge} variables. These knowledge variables are introduced into the interaction matrix obtained from the joint-effect models, and are recursively combined to select the optimal joint-effect model for feature representation. To solve the challenging optimization problem, we propose an iterative constrained variational algorithm to find the optimal joint-effect model, where the constraint is placed on the approximate posterior density to find an optimal solution. We conduct experiments on several standard protein-protein interaction datasets. Our experiments demonstrate that our proposed model outperforms state-of-the-art methods on one dataset, and scales up to datasets with 100K data points.
196. **Short-duration Speaker Verification (SdSV) Challenge 2020: the Challenge Evaluation Plan**
    The process of short-duration speaker verification (SdSV) is a challenging problem in speaker recognition that requires simultaneous detection of voices of different speakers and classifying them into respective classes, also known as speaker-independent speech classification (SIC). Most state-of-the-art SIC methods typically rely on individual features extracted from voice samples which are fixed across training and testing and do not generalize well. In this paper, we explore feature-based SIC models and a particular soft clustering method for SIC with deep neural networks. However, these methods suffer from insufficient training data and limited accuracy. To overcome this, we propose a novel feature-based SIC method, namely Deep Feature-Based SIC (DFBSIC), and apply it to a combination of three datasets and three SIC models. Experimental results show that DFBSIC achieves the best performance among the 17 short-duration SIC methods on the Extended Cohn-Kanade SIC (XCSIC) dataset with 47.7% accuracy (62% on Dice score). It also achieves the best performance on the Combined Cohn-Kanade SIC (CKCSIC) dataset with 36.0% accuracy (39% on Dice score).
    **Shaping representations through communication: community size effect in artificial learning systems**
197. The community structure of artificial learning systems is an important research topic in machine learning. However, in real-world learning systems, the community structure plays a limited role. Previous studies have shown that the community structure of artificial learning systems is more important in partially observed systems. In the partial observation regime, we are not able to observe the community structure of the learners. In this work, we conduct an experimental study to determine whether the community structure of artificial learning systems can be determined from the communication behavior. We collect a community structure metric in a partially observed two-player zero-sum zero-sum games with a fixed communication channel. We also study the effect of different feedback structures in partially observed zero-sum zero-sum games. Our study shows that in general, feedback structures affect the community structure. A major key in these results is to study the feedback in partially observed zero-sum zero-sum games. The interactions between two players after a communication can be very fast and therefore we can obtain a time-varying community structure. Finally, we apply the metric to more realistic algorithms and three real-world data sets to investigate whether the community structure of artificial learning systems can be successfully predicted using feedback structures.
    **Improving Interpretability of Word Embeddings by Generating Definition and Usage**
    In this paper, we propose an improved vector-space representation for words, which is equivalent to 1) a semantic vector-space for words (STV) and 2) a distributional vector-space for word vectors (DVV). The DVV representation has proven effective at incorporating semantic information for single words, but its performance is not as good when it comes to sentences. Therefore, we propose a novel method for learning a better DVV representation and evaluate its performance on two datasets. We show that the proposed method outperforms the SV representation, obtaining an improved performance on two tasks: term extraction and definition extraction.
198. **Context-aware Entity Linking with Attentive Neural Networks on Wikidata Knowledge Graph**
    Existing models that address link prediction on Wikidata entity data suffer from a lack of context information, and typically involve language modeling with random-walk ensembles to produce outputs conditioned on the entity context, which might be difficult for less-resourced entities to embed in the entity-annotated knowledge graph. We propose Attentive Neural Networks to learn contextualized entity embeddings, which explicitly attend to the context and dynamically adjust the network parameters, making the predictions both context-aware and translation-invariant. Experiments on two datasets from different domains show that Attentive Neural Networks consistently improve the performance of existing methods and achieve state-of-the-art results on both datasets. Our dataset and source code will be publicly available at http://w3c.github.io/attentive-neural-networks/
    **Lessons from reinforcement learning for biological representations of space**
199. Reinforcement learning has enabled machines to explore and learn complex environments. When applied to biological models, such as the E. Coli bacteria, its exploration strategies can be highly controlled and may require significantly less time to learn a good policy than competing approaches. In this work we perform a computational analysis of the exploration and exploitation strategies learned by E. Coli and other members of the so-called gravitropic group and observe how they can be influenced by chemical signals that control the overall microbial population dynamics. We also use this framework to demonstrate how intelligent choice of location of replication templates can be conditioned on exploration strategies.
    **Document Sub-structure in Neural Machine Translation**
    It is well known that statistical language models exhibit some ability to capture lexical representation in their decoding process, but as far as we know, they are not capable of expressing dependency structures beyond a fragmentary, sentence-level representation. We develop a supervised neural machine translation system that uses a sequence-to-sequence deep neural network to translate an English sentence with dependency structures into a short, lemmatized form. To tackle the problem, we introduce a novel and simple structural penalty into the word order. Compared with strong dependency parsers such as the BPE model, we show that the proposed penalty improves the translation performance.
200. **Long-length Legal Document Classification**
    Classifying a document as relevant to a particular law domain is an important application of semantic parsing. The problem is challenging, since the document must be successfully classified without missing important contextual information, and must be simultaneously represented in a form that is semantically interpretable and easy to interpret. In this paper, we present a parser that efficiently performs unsupervised document classification while avoiding many of the worst performing (minimally) greedy classifiers. The parser is based on a recently developed large-scale algorithm that allows it to accurately classify paragraphs with up to 130 words and corresponding legal classifications. The parser is fast (over 20 million parse iterations per second), scalable, and can efficiently process millions of documents per second. The parser is publicly available at https://github.com/nptac/long_document_classification.
    **Efficient Convolutional Neural Networks for Diacritic Restoration**
201. This paper proposes a novel method for image restoration using convolutional neural networks, which is based on residual learning. In the proposed method, several multi-resolution filters are introduced into each convolutional layer. Compared with previous methods that use only one multi-resolution filter, the performance is improved by incorporating multi-resolution and multi-scale information into the convolutional layer. Moreover, the residual-enhanced feature of the initial layers is used to train a multi-resolution patch-based image denoiser. We further propose a generalized equal-quantile coding scheme to overcome low resolution images by gradually building up the entropy of the signal. Finally, the proposed method is applied to the aforementioned challenging images for automatic diacritic restoration. Experimental results indicate that the proposed method improves the naturalness and contrast of the images, and provides a better automatic diacritic restoration than the existing methods.
    **Towards Robust Toxic Content Classification**
    Detection and analysis of Toxic Content (TC) in Web images are a very challenging task due to their heterogeneous nature and multidimensional nature. Most of the existing approaches are tailored to the fact that such images have been compressed or otherwise lower-quality images have been degraded first before classification. This leads to potentially suboptimal results. In this work, we propose a novel approach for TP identification and classification from low-quality images captured in natural light conditions. By identifying TCs from these low-quality images, we are able to do the following: i) learn a robust deep neural network (DNN) with input features from the low-quality images; ii) learn an embedding from the DNN representation space, which we can use to detect TCs from the images, and iii) efficiently classify the TP from the newly identified TCs. We evaluate our method on three public image datasets. The results show that the proposed approach outperforms existing methods for TC identification and classification with much lower computational requirements, compared with prior methods.
202. **Knowledge-based Conversational Search**
    Conversational Search (CS) has demonstrated its outstanding ability in reducing the computational complexity while achieving strong results. However, in order to improve the relevance of CS, more information must be incorporated to satisfy the user's personal knowledge. There are many research approaches for incorporating more information into CS such as features, automated reasoning or collaborative models. However, this paper studies the merits of incorporating the user's knowledge into CS and proposes an approach for knowledge-based CS that combines supervised learning and Q-learning. We evaluate the proposed approach on the BNCC-KBQ corpus for multiple tasks. Our experimental results show that the proposed approach outperforms the traditional CS models with the help of knowledge-based reinforcement learning.
    **LScDC-new large scientific dictionary**
203. The article introduces a new large dictionary for interlinear nonlinear phenomena using genetic programming (GPP). The proposed dictionary represents astronomical objects as sequences of eigenvectors of orthonormal matrices, and it uses the edge weights of all the epipolar lines within an observation pair as a measure of separation. This implies that each observed line may have either a distinctive measurement signal or a weakly associated, low-dimensional signal in addition to an intrinsic signal which can be represented as a sum of all of its component lines. We also introduce and validate a genetic programming implementation of a novel performance measure for interlinear data sets called the Perceptual Eigenvector Distribution (PED) which allows us to control the rank of the PED in terms of the statistical distribution of the detected lines. We then describe how the novel dictionary can be used to produce a very large dictionary of instances, which may be used for extracting signal properties from entire data sets and for detecting correlated structures in such data sets. We then describe a numerical method for generating a dictionary that is optimized for identifying image features that are used for modeling the image. We discuss a statistical method for obtaining a fit that minimizes the number of outliers and minimizes the number of descriptors in a given data set. This paper contains technical details, because it is not usual to show this kind of details when using a GPP implementation.
    **#MeTooMA: Multi-Aspect Annotations of Tweets Related to the MeToo Movement**
    With the recent widespread development of social media, a larger number of issues of public interest have been discussed, such as domestic violence, sexual harassment, bias and discrimination, terrorist attacks and so on. As part of this process, several events are discussed in social media which generate posts about these topics. Most of these posts are accompanied by a hashtags, which tend to promote a certain emotion. In this paper, we analyze tweets related to the #MeToo movement in social media, and conduct multiple automatic and manual tests to discover the content of the tweets containing the hashtags in the order of their occurrence in the growing number of hashtags. To reveal the effect of the hashtag relatedness, we develop a multi-aspect topic detection approach based on features like sentiment, positive sentiment, negative sentiment and other specific sentiment cues. In this approach, different topics have different ways of saliency and mention relevance of the hashtag. On the other hand, different aspect topics have different mentions relevance. Experiments show that using more aspect-related features leads to a higher detection accuracy, while using less aspect-related features does not affect the accuracy of detection. We present the results of multiple aspect relatedness-based topic detection approaches, which outperform state-of-the-art one-class approaches, in different datasets.
204. **Computational Induction of Prosodic Structure**
    The ability to automatically generate prosodic information is essential for successful spoken dialogue response generation. This paper presents a new approach to this problem and a new corpus for this problem. It is based on the observation that prosodic information may be represented as the pairwise co-occurrence between linguistic events such as the addition of an adverb. Following this observation, the objective is to discover such co-occurrence relationships in spoken language data in order to automatically generate the input response. We use this approach to generate a set of synthetic (silently) generated examples that can be used for training neural models for prosody generation. A generative model is then trained using these synthetic examples to discriminate real versus generated utterances. We also show how to combine the syntactic and the prosodic sources of information and to effectively combine this with other sources such as word embeddings to improve prosodic-to-syntactic generation performance. We evaluate our approach on the dataset for the MS MARCO project and show that we can effectively incorporate a variety of prosodic sources (embeddings, syntactic features and lexical resources) with a single model. We show that the combined model learns to generate prosodic structure and to discriminate between real and generated speech.
    **A Comparison of Architectures and Pretraining Methods for Contextualized Multilingual Word Embeddings**
205. We consider contextualized word embeddings, and compare the behavior of various architectures, such as convolutional neural networks (CNNs), contextualized max-pooling, and context-aware encoders. This context-aware encoder has been shown to improve word embedding quality in past studies. This work extends this approach, and improves it by using max-pooling instead of convolution. We find that max-pooling outperforms other architectures by a significant margin on 8 out of 10 languages. We also find that more sophisticated architectures (namely attentional encoders) outperform basic architectures. This work opens up a range of interesting directions for developing more advanced models.
    **Indiscapes: Instance Segmentation Networks for Layout Parsing of Historical Indic Manuscripts**
    Layout parsing is a key step in every stage of textual analysis of manuscript, for digitizing manuscripts and assisting scholars in their research. Semantic coherency is the primary goal for layout parsing, while statistical analysis, e.g. identification of regularity and linguistic coherence of the text, are the tools for applying the framework. In this paper, we address these two complementary tasks with a new network architecture and an ensemble method, which we refer to as an instance segmentation network (ISN). The ISN module is a neural network that learns to segment all possible instances of a given document. The ensemble method is a hybrid approach based on the convolutional layers and the sparse attention layer. We evaluated the ISN module and ensemble method on four textual databases, namely, Manuscript5k, ProSparse, MNIST and Microsoft Word. Evaluation results show that the ISN module yields very promising performance in all cases, e.g., the average accuracy of 93.1% is achieved by the proposed ISN for Manuscript5k.
206. **Multilingual is not enough: BERT for Finnish**
    We describe a BERT-based model for Finnish that we have deployed to an NER task. BERT is the state-of-the-art in sentence embedding, and has been shown to be superior in NER compared to a number of other systems. However, there are areas in which BERT underperforms, such as with languages like Finnish, where domain-specific structures and language varieties are prevalent. Our BERT-based model is trained on the standard aligned Finnish corpus and does not rely on any parallel corpus or bilingual supervision. While we were able to show state-of-the-art performance with current limited resources, we show that the use of model ensemble and targeted fine-tuning for highly resource-constrained domains can yield better results.
    **Robust Named Entity Recognition with Truecasing Pretraining**
207. Entities need to be identified from their body text in a text document with high precision to ensure their semantic semantics. Previous studies have shown that entity recognition (ER) algorithms are sensitive to external features and sometimes fail in a natural language document. In this paper, we propose an end-to-end trainable framework, named Truecasing Linguistic Feature (TFLF), to address these issues. The first stage is to learn from the documents with naturally occurring features. The second stage predicts entity recognition score on a large corpus using deep supervision. The top-N entity score in the first stage is propagated to top-N entity prediction score in the second stage, while the relation between entities is preserved in the inner neural networks. To reduce the computational cost, we apply a simple sliding window with the knowledge of the score in one frame of the first stage. Our proposed model yields much improved recognition on the ICDAR2013 dataset (IPDD-2013), and even better than some recent state-of-the-art systems.
    **Characterizing the dynamics of learning in repeated reference games**
    Behavior modeling and explanation have long been considered to be equally important. However, current literature usually addresses this problem by looking at both separately. In particular, behavior modeling is usually described with various kinds of graphs. As a consequence, if behaviors are modeled by graphs, we have to find the effective spatial-temporal frequency of the agents behavior for two consecutive time steps. However, very often there are some situations in which this frequency does not have any regularity. It is difficult to characterize the frequency of a behavior in a sequential setting. Recently, a new method that uses information collected from prior behaviors is proposed. However, it does not provide a way to characterize the dynamics of learning with respect to the prior behaviors. In this paper, we propose a method to characterize the dynamics of learning with respect to the prior behaviors. The method is based on following a distributional logic perspective on sequential learning with two assumptions: i) the optimal agent moves to a given position $x_1$ and is always in the center and ii) the distribution of agent's prior behaviors $f$ is a Gaussian distribution. In this paper, we characterize the relationship between the control dynamic of the learned agent and the dynamics of the prior behaviors of the agents by analyzing the behavior transition function of the control dynamics. Based on this characterization, we propose a new parametrization that includes parametrization of the prior behaviors, and represent it as a generative model for a joint continuous control system. Experiments show that the proposed method can characterize the dynamics of learning in two consecutive sequential learning environments.
208. **Graph-based Neural Sentence Ordering**
    The paper describes a neural sequence-to-sequence model for spoken document ordering. Different from existing sequence-to-sequence models, we extract sentence sequences directly from raw speech input and propose to arrange sentences from them along with natural language instructions, essentially creating a graph. The nodes of the graph represent sentences and edges refer to an ordered sequence. The graph is constructed using LSTM-based encoder-decoder architecture. Experiments on a benchmark dataset have shown that the proposed model outperforms existing methods.
    **Optimized Tracking of Topic Evolution**
209. We consider the problem of sampling noisy topic models using efficient distance measures such as DCT or HCMC. Topic models have proven useful in many applications including topic modeling, cold-start events, review annotation, and document clustering. However, given the diverse and often highly overlapping vocabularies of topics and users, optimal point-to-point model selection becomes challenging. In this paper, we revisit the optimal sampling problem for Gibbs sampling. We show that the optimal sampling problem for a general Bayesian topic model, as well as a natural number of topics, is surprisingly simple. In contrast, we show that the optimal sampling problem for a low-rank Laplace Gaussian Topic Model is highly non-convex, and can be made tractable using standard coordinate ascent and proximal gradient methods. Our results are based on duality theory and manifolds.
    **Improving Knowledge-aware Dialogue Generation via Knowledge Base Question Answering**
    Recently, end-to-end dialogue systems have achieved great success. However, these approaches neglect to integrate dialogue knowledge with the end-to-end model, thus suffer from the low-quality of the generated responses. To overcome this problem, in this paper, we propose Knowledge-aware Dialogue Generation via Knowledge Base Question Answering (K-DQA). Our model models dialogue knowledge via question answering and generates answers by leveraging knowledge from multiple knowledge bases (KBs). In our model, we incorporate both knowledge-aware knowledge inference from dialog history and knowledge from dialog corpus, which are beneficial for question answering. Specifically, dialog history serves as the knowledge history for inference. The knowledge from dialog corpus is used to enhance the accuracy of generated responses. Experimental results demonstrate the effectiveness of our model.
210. **Scale-dependent Relationships in Natural Language**
    In this paper we address the question of how such relations between sets can be expressed. We pose this as an instance of the semantic puzzle of "how to encode relative quantities" in a generic representation (G.I.P.R.). Our goal is to retrieve a set of objects which are similar in at least some sense, when such relations have been specified. We introduce a methodology to study the relation between partial orders and relative quantities in the presence of scale and word order ambiguities. We discuss the contributions of our method and discuss its limitations.
    **Iterative Dual Domain Adaptation for Neural Machine Translation**
211. Translation by domain adaptation uses machine translation models trained with data from a source domain to assist in the translation of a target domain data. It is used to improve the translation of the source data given the model adaptation for the target domain. Traditional model adaptation based on domain adaptation, tries to infer a transfer function for the source domain data given the model adaptation for the target domain. The transfer function is calculated with the target domain data firstly and then the source domain data is inferred for the target domain. However, the traditional method with domain adaptation only considers a domain partition and the source domain translation distribution is relatively small. In this paper, we proposed a novel framework that allows us to evaluate the model adaptation in an iterative fashion based on a refined accuracy and translation accuracy for each target domain, where the iterated accuracy is calculated with the target domain data firstly and then the source domain data is inferred for the target domain. To the best of our knowledge, this is the first time to evaluate the model adaptation in an iterative fashion. The experiments show that the proposed method achieves the best result compared with the state-of-the-art model adaptation methods.
    **Chinese Named Entity Recognition Augmented with Lexicon Memory**
    Recently, it has been shown that the use of the regularized word representations (embeddings) can substantially improve named entity recognition performance. However, learning a regularized embedding for every entity class at every step is considered challenging since this would lead to overfitting, especially for Chinese named entity recognition. This is a serious problem when the normalized embeddings of all classes are used as input for the network. In this paper, we propose a simple and effective method to employ lexicon memory to construct a normalized embedding for every entity class in an unsupervised manner. The method can be applied to any existing named entity recognition model. Moreover, we observe that the normalization method can boost the performance even further when the base-level entity representations are augmented with lexicon memory. Our experimental results show that the proposed method is effective and can boost the performance of state-of-the-art models by 4.6-4.8% on FreeWeibo and 3.3-3.7% on Sina Weibo, and 2.4-2.6% on DSTC7, respectively.
212. **Open Set Authorship Attribution toward Demystifying Victorian Periodicals**
    The paper presents a method for authorship attribution of historical articles based on an open set hypothesis. This method is based on the previous work that attributes authorship by matching the article with a set of labeled historical articles. One such article is identified by querying a web database of labeled articles with the term "author" and observing the frequency of the article's occurrences in similar articles. After training an attribution model with data from the database, the method is used to query unknown articles in the corpus and identify those that are associated with the author. The researchers point out that the proposed method can be used to make comparisons of historical articles from different periodicals in the same topic.
    **M$^2$: Meshed-Memory Transformer for Image Captioning**
213. Image captioning is a challenging task in computer vision. It can learn useful features and generate better captions. The best sequence-to-sequence model can produce image descriptions with language reasoning. However, this kind of model faces two major challenges. First, it has to learn long-distance dependency of visual content and language topics, leading to inconsistency between them. Second, it cannot express the common semantic concepts and discriminative structures of image content and language topics, especially long-distance dependency, since the system may generate long descriptions only with a short description of each image. In this paper, we propose an efficient neural network architecture named Meshed-Memory Transformer (M$^2$) for image captioning. It has a simple and powerful module, called meshed memory, that model long-distance dependencies of image content and language topics. Then, we integrate attention-based sequence generation (BIG-RETAIL) to handle long-distance dependency. Experimental results on four benchmark datasets show that the proposed approach consistently achieves the best captioning performance. In addition, when only short captions are used, M$^2$ achieves the first position on MS-COCO dataset and second place on Flickr30K datasets.
    **DMRM: A Dual-channel Multi-hop Reasoning Model for Visual Dialog**
    Deep learning methods have shown their ability to perform accurate action selection in sequential visual dialog. In the multi-hop framework, which is a particular case of deep learning models, the user is allowed to select actions (each action is the common response from an action set) sequentially in a certain order. In our method, we have implemented this multi-hop reasoning into the DMRM model, where the proposed DMRM model can dynamically adapt its current prediction to reach the most probable goal in the sequence of actions. The experimental results show that our proposed method can perform efficient action selection and reach to the goal more accurately than several baseline methods.
214. **To What Extent are Name Variants Used as Named Entities in Turkish Tweets?**
    This paper explores the use of Arabic name variants as named entities in Twitter. We make an empirical study of the frequency and size of various name variants in Turkish Tweets over a period of 14 months. We use data mining techniques to analyze how popular names, such as "Iftaaq" and "balada" become popular names in Arabic in Twitter. Furthermore, we examine how different named entities (e.g., Turk and Fatemah) are used as linguistic and topical markers. The results demonstrate that there is a significant decrease in the frequency and size of name variants in Turkish Tweets and those particular names are used only in limited contexts. Using the limited list of previously discussed names as linguistic markers, we perform content-based researches into topics such as the linguistic influences of social media, Fatemah and Turk, Fatenuret and Ulusoyut, and the trends in Turkish naming practices.
    **Libri-Light: A Benchmark for ASR with Limited or No Supervision**
215. Automatic Speech Recognition (ASR) is one of the core components of Automatic Speech Recognition (ASR) systems. On the other hand, transcribing speech from text becomes difficult when the amount of data is limited or when supervision on training data is unavailable. Therefore, in this work, we propose a novel method for ASR using only unlabeled speech data without using additional speech recognition output. We train a bidirectional transformer network on unlabeled speech with a single-output layer (then switches to the output layer), and find that despite only utilizing speech signals, the model outperforms the previous best method with no ASR data or supervision. This also enables this method to take advantage of unlabeled speech data with small amounts of supervision.
    **The performance evaluation of Multi-representation in the Deep Learning models for Relation Extraction Task**
    The relation extraction task is considered as one of the key tasks in the information extraction task. In the article, the performance of Multi-representation (MR) algorithm is evaluated on the relation extraction task using various relation extraction models such as BERT, ELMo and SHAP. A comprehensive dataset on 4 features, 10 relation types and 20 relation types is utilized for training MR algorithms. Finally, the performance of these models is tested by several benchmark test sets. Experimental results show that Multi-representation model achieve the best result in the benchmark test sets, which are two test sets of Portuguese (news, soccer) and Japanese (medical) domains respectively.
216. **A Cycle-GAN Approach to Model Natural Perturbations in Speech for ASR Applications**
    Recently, researchers have employed image processing algorithms for generating adversarial examples for various speech recognition tasks, including ASR. This is a natural and effective method for generating adversarial examples, however, one limitation is that the image quality is only perturbed when it is noisy. In this paper, we propose a novel deep-learning method, Cycle-GAN, to generate natural perturbations in speech data for various ASR tasks. The proposed method consists of two stages. The first stage utilizes the ability of cycle-based generative adversarial network (CycleGAN) to generate perturbations in low-quality speech data. In the second stage, the perturbed speech data are used to train a different classifier to find out the high-quality speech data. The second stage uses the modified ASR system that does not require a masked low-quality speech segment. Experimental results show that the proposed method is effective in generating natural perturbations in speech data for various ASR tasks.
    **PEGASUS: Pre-training with Extracted Gap-sentences for Abstractive Summarization**
217. Abstractive summarization methods generally learn a linear summarizer from a large corpus of text. However, many real-world summarization tasks require abstractive information processing, and not only about the content of the document, but also about the information the document contains. In this paper, we propose a novel approach that exploits partially (and sometimes fully) generated gaps from open domain discourse, such as in a news article. These gap-sentences form an objective assessment of information, rather than a homogeneous "English sentence" summarization. Therefore, we can leverage a large corpus of strong text to train a pre-trained neural sentence summarizer. Our experiment on two well-known summarization tasks shows that our method outperforms recent state-of-the-art approaches on each task, for three different factors of variation in the training data and for different evaluation metrics.
    **Identifying Adversarial Sentences by Analyzing Text Complexity**
    Deep learning models are susceptible to adversarial examples, automatically generated samples crafted to fool a classifier. Adversarial examples are a particular challenge for models that rely on large number of training samples. In this work, we present an algorithm to identify adversarial sentences using a minimal amount of training data. Our method leverages language features, including lexical entropy and word embeddings, to identify difficult-to-parse adversarial sentences and uses an attention mechanism to discriminate such sentences. We evaluate our method on three tasks of semantic parses, semantic relatedness, and English model classification. Experiments demonstrate that the proposed method is capable of automatically identifying adversarial sentences in human-annotated datasets with minimal amount of human effort, even for hard-to-parse adversarial sentences.
218. **End-to-end training of time domain audio separation and recognition**
    In this paper, we propose an end-to-end trainable system for audio separation. Our system can be trained using a combination of a time-domain sequence model (TDSM) and a deep learning model (DML). Unlike the existing end-to-end architectures (which require extensive preprocessing) that rely on a pre-trained audio separation model, our system is unsupervised. Hence it can be trained without any audio separation pre-processing. Experimental results show that the performance of our system is comparable to the performance of existing approaches, while with many more parameters and parameters to be tuned (e.g., the data augmentation, parameter adjustment and data augmentation strategies). In particular, the performance of our proposed system is much better than the performance of some other highly-trained models, while keeping a good performance on our standard performance metrics (e.g., highest signal-to-noise ratio and highest mean absolute error). Moreover, this work can be used to explore the limits of the unsupervised training of deep learning models.
    **Generating summaries tailored to target characteristics**
219. Automatic summarization can be divided into two different tasks: (i) generating a short textual summary of the document, and (ii) selecting a subset of sentences that best describe the document. This paper studies a new aspect of summarization, i.e., generative modeling. To this end, we propose a probabilistic generative model for summarization. The model takes as input a set of documents and outputs a probability distribution over its elements, where the probability probability is decomposed into a probabilistic component and a marginal component. The marginal component captures inter-sentential and inter-group relations, and is formed using a joint latent factor. The generative model thus learns to capture a rich set of transferable characteristics that can be transferred across documents. The contribution of this paper is threefold: first, we study the quality of summarization generated by the model, which shows that the model generates summaries that are also transferable; second, we use our model to generate short summaries for two popular dataset types: news and technical reports; and third, we evaluate the relevance of our results to several types of summarization tasks, including essay scoring, movie summaries, and scientific abstracts. Experimental results show that the model has a significant impact on these tasks.
    **A Survey on Document-level Machine Translation: Methods and Evaluation**
    In recent years, the use of machine translation (MT) in human resource management (HR) systems has increased rapidly. While much attention has been paid to MT methods that rely on human in the loop, existing methods are still not very robust in high-resource settings, where an overwhelming number of training documents are available. We review recent literature that addresses the shortcomings of existing methods. These include novel re-scoring techniques, re-training methods that leverage high-resource text, and new MT model architectures. Furthermore, we provide a survey of the MT evaluation methodologies that have been implemented and evaluated on document-level data. We argue that the literature on document-level MT is still immature and must be reviewed more thoroughly for two reasons: First, MT evaluation is very similar to traditional domain-independent evaluation, which means that a lot of systems can already be evaluated on different evaluation tasks. Therefore, it is difficult to compare results of different MT methods. Second, evaluation suffers from noisy and conflicting evaluation tasks. In this survey, we discuss these challenges, survey the recent evaluation methods, highlight the limitations of the evaluation methodology and the open problems. In addition, we discuss promising research directions and future research directions in document-level MT.
220. **Towards an automatic recognition of mixed languages: The Ukrainian-Russian hybrid language Surzhyk**
    The literature on Mixed Language Social Network (MLS) research has been growing for some time now. In this paper, we propose an automated method for the recognition of mixed languages on the web. In this approach, we first extract information from a corpus of 15 million posts on a Ukrainian-Russian language hybrid sports website (yuli-zhob.uk) and two English language sports website (ncaa.in and espn.go.pl). Then we extract features of each post using a hidden Markov model and train a new classifier for the Ukrainian-Russian hybrid language domain. The resulting classifier achieves an accuracy of 0.28. Moreover, the proposed method can also be used to extract the linguistic features of the social network data. It is able to accurately predict the language of users even if their language is not their native language. The automatic system performs as good as the proactively initiated prospection systems.
    **Macaw: An Extensible Conversational Information Seeking Platform**
221. Macaw is a scalable, end-to-end neural conversation system developed at Google for work, private, or academic purposes. Macaw presents a conversational recommender system with rich capabilities to achieve deeper understanding of user preferences. It provides a flexible framework for integrating and building information seeking systems with conversational end-to-end approaches. For data, it leverages a large collection of candidate answers from Google search with rich inferencing capabilities. With more than a billion conversation interactions in the first four months, Macaw achieves a normalized word error rate (WER) of 2.5%, a 13.7% relative improvement over a state-of-the-art conversational system built at Pinterest. Compared to other end-to-end systems, it is 1.5% WER better, and 5.9% better than the previously reported top end-to-end system.
    **Discriminative Sentence Modeling for Story Ending Prediction**
    In this paper, we present a comprehensive study on learning end-to-end story understanding from movie scripts. Our model, Discriminative Sentence Modeling (DSM), is able to discover local word pairs shared by many story endings from all movies. In addition, DSM learns to perform cross-movie entailment as a pre-processing step. Additionally, we perform extensive experiments to analyze the two-stage reasoning in movie scripts, comparing the behavior of DSM to several strong baselines, and how information obtained from each stage of reasoning influences each other. Our experimental results demonstrate that, when pre-processing cross-movie entailment information, DSM outperforms the baseline model by 15.16% and 26.55% in ROUGE-1 and ROUGE-2 scores respectively, and that the model's attention model learns better topic saliency information.
222. **LSTM-TDNN with convolutional front-end for Dialect Identification in the 2019 Multi-Genre Broadcast Challenge**
    Dialect identification is a useful research problem for monitoring and improving the quality of broadcasting services in countries that have limited linguistic resources. Accurate dialect identification is the foundation for automatic speech recognition systems. In this paper we present the methodology of our team at the 2019 Multi-Genre Broadcast Challenge, based on the addition of a fully convolutional front-end for text-to-speech model, as well as applying temporal recurrent neural networks. The system uses Long Short Term Memory networks with dropout to learn the time dependency of all the text segments of a broadcast. The LSTM-TDNN architecture has been considered as a suitable model for speech recognition in this context, especially in relation to computational cost. The pipeline which was used to train the model as well as the evaluations on the TIMIT corpus were presented.
    **Going Beneath the Surface: Evaluating Image Captioning for Grammaticality, Truthfulness and Diversity**
223. Exploring the full extent of the accuracy gap between image captioning and human annotation is challenging because the set of possible captioning approaches is much larger than existing manual metrics. In this paper, we go beyond the limited literature on domain adaptation by systematically evaluating image captioning systems for three language pairs, English-German and English-French, as well as a new language pair, English-Czech. This article summarizes the evaluation methodologies, summarizes the captioning system results and gives an overview of the key empirical findings. This work should motivate new investigation of image captioning because of the small set of benchmark dataset and the generally high error rates observed in the literature.
    **DP-LSTM: Differential Privacy-inspired LSTM for Stock Prediction Using Financial News**
    News developments have always been helpful to stock market analysts to gain a deeper insight about how the stock market is performing. There is a need to combine the analysis of many news to provide an accurate prediction for an unknown future. An automatic trading system that can predict the future price of stock can be a promising solution. Stocks can be predicted using news such as news about the company's previous annual reports. News can also be used to predict future news from different stocks in a day or a few days. However, this paper is devoted to differentially private LSTM-based LSTM for stock prediction that uses news to avoid revealing the private information in news records. The standard LSTM model uses static input features in order to extract relevant features and use the extracted features to make prediction. However, all information that is extracted and stored by LSTM and then fed to LSTM will make prediction as private. In order to build a differentially private LSTM model, the private information of news records is removed. The proposed differential privacy LSTM uses a differentially private LSTM that takes the news features and keeps the other private information in the original LSTM model. In this way, the proposed model is differentially private. Experimental results have shown that using private information extracted from news records can be very effective for stock prediction.
224. **Neural Simile Recognition with Cyclic Multitask Learning and Local Attention**
    We introduce a novel framework to learn the attention of a convolutional neural network by jointly training a single neural network that takes the common phrase or series of phrases as input and generates the common phrase representation for a subset of the input phrases. The learning module takes the knowledge from a sequence of phrases and can output the common phrase representation for any individual phrase, which can be used to solve the common phrase recognition task. We use a special formulation of the neural multitask learning, in which the neural network and attention module co-learn to adapt to the incoming inputs. We then exploit an unsupervised local attention mechanism to extract the common phrase representation. In this way, our model learns an attentive representation of the phrases in an unsupervised manner. The attention module is implemented using a cyclic multitask learning strategy and a random disjoint sampling method with a fixed global weight initialization. The performance evaluation of our model on three datasets including a popular and long-tailed dataset show that our model can effectively learn the attention and can effectively classify multiple common phrases.
    **Gaussianity and typicality in matrix distributional semantics**
225. The goal of this paper is to investigate (1) the link between Gaussian distributions of mathematical matrices and their typicalities, and (2) the role of generic properties on matrix frequencies. Using the theory of binomial distributions, we show that the Gaussian distribution on a matrix, even if random and independently distributed, exhibits a universal mean and universal variance. We also establish a novel connection between the conventional Gaussianity criterion for word-likelihood estimation, and the standard maximin expectation.
    **Annotating and normalizing biomedical NEs with limited knowledge**
    Biological NEs occur in all tissues and organs, from peripheral blood to the brain and nervous system. For applications in clinical trials and neuroscience, these NEs may be expressed in sequential data of various lengths, structured as multi-dimensional biological structures and associated to biological inputs. In order to more effectively utilize biomedical NEs in prediction and disease progression, we need approaches able to unsupervisedly recognize the basic structures of such data. The needs of large NEs require data representations that are interpretable. However, there are very few work tools that can fully unsupervisedly recognize multi-dimensional biomedical NEs and can thus provide informative and useful representations. In this paper, we provide a framework that allows efficient handling of large-scale multi-dimensional biomedical NEs, with little manual effort and low computational effort, using an annotation and normalization mechanism. To address the issue of how to effectively annotate large-scale multi-dimensional biomedical NEs, we introduce a novel annotation scheme that reflects the hierarchical nature of multi-dimensional NEs. As a case study in this context, we annotate a real clinical dataset with a multi-dimensional NE from 1-D long trajectories. The contribution of this paper is the development of annotation schemes to allow for the computational efficient annotation of such datasets, and to introduce multi-dimensional NEs that are more interpretable for downstream application purposes. We explore the application of such methods for disease progression prediction, with several other important applications of large-scale multi-dimensional NEs in clinical trials, neural connectivity in the brain, and animal learning. The multi-dimensional NEs learned by our annotation scheme are interpretable and useful, with application in decision-making, self-organization, and brain function prediction.
226. **Towards a Philological Metric through a Topological Data Analysis Approach**
    Many approaches to textual analysis like Lexical Analysis, Morphological Analysis, Morpho-logy, Semantic Analysis, among others have been proposed to make use of the semantic structure in a corpus. Semantic similarity is a quantitative measure of how similar a lexical expression is to a word in a language. One of the most common approaches is by measuring similarities with semantic distance. However, it is not the canonical form of similarity. In this paper we introduce a new metric of semantic similarity using a topological approach and an embedding of symbols into a vector space. We propose three different measures to evaluate the similarity between symbols and data: Mean Distance, Coefficient of Similarity and Semantic Similarity. On two standard benchmark corpora we demonstrate that the proposed metric is very accurate and effective.
    **BERTje: A Dutch BERT Model**
227. Deep learning has made tremendous progress in the past two years in various areas such as natural language processing, speech recognition, machine translation and many others. However, applying machine learning methods in order to identify specific criminal behavior of the individuals or other crime classes requires an aggregation of training data into the database, from which it is possible to find out a detailed semantic description of each crime. The task is a fundamental one in order to conduct accurate prediction of criminal behavior for new cases, that is, new crimes that have never been seen during the training process, and to collect crime patterns from existing cases. However, due to the limited amount of training data available, such a task is very difficult, not only for training and testing but also because the data are often incomplete. Therefore, it is very important to model the individual background and interests of the individual case in order to estimate the accuracy of prediction. In order to address this problem, we developed a novel background modeling model, named BERTje, based on the Deep Reinforcement Learning algorithm BERT. BERTje consists of three modules: a latent variable extractor, an individual model based on the LSTM (long short-term memory), and an ensemble model based on the Random Forest (Forest). It utilizes the two existing methods: the LSTM is based on the reinforcement learning algorithm and extracts the main structure from input sentences. The individual model is based on a Long Short Term Memory (LSTM) and is trained to classify the sentence containing the entity in question and predict its content. It outputs a score for each prediction based on the learned document characteristics and the mutual information between these two models. Experiments performed on the Netherlands Robust Identity Database (NRID) database, the publicly available victimization prediction dataset from the LUT Policing Fraud Database, show that the proposed BERTje model can outperform standard ensemble models.
    **RIMAX: Ranking Semantic Rhymes by calculating Definition Similarity**
    Keyword-based word embeddings represent words as a vector of word vector representations. The semantics of the word are used to compare the vector representations in two terms: (i) similarity between the vector representations of the two words, and (ii) relatedness between the two words, both by calculating the co-occurrence vector. Such methods have shown their effectiveness in several tasks such as machine translation and sentiment analysis. However, with the rapid growth in the number of the provided word vector representations, the learning performance deteriorates significantly. The problem is that in the relational embedding model, the vectors of words are usually normalized to represent vectors of the same word vectors. But to respect the properties of the relational embedding model, this norm is difficult to compute. To address this problem, we present a simple approach to define a threshold on word vectors and compute the similarity between a vector of a word and its neighbor vectors, that directly preserves the properties of the relational embedding model. It is evaluated on two real-world datasets: TED Talks and the Wikipedia. The experiments show the effectiveness of our proposed method compared to state-of-the-art semantic approaches.
228. **An End-to-End Dialogue State Tracking System with Machine Reading Comprehension and Wide & Deep Classification**
    Dialog State Tracking (DST) is a critical task in many intelligent systems. Therefore, it is crucial to enhance the robustness of these systems. This paper presents a DST system with: (1) a readout dialog system for direct reading comprehension of the utterances and (2) a text analysis model, deep reading comprehension neural network (DRNN), for broad and deep classification of dialogue acts. These DST systems are evaluated on two large data sets: the Switchboard corpus and the DST Corpus. The DST systems achieve high performance (and state-of-the-art results) in both supervised and unsupervised tests, including two reading comprehension tasks with up to 35k dialogues each. Moreover, we construct a wide & deep classification model for DST based on wide beam style images (WBAs), which can achieve a state-of-the-art result in the open-domain machine reading comprehension setting.
    **Pretrained Encyclopedia: Weakly Supervised Knowledge-Pretrained Language Model**
229. The thesis aims to improve language modeling by integrating prior knowledge with supervised models. We present a simple yet effective approach that takes the usage of articles on Wikipedia as the supervision signal. In the first step, we remove irrelevant articles and train a pretrained language model by copying the article paragraphs to the document in a clean textual environment. To obtain the similarity score between the training corpus and clean corpora, the reference corpus is used as a plug-in corpus. During the second step, we restore the fine-tuned language model and perform a robustification on the importance scores. The final text is then constructed from the cleaned sentences by a cross-entropy penalty method. Experimental results show that our model improves the language modeling accuracy by about 4.7% on the Stanford Natural Language Inference Dataset. We also show that our method can be used to pretrain language models based on knowledge extracted from Wikipedia.
    **Hierarchical Character Embeddings: Learning Phonological and Semantic Representations in Languages of Logographic Origin using Recursive Neural Networks**
    We present a deep learning architecture for hierarchical character embeddings (HCE) in languages of logographic origin and show that this architecture is able to learn both phonological and semantic representations. Our model learns a hierarchical representation of characters that embeds phonological features into a fixed dimensional latent representation and in turn improves performance on phonological word similarity tasks, producing character embeddings that closely resemble phonological representations.
230. **SberQuAD - Russian Reading Comprehension Dataset: Description and Analysis**
    Current reading comprehension models rely on the global context of all the sentences to predict the answers. However, considering sentence comprehension as a prediction task may lead to an incorrect understanding of a sentence. Moreover, considering context in a joint way (rather than a series of independent sentence representations) can make it possible to overcome the drawbacks of the global context. In this paper, we describe and analyze the SberQuAD dataset which consists of real reading comprehension and natural language inference tasks. The dataset consists of 500k synonyms, 105k entity mentions, and 1M sentences. We evaluated the dataset using both a state-of-the-art model and other existing joint multi-task learning models. Our experiments indicate that, with joint multi-task learning, it is possible to outperform the state-of-the-art methods with a negligible drop in performance. Furthermore, as a byproduct of our study, we propose a joint learning model which considers all the words in the sentence together to predict the answer. We show that this model can provide a competitive baseline performance.
    **Learning Singing From Speech**
231. In this paper, we investigate the extent to which domain adaptation, which is an adaptation technique for a source domain to a target domain, can be helpful in improving the performance of state-of-the-art acoustic modeling approaches on the singing voice. In particular, we explore how domain adaptation helps to optimize the acoustic modeling of singing voice on the basis of multiple, learned prosodic characteristics of the singing voice. For this purpose, we construct a new dataset, called {\bf Singing Voice Prosody Dataset}, that includes 100,000 singing voice spectrograms sampled at 12 different pitch directions and recorded by various sources. Moreover, we propose a domain adaptation approach to improve the accuracy of the acoustic modeling of singing voice for the second approach, by (1) initializing the acoustic model with prosodic features of the source, and (2) fine-tuning the acoustic model on the new features. In doing so, we achieve relative improvements of 14.4% and 10.8% on the \sf{Captioning} and \sf{Smile} speech recognition tasks, respectively, with only slight degradation on the \sf{Lily} speech recognition task.
    **Modeling Intent, Dialog Policies and Response Adaptation for Goal-Oriented Interactions**
    Goal-oriented dialog is a natural extension of conventional dialog, where the goal of the agent is to achieve a goal while interacting with the human. Prior work in goal-oriented dialog has focused on implicit goals or heuristic retrieval of such goals. Intent-oriented dialog, on the other hand, is more challenging to model. The goal-oriented AI community has recently proposed methods to represent and reason about natural language intentions, but very little work has been done on model learning in this domain. Here, we propose methods for building goal-oriented models in a hierarchical manner that allows for both natural language understanding of the human (an implicit goal) and goal-oriented dialog context modeling (a goal-oriented dialog context). In an online setting, our proposed methods were able to provide a dialogue policy that was consistent with several human evaluations.
232. **Leveraging Topics and Audio Features with Multimodal Attention for Audio Visual Scene-Aware Dialog**
    For multi-turn dialogs in virtual reality, an audio visual scene-aware dialog system needs to provide both audio and visual information (e.g., audio attributes and visual semantics). Previous works mainly focused on either audio visual scene-aware dialog or audio text scene-aware dialog. In this paper, we propose a novel Audio Visual Scene-Aware Dialog (AVSD) framework for audio visual scene-aware dialog. In the proposed AVSD framework, a caption model is employed to generate an audio visual scene description that is incorporated into the dialog context. Different from previous work, we propose to model dialog content as a set of overlapping concepts. With this, our proposed AVSD framework incorporates audio visual scene-aware dialog into a single loop, and thus, jointly model the dialog content and language features to improve dialog quality. We also introduce a multimodal attention method to measure the importance of different modalities for the task, which could be flexibly applied in multi-task training. Experimental results demonstrate that the proposed AVSD framework achieves state-of-the-art performance on the Multi-Turn Audio Visual Scene-Aware Dialog (MVSD) dataset.
    **Exploring Context, Attention and Audio Features for Audio Visual Scene-Aware Dialog**
233. With the rapid increase of large-scale multimedia data on the Internet, a significant proportion of dialogues in dialog systems are on-demand mediated by visual scenes. However, most of existing chatbot research has focused on augmenting visual dialogue by using sophisticated interaction mechanisms to elicit responses to user dialogues, thus ignoring its richer potential for interactions with scenes and object classes. In this paper, we propose a new approach that encodes dialogues based on rich visual and textual cues, then learns to infer visual object class association without necessarily relying on any training images, nor any scene annotations. We perform an extensive investigation of different factors of dialog system performance in terms of learning models, inference strategies and active recognition strategies. Finally, we demonstrate our findings on the new publicly available dataset of over 1 million dialogues, and show that our approach not only outperforms other state-of-the-art visual dialog systems for on-demand mediated dialog, but also outperforms existing object-based systems.
    **When to Talk: Chatbot Controls the Timing of Talking during Multi-turn Open-domain Dialogue Generation**
    Open-domain dialogue systems are evolving rapidly due to rapid development of dialogue learning methods, which enable learning of dialogue states and behaviors from human feedback and experience. However, most existing deep learning dialogue systems do not leverage the temporal structure of human-human conversations. This work presents a new neural model that automatically models the temporal structure of human-human conversations to create a better open-domain dialogue system. Our model integrates rich contextual information to overcome the weakness of previous dialogue systems with a hierarchical recurrent model. We analyze the discriminative power of recurrent neural networks on different temporal linguistic structures and observe that recurrent neural networks with attention maps learn better temporal representations. Furthermore, we show that the sentence sequence space of contextualized sentence representations, in addition to the sentence sequence space learned by conventional sentence encoders, also improves the deep conversational model, especially at a low resource setting. We release our neural models and chatbot interaction datasets, and analyze the proposed model on benchmark data sets. Our experimental results demonstrate that the proposed model can generate more relevant, natural and engaging dialogue responses.
234. **Candidate Fusion: Integrating Language Modelling into a Sequence-to-Sequence Handwritten Word Recognition Architecture**
    A hand-crafted feature space for the handwriting recognition task is commonly exploited to improve performance. Although the robustness of handwriting recognition to a vocabulary of previously unseen letters has been demonstrated, there still exist other sources of hidden information for improved performance. In this work, we propose a novel method of combining the modelling capability of a sequence-to-sequence neural architecture with language modelling to increase the robustness of the underlying system. Our evaluation shows that our method yields substantial improvements on several challenging datasets and can improve the performance of a character-level model by up to 11%.
    **Combining Context and Knowledge Representations for Chemical-Disease Relation Extraction**
235. We introduce a novel two-stage chemical-disease relation extraction system that is based on an integrated context representation and knowledge extraction modules. We develop a novel process architecture for the two tasks that are integrated using a bottom-up pipeline. This architecture employs ensembles of local word embeddings with contextual information. We evaluate the quality of each embedding for chemical-disease relation extraction and for quality control. We integrate context information into knowledge extraction in three different ways: using knowledge graphs as context representations, using regular expressions for context representations, and exploiting auxiliary knowledge bases. We show that our system outperforms state-of-the-art methods on six publicly available datasets. We also propose a technique to control the quality of each context embedding, as well as a setting where context embeddings are guided by the form of the query. Finally, we evaluate our results on a new benchmark dataset that is built from comprehensive clinical context. The new benchmarks show that our system is able to outperform prior methods, particularly in relation extraction.
    **Knowledge-guided Convolutional Networks for Chemical-Disease Relation Extraction**
    Existing machine learning models for chemical-disease relation extraction focus mostly on relations between chemical compounds and disease. However, chemical research is a multimodal task, where relations can also be between different types of diseases and they can be extracted using different machine learning models. To extract compound-disease relation pairs, we propose to perform knowledge-guided convolutional networks on each query and extract the representations of disease-conditions pairs. In this context, the structural similarity between diseases and conditions is exploited to provide a translation from one disease domain to another. The experimental results on the SimpleDisease and FreeChem corpora show that our model outperforms the state-of-the-art models.
236. **Harnessing Evolution of Multi-Turn Conversations for Effective Answer Retrieval**
    This paper introduces a Multi-Turn Conversational Evolution approach (MER) which effectively captures the relational changes in conversation. The MER model is used to extract the hierarchical dialog context which contains the whole underlying complex dialog flow. Then, the fuzzy relation between the extracted context and the question is directly embedded to model the answer retrieval task. The obtained model outperforms the state-of-the-art answer retrieval algorithms and is comparable to the state-of-the-art human-in-the-loop approach.
    **Emotion Recognition from Speech**
237. Automated speech emotion recognition has recently gained attention. This is mainly due to the availability of large-scale emotion recognition datasets, whose vast majority is available in controlled conditions. However, it is not clear whether the performance of such systems is actually affected by these conditions, as a real-world scenario of emotion recognition is not straightforward and the output of a system may be affected by confounders as well. This work explores whether an emotion recognition system can be effectively used when the context of the utterance is controlled, i.e. its emotional content is known and controlled by the speaker. We investigate the impact of different sources of information on emotion recognition performance. Experimental results show that emotional information is not a limiting factor in the performance of emotion recognition systems. Moreover, the use of controlled emotion information allows us to analyze the impact of different emotion words in terms of the speech signals on emotion recognition systems.
    **Siamese Networks for Large-Scale Author Identification**
    In this paper, we present a novel and flexible architecture for large-scale author identification. We embed a semantic embedding model as a deep neural network with a convolutional residual network as the feature extractor. Our architecture can be trained end-to-end and learns semantic word embeddings via nearest neighbor search. The architecture enables the actor network to better exploit context information for predicting the author label. In addition, we employ a self-attention mechanism to learn intermediate representations and aggregate information. Experiments on four popular large-scale authors datasets show the efficiency and effectiveness of our approach.
238. **Semantics- and Syntax-related Subvectors in the Skip-gram Embeddings**
    We present a semantic- and syntax-based analysis on embeddings of semantic relations and sentence templates extracted from skip-gram model, and study their properties. We show that these two relations can be used to generate a compositional vector space. We discover that the "queens" of this space are syntactically neighboring, and that different relations can be expressed in different syntactic structures. In addition, we investigate different dimensions of syntactic structures, which we use to construct different syntactic subvectors, which we evaluate on similarity and coverage. Finally, we compare syntactic embeddings with three standard machine learning models: the language model (LM), word2vec embeddings and the traditional word embeddings. We also compare the model performance in terms of classification accuracy, different relation parsing performance, semantic and syntax-based similarities and dissimilarities, and simplicity of parsing. We have achieved state-of-the-art results in both similarity and coverage for both syntactic and semantic relation identification.
    **Probing the phonetic and phonological knowledge of tones in Mandarin TTS models**
239. Deep learning-based systems have achieved remarkable performance on speech recognition (SRE). However, their use has been largely restricted to cases where individual labels (like Mel-1) are encoded as complex discrete attributes. In this work, we present a new and challenging application of deep learning-based systems for Mandarin TTS, which aims to infer utterance label for each of the 1000 possible tone sequences during utterance synthesis. We show that in contrast to conventional systems, these models have the capacity of generating realistic tone sequences as a result of learning to map from a joint probability distribution of the TTS models to the languages label. A more challenging assumption is that the states of the model must be well specified in advance. To support this experimentally, we have released all of our training and testing data from our previous work. Furthermore, our experiments demonstrate that our system outperforms existing models in terms of representing the utterance label and has the capacity to accurately reconstruct tones.
    **Improving Abstractive Text Summarization with History Aggregation**
    Contextual information is crucial for text summarization. Although a lot of approaches have been proposed, they are unable to extract informative sentences from complex texts. History aggregation, a form of attention mechanism, has been proposed for this task. However, previous work has focused on specific types of source and target sentences, which are not considered in this paper. We find that historical summaries of several texts can be extracted with the help of a set of hard-wired filters. The proposed model aims to use these filters to predict the importance of certain words in a source document and then aggregates the importance weights of these words based on a context matrix. Our experimental results show that the proposed approach can outperform several baselines.
240. **FALCON 2.0: An Entity and Relation Linking Tool over Wikidata**
    The introduction of entity and relation information is enabling the comprehensive and richer understanding of complicated natural-language structures. This paper describes FALCON, a rich and open source knowledge graph extension to Wikidata, to support this cross-media semantic workflows. Our approach involves three steps: First, an entity information embedding is extracted from all documents in a specific domain. Second, using this entity embedding as an initial knowledge base, the built knowledge is linked to a second entity information tagset and refined on the fly. Third, the new information tagset is used to create a few mentions of the newly linked entity and links between other related mentions. FALCON supports lightweight user interfaces and multi-lingual data ingestion, and offers high precision and useful embeddings. It is the first fully open source knowledge graph tool to allow such embedding interaction and cross-media semantic linking of all kinds. We demonstrate the efficiency and efficiency of FALCON on three different Wikipedia knowledge graphs: People, Places, and Events.
    **THUEE system description for NIST 2019 SRE CTS Challenge**
241. This paper describes THUEE system description, submitted to NIST 2019 Speech Technology Systems Technology Conference (STC) as an adaptation of the original THUEE system developed in DC this year. The THUEE system is composed of three sub-systems, each with individual methods to achieve the intended task. The text generation method is developed to encode speech intelligibly and follows a two-stage methodology; it involves a language modeling and a deep learning-based text recognition step. The language modeling is formulated as a sequence-to-sequence model and the speech recognition is based on neural networks. The two-stage approach supports speech recognition without any lexicon or speech recognizer. In addition, we experimented with two-stage models to improve upon the linguistic capability of the systems and achieved competitive performances.
    **Convolutional Quantum-Like Language Model with Mutual-Attention for Product Rating Prediction**
    This paper presents a deep convolutional quantum-like language model, with mutual-attention to predict a product rating score from online reviews. The model is an extension of quantum word embedding model that takes a piecewise constant product of frequency and entropy vectors to encode the content of words. It is trained with the task of rating prediction in the bottom layer of neural network in order to predict a product rating. The proposed model has been applied to both Chinese and English reviews to improve prediction accuracy. Experimental results on two Chinese and three English datasets show that the proposed model can significantly outperform previous state-of-the-art products-rating prediction models, especially for smaller Chinese and English-speaking markets.
242. **A Study of Multilingual Neural Machine Translation**
    Neural machine translation has shown very impressive performances over a wide range of languages. In contrast, most previous studies on neural machine translation simply combined a multilingual machine translation system with a parallel language translation system. Such a system has two high-level components: a multilingual neural machine translation decoder and a parallel language machine translation decoder. In this paper, we investigate the ability of multilingual neural machine translation to learn parallel translations with minimal cost. Our results show that the proposed neural machine translation system achieves a certain level of translation quality at low computational cost.
    **N-gram Statistical Stemmer for Bangla Corpus**
243. This paper proposes an efficient N-gram statistical stemmer which enables the search for an exact match of a given context word phrase. The proposed system incorporates two phases. In the first phase, it parses a sentence into n-grams, thus the root word phrases are removed and then the complete phrase is searched for in the dictionary. The second phase takes the extracted n-grams to build a tree and the root word phrase is attached to each node in the tree. The efficiency of the system is evaluated over a test set.
    **Make Lead Bias in Your Favor: A Simple and Effective Method for News Summarization**
    When extracting relevant sentences to provide a quick summary for a news story, the task is difficult because the summary itself is ambiguous. The texts and sentences belong to different but related news sources which require an interpretive process for the generation of short summaries. Different traditional summarization methods aim at extracting one-word summaries, which are not representative enough for summaries of different news sources. We present a new method for automatically generating a summary which is effective at covering the information of different news sources. Our method extracts sentences from the news sources which are related to the query, and applies a two-stage reasoning process to find the semantically appropriate sentences. In the first stage, we extract the sentence set from the given news source, and then we match them to a tree-based vocabulary based on the extracted sentence set. In the second stage, we extract the sentence sets from the newly generated tree. In order to apply the tree-based sentence extraction, we created a training corpus for a named entity recognition system. The training corpus can be used for learning and evaluation of the proposed method. In our experiments, we evaluate the effectiveness of the proposed method in a multi-source scenario and an extractive scenario. Our experiments demonstrate that the proposed method can achieve superior performance compared to state-of-the-art approaches. In the extractive scenario, the proposed method can extract more relevant sentences than the state-of-the-art approaches.
244. **A statistical test for correspondence of texts to the Zipf-Mandelbrot law**
    A statistical test for correspondence between texts to the Zipf-Mandelbrot law is proposed. The testing sample consists of a random sample of texts and the so-called pairs, corresponding to the occurrence of two occurrences of two textual phenomena (observations) in the texts. To evaluate the proposed approach, a statistical test for correspondence between a text and a lower dimensional time series of time-varying parameters (measures) of the text, in terms of their average measures (quantities) and their order, is introduced, based on Gewirtz and Jacobi's law. These quantities are nonlinear functions of the piecewise polynomial activation, which themselves are generated from the set of parameters of the text. The proposed method provides a framework for further statistical tests for text correspondence, and thus also for the study of text nonlinearity and its relation with other dimensions in the text.
    **Explicit Sentence Compression for Neural Machine Translation**
245. Neural machine translation (NMT) usually needs to compress the sentences in order to deploy it to mobile devices. However, it is less guaranteed that these compression strategies are directly useful for decoding performance. In this paper, we introduce an architecture that directly benefits from these compression strategies. It enables NMT to generate more compact sentences by explicitly compressing them. We evaluate the compression effects of our model on common sentence compression benchmarks and competitive NMT systems. Our experiments show that our model is effective, when compared to alternative compression strategies. The source code is available at https://github.com/sfzheng001/Sentence-Compression.
    **Language Independent Sentiment Analysis**
    There has been growing interest in characterizing the social attitudes of individuals by analyzing their writing style. The ability to capture such sentiment patterns is essential for people to evaluate the sentiments of sentiments, understanding and knowing the social sentiments of individuals. In this paper, we propose to perform language independent sentiment analysis (LISA), which is composed of two tasks, characterizing the sentiment polarity of individual words and classifying individuals into positive and negative sentiment groups. We have performed the experiments on a corpus collected from Twitter. The baseline is to perform either task separately, and then combine the two tasks to make a final classification. However, a better approach is to perform the two tasks in an efficient way. In this work, we present a novel approach to perform the two tasks simultaneously. We describe a convolutional neural network model based on Long Short-Term Memory (LSTM) which is used to classify individual words by using a hidden state state, and then fusing these states to generate the final sentiment polarity scores. Experimental results have shown that our approach outperforms the baseline approach in both tasks.
246. **Visual Agreement Regularized Training for Multi-Modal Machine Translation**
    Multiple modalities of text contain complementary information, and it is crucial for multi-modal Machine Translation to exploit these complementary information. In this paper, we propose a novel regularization method for multi-modal Machine Translation (MMT), named visual agreement regularized training (VAR) based on multi-modal word vector. As the training data is often not aligned well between different modalities, we propose a method of alignment refinement by aligning the test data with a pre-trained translation model. The resulting multi-modal model is supervised by a student model for cross-modal pairs. Moreover, we proposed a novel strategy for knowledge transfer from a pre-trained teacher model to the student model. This knowledge transfer model enables the student model to leverage the knowledge from the teacher model when translating multi-modal sentences. Experiments on four multi-modal data sets show that our proposed method improves the accuracy of five MT systems in comparison to the baseline.
    **A Multi-cascaded Model with Data Augmentation for Enhanced Paraphrase Detection in Short Texts**
247. Paraphrase detection has received significant attention in recent years. The complexity of training for paraphrase detection methods is very large. Therefore, a parallel corpus is a promising way to improve the quality of training. However, capturing contextual information to enhance the performance of paraphrase detectors is an essential but difficult task. In this paper, we propose a novel paraphrase detection approach with multi-cascaded deep features. A multi-layer sequence model with attention mechanism is adopted to capture both global contextual information and local sentences by separate convolution layers. When evaluated on the benchmark dataset WikiParse, our model achieves the state-of-the-art performance with more than 6 percentage points higher accuracy.
    **Job Prediction: From Deep Neural Network Models to Applications**
    This paper presents a deep learning-based method for job prediction in manufacturing settings, and applies it to a manufacturing process data set. To our knowledge, we are the first to apply deep learning to the predictions of jobs in this setting. We do this by applying two standard machine learning methods (Deep Learning and Support Vector Machines) to a production process data set and propose a novel formulation of job prediction using multiple classification models. We propose to use support vector machines to extract features for each possible job label for building up a representation of jobs as classifiers. By taking advantage of multilayer perceptrons, the trained models can predict the class labels and obtain decision points of jobs. We then apply Decision Tree to cluster the results and consider the cluster predictive performance for each class label. We test our models on a dataset from a manufacturing company and show that it is feasible to predict jobs in a manufacturing system in a reasonable time given proper collection of training data.
248. **Encoding word order in complex embeddings**
    In this paper we present an unsupervised framework that is able to combine feature vectors of different orders. In particular, we present a simple method for combining word order vector representations obtained from different learning techniques, such as character and word embeddings. We evaluate our framework on multiple tasks, and the experiments show that our approach is able to obtain higher accuracy than state-of-the-art techniques for clustering and classification tasks.
    **Structural characterization of musical harmonies**
249. In this paper, we present a study of the use of probabilistic model-based statistical methods for clustering chords in polyphonic music. By analyzing a corpus of ten million musical chords, we find a significant amount of overlap in the k-means, t-distributed EM, spectral embedding, and structural similarity measures over different clusters. Our results indicate that two of the most prominent probabilistic models, the Apriori and Expectation Maximization (EM) clustering algorithms, are sufficient for processing data that combines harmonic characteristics with other measures such as genre. In addition, when the ratio of harmonic to polyphonic measures is high, structural clustering methods do not perform as well as when it is low, suggesting the potential for further improvements in such models. Finally, by using a compact KB, an approximate k-means and two spectral embedding algorithms for harmonic clustering, we are able to perform analyses with greater compactness than those that were previously possible using such models.
    **Knowledge-guided Text Structuring in Clinical Trials**
    Clinical trials are highly diagnostic for the nature and potential consequences of new drugs and medical procedures. Commonly, clinical trials are automated for optimisation and validation purposes, while less commonly, significant human annotation of the trial documents is usually provided. This paper discusses several methods for generating and utilising such structured data. More specifically, we focus on expert-generated structured data by using domain knowledge, and describe a way of implementing these methods on top of knowledge graph embeddings, thus facilitating the use of knowledge graphs in clinical trials.
250. **Synthesising Expressiveness in Peking Opera via Duration Informed Attention Network**
    Recent studies have highlighted the importance of dynamic dependencies among objects in visual scenes. One of the main challenges is that typically, visual objects are presented from fixed and fixed spatial positions. However, people and machines interact with objects dynamically and dynamically change the spatial location. This will lead to dynamic dependencies among visual objects and hence impair the user's experience. To address this problem, we propose a dynamic dependencies based information mechanism to generate features from an object position and its variability by modulating temporal dependencies at each stage of visual encoding. Specifically, for the propagation of temporal information in the convolutional encoder-decoder structure, the learnt representations for each local feature will be modulated at each time scale such that the representation duration is affected by the distance between the current frame and previous frames. Accordingly, the responses of global features along with those of local features will become involved with the temporal features and significantly contribute to semantic representations at a level that is broader than a single scale. Extensive experiments conducted on standard benchmarks show that our method achieves the state-of-the-art performances.
    **Complex Cepstrum-based Decomposition of Speech for Glottal Source Estimation**
251. Current glottal source estimation systems are dominated by models trained with large feature-based corpora. This paper presents a novel model using two simple computations. Firstly, glottal sources are represented as binary vectors. Secondly, they are subdivided into multiple Gaussians. Then, the Gaussian cross-correlations and entropy function are updated at each cycle. The advantages of using the proposed model are compared with the current dominant model. Secondly, the results are compared with the well-established variational version.
    **Glottal Source Processing: from Analysis to Applications**
    In this paper, we propose a computer vision pipeline to process the acoustic signal produced by the glottal velocities. We experimentally show that our approach can provide a significant advantage over the standard method of processing source spectrograms from the interval [0, 9]. The frequency information of the glottal velocities are captured by measuring their relative time difference with the same time delay in the time domain. We implement the proposed approach by means of an implementation of hybrid wavelet transform and convolutional neural network with a few trainable parameters. The performance of our approach was demonstrated on a number of new sound clips recorded with phantom phones and a series of real recordings. This paper presents a first empirical study using a time-domain time-frequency representation to explore the use of the glottal velocities as a time-frequency source in acoustic source separation.
252. **\AE THEL: Automatically Extracted Type-Logical Derivations for Dutch**
    Type-logical derivations play a crucial role in knowledge representation and reasoning. From a formal concept, we extract information related to the specific formal concept. In this paper, we present an extension of well-established Kneser-Ney type-logical derivations (KNTDs) that allow for type-logical Derivations with topic/term-labeled formal concepts. More precisely, we analyze KNTDs derived from type-logical derivations of the derivations of the topics of interest to the derivation of type-logical Derivations. In addition, we investigate three different types of topic-labeled formal concepts: explicitly labeled formal concepts, implicitly labeled formal concepts and subconcepts. We show that these kinds of formal concepts can be associated with an inner meaning which allows for the application of KNTDs to automatically extract type-logical Derivations for type-logical Derivations of subconcepts. The experimental results of the automatic derivation of type-logical Derivations of implicit concepts and subconcepts by our extension of KNTDs demonstrate that these kinds of formal concepts can be automatically associated with an inner meaning which can be exploited for automatically extracting type-logical Derivations for type-logical Derivations of implicit concepts.
    **ORB: An Open Reading Benchmark for Comprehensive Evaluation of Machine Reading Comprehension**
253. Recent advances in question-answering (QA) tasks have dramatically improved the state of the art in measuring human intelligence and reading comprehension in machine reading comprehension. However, human performance is only one aspect of machine reading comprehension, and it is often not observed in the recently proposed or benchmarked QA tasks. Therefore, we propose ORB, an open reading comprehension benchmark for more comprehensive evaluation. By aggregating several answer sentences and transcripts from widely-used QA datasets, ORB contains nearly 10,000 questions collected from the web with several varying complexity levels. We show that reading comprehension with weak comprehension is challenging for humans and that different types of candidate answers are revealed even with well-structured question-answer pairs. In this paper, we show the effectiveness of ORB in quantifying the performance of seven representative machine reading comprehension models on different types of knowledge base completion. We also propose four powerful non-ensemble models that capture related information from multiple sources and compare the performance of them on the readability and answer relevance tasks. Based on our evaluation, we observe that the best performing models outperform the state of the art on all the evaluated tasks.
    **Causal-Anticausal Decomposition of Speech using Complex Cepstrum for Glottal Source Estimation**
    Deep learning-based speech source localization (SSLD) has attracted a lot of attention due to its potential ability to successfully suppress side channel artifacts and integrate speech information within a single-channel signal. In many scenarios, however, a DNN-based SSLD system must also deal with voice conversion, speech degradation, and the fact that the context for a reference signal, such as a speech recognizer, is provided by human speech. For this purpose, it is usually necessary to solve multiple tasks simultaneously. In this paper, we propose a novel algorithm for solving the aforementioned tasks by decomposing speech using complex cepstrum, which involves: (1) each source spectrogram as a weighted sum of contexts, (2) each source context as a weighted sum of target spectrograms, and (3) a central source context that acts as a noise filter. To the best of our knowledge, the proposed algorithm is the first time that complex cepstrum is used in SSLD. We evaluate the performance of the proposed algorithm on a simulated domain using a state-of-the-art DNN-based SSLD system, as well as on a benchmark dataset for recording test signals from the Mel-4 challenge. We show that the proposed algorithm outperforms the existing approaches for solving two types of tasks simultaneously.
254. **Using a Pitch-Synchronous Residual Codebook for Hybrid HMM/Frame Selection Speech Synthesis**
    This paper describes the design and implementation of a hybrid HMM/frame selection based speech synthesis system, which can reconstruct the most useful features from sparse speech fragments. This system consists of two modules. The first module is an auto-regressive oscillator, which trains a multi-level high-level speech representation to remove any overlapping/irrelevant speech frames that may appear between a beginning and an end frame. The second module is an auto-regressive Q-function model, which sequentially constructs speech patches according to a reconstructed acoustic embedding. The synthesis results of this approach are shown to improve a conventional auto-regressive HMM/frame selection (ARHS) approach.
    **CASE: Context-Aware Semantic Expansion**
255. Global transformation of an input document by domain experts presents a challenge to the framework of Transformer, a recent popular model for Machine Translation. In order to improve the state-of-the-art quality of translations, in this paper, we investigate the key role of context in translating. We show that context-aware semantics can be leveraged to improve the Transformer learning of a Transformer based on static and dynamic training. We evaluate the proposed methods on a French/English and German/English machine translation task using a large dataset. Empirical results show that context-aware semantics enhances the performance of the Transformer model compared to those based on static or dynamic training.
    **LayoutLM: Pre-training of Text and Layout for Document Image Understanding**
    Recent deep learning based learning models that have outperformed human performance on visual tasks such as image classification have also outperformed human performance on text tasks such as sentiment analysis and question answering. Though text is usually smaller in size, a text document typically contains many more layout elements. In this work, we show that such layout is critical for document image understanding. Specifically, we propose a dual-training framework that consists of a document embedding and a layout embedding network. Specifically, the former generates an accurate model for both the shape and the layout of a given document. The latter, in addition to generating an accurate document representation, provides input features for learning the text-to-image model. Moreover, the dual-training framework allows efficient document labeling and feature extraction. Experimental results show that the proposed method is effective for text document understanding tasks.
256. **OTEANN: Estimating the Transparency of Orthographies with an Artificial Neural Network**
    In natural language processing, expository texts, such as news articles, are the most important way to convey information about real world events. However, due to corpora sparsity and structural imperfections in these texts, traditional statistical methods fail to model their distribution. Inspired by the "posterior collapse" phenomenon in human cognition, we investigate the effect of expository texts on human language, aiming to reconstruct an expository text's distribution in a latent space. Previous studies have shown that, for natural language, the distribution of words and their contrast reflects what would happen if the expository text was generated by another human, and when the word spacing is decreased, the discrepancy between the expository text and the language of the primary writer should decrease. In this paper, we attempt to quantify this discrepancy using a pre-trained artificial neural network. To the best of our knowledge, this is the first attempt at estimating the discrepancy of expository texts. We develop an algorithm for evaluating the transparency of a given text, using convolutional neural networks as the language model. We show that the proposed method can accurately infer this gap, as it successfully predicts the word gap and thus the order of words in a given text.
    **What Does My QA Model Know? Devising Controlled Probes using Expert Knowledge**
257. As large-scale question-answering (QA) datasets continue to grow, multi-modal question answering becomes a reality. Using such datasets, the goal of this paper is to explore the cross-modal nature of the data, and to characterize the expert knowledge that has enabled rapid progress on the task. As a concrete example, we evaluate the performance of existing automatic evaluation methods, showing how effective the expert knowledge they rely on is. Our study reveals that (1) automatic evaluation methods that do not use expert knowledge are often inaccurate, as the model's performance is not sensitive to the quality of the expertise; (2) on the other hand, better but less-discriminative datasets make it difficult to extract answers that require expert knowledge. We also empirically find that in certain settings it is possible to extract answers that require expert knowledge via standard access-style questions. We further identify areas for future research to improve automatic evaluation, including higher fidelity questions and improved feature selection.
    **Essential Sentences for Navigating Stack Overflow Answers**
    Understanding the reasoning structure of a large amount of information has become one of the challenges of Natural Language Processing. Natural language understanding is a crucial problem for large scale information resources to extract useful information. The semantic relations and non-monotonic relations of natural language can capture the common sentence structure of a set of basic sentences. In this paper, we propose essential sentences for parsing, which identify crucial sentences in a large set of documents. They use an attention mechanism to extract the most significant sentences which are not in the original set of sentences. The proposed approach is evaluated on the default configuration of StackExchange where the main input of the parser is the complete set of answers. Experimental results show the effectiveness of the proposed approach compared to a state-of-the-art parsing technique for Google Web Search which uses all the matches of a query in the whole StackExchange.
