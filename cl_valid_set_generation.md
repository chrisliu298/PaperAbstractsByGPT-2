# Computation and Language Paper Abstracts Generated by GPT-2

1. Multi-stage Pretraining for Abstractive Summarization**  
  Automatic extraction of sentences from news reports has been a longstanding problem in NLP. This work shows that transfer learning is a promising approach that improves abstractive summarization. A multi-stage framework is used to train a RNN-based summarization model with unsupervised learning. A pretraining algorithm called hierarchical fuse is used to combine a small set of language attributes into a multi-valued attention score for further learning. Both an ensemble classifier and a bag-of-words model are used in the experiments. It is shown that the additional unsupervised training by multitasking can improve the extractive performance and the final result.
2. **TripleNet: Triple Attention Network for Multi-Turn Response Selection in Retrieval-based Chatbots**  
  We consider the retrieval-based chatbot (Retrieval-bot) scenario and study the multi-turn response selection problem in retrieval-based chatbots. Our method is called TripleNet, and is an extension of the linear attention network (LA). The main idea is to incorporate multiple attention mechanisms into the output features of the neural network. For example, triplet mechanism can refine multi-turn predictions by maximizing feature relevance with respect to a set of triplet candidates, while triple level attention and softmax mechanisms refine the final answers with regard to different-turn candidates. We apply our proposed model to the challenging task of question-answer prediction in the Retrieval-bot. Our experimental results show that our model obtains the state-of-the-art performance on both the open-domain dataset (Showshow) and the offline crowdsourced dataset (STAudo). Further, we evaluate the retrieval-based conversation performance by fine-tuning Retrieval-bot with different amounts of chat history and load data. Experimental results show that the model outperforms the current state-of-the-art methods.
3. **Syntax-Aware Aspect-Level Sentiment Classification with Proximity-Weighted Convolution Network**  
  Aspect-level sentiment analysis is a method to summarize user preferences in a sentence. Given an aspect that expresses a particular preference of a user, it is usually very difficult to deal with rich case statements (e.g. special needs, ethnic origin, etc.), which can be resolved by prior languages (e.g. Chinese) or by domain-specific knowledge (e.g. US politics). This issue can be resolved by pre-processing a large document, but this requires manually labeling the aspect and exploiting the frequent pattern in the text. In this paper, we present a novel method to automatically identify and classify the aspect-level sentiment in long texts. Our method utilizes a domain-specific, context-aware relation dictionary that can be learned from labeled documents and generates a model to predict the aspect-level sentiment. We propose a new attention-based convolutional neural network (CNN) model that takes the relation dictionary as input and works on the word representations extracted from the contexts (i.e. word embeddings) of the words. Experimental results on the SemEval-2010 task demonstrate that our method performs favorably against existing methods, which are capable of either extracting the relation dictionary or mining feature vectors.
4. **Using Priming to Uncover the Organization of Syntactic Representations in Neural Language Models**  
  When we train NLP models using supervised, sentence-aligned corpora, we often observe that the output labels of pre-trained NLP models are often related to the training data. We propose a method to uncover the relation between pre-trained NLP models and the output labels of these models, by showing that some important syntactic representations are shared across languages, and vice versa. Our results are applied to the recently proposed Neural POS Tagger (Neural POS), and we find that it correctly identifies the English subject--object pronouns (e.g., I don't know how to write) and prepositions (e.g., by the way).
5. **Formalism for Supporting the Development of Verifiably Safe Medical Guidelines with Statecharts**  
  This paper presents a novel approach for deriving a list of plausible mechanisms which will satisfy the requirements of verifiably safe medical guidelines, or, in other words, will be safe from potential violations of those guidelines. The work is based on the notion that many factors are closely linked to a guideline's outcome. As such, the health of the guideline's participants can be used to automatically derive a list of scenarios that may constitute violations of the guideline. In addition, using statistical distributions for the inputs allows for the calculation of higher precision than would be possible using hand-crafted features. A data collection tool is also proposed to support the researchers in their work.
6. **Sliding window property testing for regular languages**  
  In this paper, we study the property testing problem for regular languages. In this problem, in each sliding window, the test variables are the filters of a given regular language. The framework we propose provides a natural generalization of variational approaches for this problem. For regular languages defined over higher-order formulas, for which we can prove the probability bound $\propto$ that there exists a filter $\phi_x$ which is given by an assignment $x \in \mathbb{R}^p$ for some function $f_x \in \mathbb{R}^q$ and some type $x_i \in \mathbb{R}^{n \times p}$, we show that the problem of property testing for $x_i$ is equivalent to testing whether $\phi_x \in \mathbb{R}^{n \times p}$ is a filter. For other regular languages with $n=2$ and $q=2$, we prove that the problem of property testing for $x_i$ is equivalent to testing whether there exists a regular filter that maps every variable to a basis basis element. The results presented in this paper are given by a novel property testing method. Under suitable structural properties on the language class $\mathbb{R}^p$, we establish a $\propto$ probability bound on the properties that the problem of property testing for $x_i$ is equivalent to testing whether the filter $\phi_x \in \mathbb{R}^p$ is a strong constant function.
7. **Dependency-Guided LSTM-CRF for Named Entity Recognition**  
  In this paper we propose a novel end-to-end neural network architecture, named as the dependency-guided LSTM-CRF (DGC-CRF), for Named Entity Recognition (NER) task, which consists of multiple LSTM cells, and accordingly exploits long-range dependencies for building a more effective representation of an input sequence. The proposed method aims at extracting a structured, entity-centric, semantic vector for each input character sequence. To exploit the long-range dependencies of different character sequences, each LSTM cell is coupled with a pair of dedicated CRFs to obtain dependency dependency information. Through two-step layer-by-layer optimization, we present a unified end-to-end deep architecture which simultaneously learns the features and attention between characters. Extensive experiments on two benchmark datasets, New York Times (NYT) and Wikipedia data sets, demonstrate that our proposed method outperforms the previous state-of-the-art NER systems.
8. **Speech Replay Detection with x-Vector Attack Embeddings and Spectral Features**  
  Speech replay detection has been shown to be very important in many voice-controlled applications, such as online user interfaces. However, the large variations in speech characteristics and complex noise in real-world speech make replay detection challenging. This paper presents a novel approach to address the challenges of speech replay detection in real-world speech. First, a new replay detection method with x-vector embeddings and spectral features is proposed, which uses "concatenated" features and the x-vector clustering algorithm to automatically identify speech frames from noisy speech. Second, we develop the algorithms for the training of the x-vector clustering algorithm, and the adversarial robustness training framework with cross-entropy loss functions. The experimental results show that the proposed method achieves the state-of-the-art performance with x-vector features, and its performance is also comparable to the replay-free state-of-the-art methods.
9. **GNTeam at 2018 n2c2: Feature-augmented BiLSTM-CRF for drug-related entity recognition in hospital discharge summaries**  
  Annotating hospital discharge summaries to carry drug-related information is a key element in many analytics pipelines. Unlike other scientific and medical data repositories where data are described in terms of a single fact or concept, hospital discharge summaries present a rich multidimensional, heterogeneous and noisy information. Features are the most fundamental means to accomplish this task. To address the challenges of this highly heterogeneous data, we present an Ensemble of Gated Convolutional Spatial Transformer Networks (gnteam) to learn features and incorporate them to a BiLSTM-CRF model. This model is trained in an end-to-end fashion on an empirical gradient descent mechanism and performs better than any non-neural network based approaches that have been proposed so far. In addition, we investigate the effects of different heuristics and pooling mechanisms to improve the model performance. Overall, our model obtains an increase of +0.47 in F1-Score and +1.45 in Average Precision when compared to a simple Gaussian mixture model.
10. **Specificity-Based Sentence Ordering for Multi-Document Extractive Risk Summarization**  
  Multi-document extractive summarization has gained considerable attention from the research community in the last two decades. The key idea in extractive summarization is to construct a summary by merging sentences from multiple documents, which are usually selected according to their relevance. However, in order to be valid to be used in large-scale applications, there is a need for more fine-grained control over the merging order of sentences. To this end, a novel model, called specificity-based sentence ordering (SMA), is proposed. The proposed model incorporates aspects of similarity, novelty and diversity to assign the sentence ordering to multiple documents. The proposed model is able to perform paragraph composition automatically by incorporating textual, spatial and structural information. Experimental results on a novel multi-document dataset from six diverse genres of scientific papers show that our proposed model consistently outperforms the state-of-the-art methods, and moreover, it obtains better overall sentence quality by employing appropriate semantics of the sentence ordering, as compared to existing methods.
11. **A Consolidated System for Robust Multi-Document Entity Risk Extraction and Taxonomy Augmentation**  
   Extraction of entities and taxonomy in multi-document corpora is important for large-scale knowledge base construction, information retrieval and entity linking. However, existing methods focus on 1) extraction of entity (or taxonomy) names in a single document, and 2) merging entity and taxonomy names in a sentence to form a multi-document entity triples. To overcome these limitations, we present a unified and comprehensive system for extracting entities, taxonomies and corresponding triples from multiple documents. The system is built on top of a specialized neural machine translation (NMT) engine for text to sequence (TTS) model and a novel method for effective annotation of entity and taxonomy names. Moreover, we incorporate multi-document entity risk scores in our triples to enhance the accuracy. The experiment on four standard datasets shows that the combined system can achieve better performance than state-of-the-art systems. Moreover, for each test document, a set of risk scores can be selected based on confidence interval of each entity and taxonomy. We have used three publicly available datasets and a dataset from the third Micro-HARL workshop (acronym for Minimal Observational Risk), to evaluate the effectiveness of our system and study different configurations. Experiments show that: (1) maximum reduction in risk scores (1.2\%) is achieved by using only the first sentence of the document and that (2) entity and taxonomy names extracted from second and third sentences are good candidates for multi-document entity triples to increase precision. Our results suggest that our system is a viable alternative for multi-document entity and taxonomy extraction and can be easily adapted to other kinds of text.
12. **Automated Chess Commentator Powered by Neural Chess Engine**  
   Automated chess commentator is one of the most popular and challenging fields in artificial intelligence, because a good commentator should be a smart player, maintain long-term memory and also be able to analyze moves in chess games. In this paper, we propose the automatic chess commentator using Deep Q-Network (DQN) model. Our model can evaluate the strength of a chess move, search for the corresponding comment, filter the comments, and compare them with one another and the comments of other chess players. Our experimental results show that our model can achieve the best performance compared with the baselines for evaluating the strength of chess moves and the proposed chess commentator.
13. **Biomedical Mention Disambiguation using a Deep Learning Approach**  
   In this paper, we address the problem of biomedical mention disambiguation. This problem is crucial to the growth of information technology in biomedical domain, since it enables scientific papers and patient narratives to be accessed and referenced from multiple sources, thus facilitating the identification and understanding of important biomedical concepts. Recent advances in deep learning have been leveraged to solve the mention disambiguation task in supervised and unsupervised approaches. To establish a ground truth label for the biomedical entities, we apply a sequence labelling approach to generate human-annotated seed embeddings for clinical keyphrases. These embeddings are used as ground truth to train a deep learning model. We also propose a novel learning objective to train the model by minimizing a non-linear function, which makes use of contextual features to encode biomedical context. Experiments on three biomedical corpora, namely ILIST, ORL and MIMIC-III, have demonstrated the effectiveness of our proposed approach.
14. **Does BERT Make Any Sense? Interpretable Word Sense Disambiguation with Contextualized Embeddings**  
   We show that the disambiguation accuracy of BERT on five English disambiguated WordNet concepts is comparable to that of a fine-tuned disambiguated BERT model on non-disambiguated WordNet concepts. To explain this gap, we consider two explanations. One is that contextualized embeddings are sufficient for the automatic disambiguation of WordNet concepts, although this is not supported by our empirical findings. The other is that BERT makes some sense. To support the claim that BERT does make sense, we evaluate contextualized word embeddings on disambiguated WordNet concept word co-occurrence. This evaluation shows that the syntactic, semantic and contextual word similarities are stable across word embedding models, suggesting that contextualized word embeddings may be a more reliable alternative to standard word embeddings.
15. **Cross-Lingual Natural Language Generation via Pre-Training**  
   The success of Machine Translation (MT) is currently driven by a series of approaches, including Transfer Learning from one language to another. In this paper, we focus on applying transfer learning to generate natural language in different languages, via a pre-trained language model. The pre-trained language model has seen extensive use in the image domain, and thus can be successfully applied to the natural language domain. First, we introduce the language graph, that directly models source and target sentence distributions in different languages, and the vocabulary level of source and target sentences in each language. Second, we show how to leverage this language graph to estimate cross-lingual entailment. We develop a simple language model that is learned by sampling a weighted distribution of words from this language graph. These pre-trained language models are used to generate sentences in different languages, where the learned language model can use cross-lingual entailment between sentences to predict in-domain translation. We extensively evaluate our approach, using two language pairs in two different domains: the image domain of `Pascal Text Pascal' (PTP) and the natural language domain of `Tech Talk' (GT). In the image domain, we demonstrate that using pre-trained language models can substantially improve both MT quality and in-domain translation performance. For the natural language domain, our model can achieve state-of-the-art in-domain performance.
16. **NLVR2 Visual Bias Analysis**  
   User-generated content provides a rich source of data for model training, with information on people's inner states and behaviors stored in implicit simulators. Researchers have made advances in understanding such content by analyzing how different types of representations influence people's perception of images. However, it is unclear how representations affect people's behavior. To fill this gap, we collect a new dataset, called NLVR2, that contains realistic-looking, i.e., not manipulated, videos of people interacting in two environments with both labeled and unlabeled video examples. Our dataset has rich content and novel aspects of interest. In the first environment, participants face a real-world object while navigating an unseen 3D environment. In the second one, they pose and move objects in the unseen environment while perceiving the observed scenes. Using standard dimensionality reduction methods, we find that representations of people's behaviors are predictable and transferable across environments, while representations of visual aspects of scenes are inconsistent, even across images captured by different cameras. Our results suggest that representation learning is not only limited to representations in images and videos, but more broadly might reflect people's intuitive, implicit behavior.
17. **TalkDown: A Corpus for Condescension Detection in Context**  
   We describe a new corpus for solving the problem of computing the severity of discourse relations, named TalkDown. It is publicly available at https://github.com/XinshuiZhang/TalkDown. The corpus comprises five radio interviews, including one where each speaker talks about a true or false claim and is given four real-valued scores. It represents a natural model of discourse relations and an open-domain testbed for research in computational subtleties of language.
18. **Diachronic Topics in New High German Poetry**  
   There is a notable absence in existing research in two important fields: poetry and the study of the progression of texts in a given language. These two fields have the same genre, which requires experts in either of the two fields. Studies of poems are dominated by linguistics, where very good results have been reported. Studies of texts in a given language however are very much dominated by lexical analysis, which leads to much less research in both fields. The introduction of the NMT model, a sequence to sequence model of languages, has significantly changed the landscape of research in both fields. In this paper we discuss our participation in the development of MT in the domains of high German and Occitan. We will discuss the role that we have had in the development of NMT in the last three years and give some insight into the way that our language analysis models have developed and how our knowledge of a language has evolved over time.
19. **In Conclusion Not Repetition: Comprehensive Abstractive Summarization With Diversified Attention Based On Determinantal Point Processes**  
   With the remarkable performance of DNNs, how to train and use DNN models in practical cases has attracted much attention. Because of the powerful and efficient representation ability of DNNs, attention models have been used to model short-term, multi-sentence attention. Though both traditional attention models and DNN models have great capacity, there are some deep learning models in which DNN models have a superior capability. In this study, we propose an improved attention model to enhance DNN models. It is not conjecture that the methods proposed have better capability than traditional attention models. We have applied our attention model to extract a comprehensive abstractive summary by utilizing the knowledge in natural language processing literature. Our experimental results show that our proposed model extracts informative concepts to provide insight of data.
20. **Neural Generative Rhetorical Structure Parsing**  
   Motivated by the exponential growth of information in social media and in the digital literature, the multi-modal nature of social texts, as well as the myriad of complex and diverse languages and genres such texts take, there has been recent interest in parsing the discourse structures of social text to discover more intelligent social narrative, whether in digital or printed forms. In this work, we develop a neural architecture to build such discourse parsing systems, by leveraging the rich linguistic knowledge encoded in discourse structure. The proposed architecture is based on a multi-task neural encoder-decoder, and an attention-based recurrent neural network. We demonstrate how to extend this architecture with external linguistic knowledge learned from existing discourse analysis corpora. Our neural encoder shows significant performance improvements when compared to a strong baseline model for semantic role labeling on a variety of datasets.
21. **An Empirical Study of Content Understanding in Conversational Question Answering**  
   Recent deep learning models for question answering (QA) have achieved significant successes in achieving state-of-the-art performance on many QA benchmark datasets. One key factor behind these recent advances has been the availability of large-scale textual corpora for training. Despite their large scale, these corpora contain very little text that is easy to understand and recall by humans. Recently, several methods have been proposed to improve question understanding by augmenting the text in the source QA corpora. These methods utilize the notion of topic semantics and propose effective text mining and learning methods for question understanding in QA. To measure the effectiveness of these methods, we have compared two model sets of various models in a large-scale human evaluation task on a recently proposed QA benchmark dataset, suggesting that significant improvements are possible with more structured text. In addition to this quantitative evaluation, we have also conducted an in-depth qualitative analysis of the human evaluation results, revealing key elements that affect the human's performance in a QA task. This analysis is based on an extensive analysis of user comments from the recently introduced QA dataset. In order to obtain an understanding of which components can improve QA performance, we have defined a novel metric for human evaluation on QA tasks that evaluates several textual factors that affect the QA systems. As a result, we have obtained state-of-the-art performance on this new benchmark dataset.
22. **Situating Sentence Embedders with Nearest Neighbor Overlap**  
   Though it may not sound impressive, neural sentence embeddings with more neurons in more layers generally perform better than sentence embeddings with fewer neurons in more layers. In this paper, we propose a simple and effective method to more accurately align sentence embeddings from multiple sources by establishing a cross-source proximity. We demonstrate that our approach allows us to learn sentence embeddings for a new task with less human-error than previous methods, with significantly lower computational cost than previous state-of-the-art, and without the need for manually pre-trained word embeddings.
23. **Technical report on Conversational Question Answering**  
   We present an approach to build conversational question answering systems from scratch. Our approach generalizes recent work in natural language understanding (NLU) and uses neural networks to handle paraphrases of questions. The neural networks are trained to retrieve relevant answers given the context of a question and a historical passage in the document. Our approach is evaluated on the crowd-sourced QA dataset we have created. The evaluation shows that our approach significantly outperforms existing state-of-the-art systems.
24. **Code-switching Language Modeling With Bilingual Word Embeddings: A Case Study for Egyptian Arabic-English**  
   In this work we present a multimodal method that performs code-switching language modeling using bilingual word embeddings. As a concrete case study we apply this method to Egyptian Arabic-English (EAE) data, and demonstrate its effectiveness. In addition to the negative effect of using random weights as regularization for a separate bag-of-words classification model, our findings suggest that using a shared representation across language components can help to mitigate this problem. Our experiments, which include a comprehensive set of feature extraction tasks and testbed settings, show that the proposed method outperforms a number of state-of-the-art methods on standard Arabic-English and EAE word similarity benchmark tasks. Furthermore, we show that using monolingual language features based on BERT yields a 10.5% relative improvement in English to Egyptian Arabic translation task.
25. **Assessing the Lexico-Semantic Relational Knowledge Captured by Word and Concept Embeddings**  
   In this paper we examine the incorporation of word and concept embeddings as well as other sources of relational knowledge into an Information Retrieval (IR) system. We also investigate which of the two types of knowledge is more informative for improving the retrieval accuracy and evaluate the predictive performance of these classifiers on a state-of-the-art IR system. We also compare different baseline models in terms of their computational properties and provide a detailed analysis of the relation and extent of knowledge captured by word and concept embeddings. Our results indicate that the incorporation of word and concept embeddings does result in higher accuracy on the CRITS task and their predictive performance is comparable to previous work.
26. **Transfer Learning across Languages from Someone Else's NMT Model**  
   Distributed NMT models have shown the effectiveness on many machine translation tasks. However, current distributed NMT systems cannot be easily transferred to new languages. The number of transferred NMT systems is limited to how much training data and knowledge is available from a single source language. We apply transfer learning technique from a machine translation system from another language to transfer a large-scale distributed NMT system with similar model. Transfer learning improves the transfer performance by 1.6% on WMT 2014 English-German, and 1.6% on WMT 2015 English-Japanese, without any language-dependent adaptation.
27. **Question Answering is a Format; When is it Useful?**  
   This paper considers the problem of automatically selecting the most suitable question answering format for a given document. Most answer set programming models describe an answer set as a sequence of partial facts, and the users are typically expected to answer within the minimum feasible time limit. In the light of this observation, this paper proposes a preliminary, subjective evaluation of automatic answer set format selection. A set of possible answer set formats, covering two well-known families of answer set formats, is generated by decomposing an answer set into a series of topic-dependent key words. A global-only criterion, referred to as the compositional weighted average of unification and KLM resolution problems, is used to rank the most informative candidates. The approach was used to select the most suitable answer set formats for question answering at the Automated Biology corpus, obtaining positive results. The approach is simple, scalable and does not require any knowledge of the computational structures of answer set formats, and thus can be easily applied to other corpora and languages.
28. **Large-scale Pretraining for Neural Machine Translation with Tens of Billions of Sentence Pairs**  
   Large-scale pretraining is one of the most expensive tasks in machine translation research. Training large neural machine translation (NMT) models requires months of manually annotated data for each language pair. However, manual annotation is expensive and does not scale with high-dimensional target languages and corpora. To address this challenge, we propose a new translation task that, unlike existing large-scale parallel data tasks, is extremely correlated with existing large-scale parallel corpora. Given the large amount of parallel data that exist on a per-language basis, we create large-scale parallel corpora by creating tens of millions of parallel sentence pairs from open-source machine translation (MT) resources and also distilling or leveraging a comparable amount of parallel sentence pairs from a large parallel corpora with highly correlated corpora. To the best of our knowledge, this is the largest manually annotated sentence pair dataset for NMT. To train a large-scale NMT model, we first generate an initial language model by introducing a machine translation system that is trained on target-side language pairs from the large-scale parallel corpora and then fine-tune the model by providing gold content from source-side parallel corpora. Experimental results on the benchmark German-English (ME) language pair show that large-scale pretraining using our large-scale parallel corpora can not only improve translation quality significantly but can also lead to improved translation quality on word level.
29. **SIM: A Slot-Independent Neural Model for Dialogue State Tracking**  
   State tracking is a vital part of automatic speech recognition (ASR). Current state-of-the-art methods, which are based on deep neural networks, require a large amount of training data to model human speech, and therefore, are limited to the classification task. This restricts their deployment on speech enhancement systems for which test data is limited. In this paper, we present a simple and fast method, called SIM, for tracking state. SIM uses an encoder-decoder recurrent neural network to build a mapping from the action-state embedding space to a slot-dependent language model embedding space. The latter is then used as the representation of the action-state embedding. We compare our method to several state-of-the-art method and show that SIM achieves the best or competitive results, without using any training data or speech enhancement samples.
30. **Learning to Detect Opinion Snippet for Aspect-Based Sentiment Analysis**  
   Aspect-based sentiment analysis (ABSA) aims to capture the sentiment-specific aspect-centric information of text documents. However, although several methods have been developed to extract opinion summary from paragraph-based extracts, very little attention has been paid to opinion sentence, which has an even higher level of sentiment-specificness. In this paper, we propose a new method to extract opinion sentence of a paragraph for aspect-based sentiment analysis. Our proposed method applies an adversarial learning process to extract opinion sentence from paragraph by exploring the dependency between two consecutive sentence-level features of sentiment and aspect. In the context of ABSA, a paragraph contains opinion sentence which refers to each aspect. In the process of the adversarial learning, we make use of the dependency between two consecutive sentence-level features of sentiment and aspect as a constraint. Experimental results demonstrate that the proposed method outperforms state-of-the-art extractors and significantly surpasses the performance of existing paragraph-based extractors.
31. **The Power of Communities: A Text Classification Model with Automated Labeling Process Using Network Community Detection**  
   Automated labeling is used to reduce the amount of labor required to run a text classification model, while also increasing the accuracy of the final results. But in terms of industry scale, no automation method exists in combination with the manual labor. In this paper, we propose a novel approach to reduce manual labeling efforts by providing a mechanism to automatically and accurately label texts. We use Community Detection technique for this purpose. The objective of Community Detection is to detect communities from a collection of texts, where an input text is represented as a set of words and the goal is to find community boundaries and labels as quickly as possible. The contribution of this study is threefold. First, we describe how to use Community Detection technique to automatically label texts, and secondly, we propose a new approach to reduce the amount of manual labeling effort by providing a mechanism to automatically label texts. Thirdly, we provide an example of how to perform a comparative study between various text classification models in the context of manual labeling effort reduction.
32. **Tackling Long-Tailed Relations and Uncommon Entities in Knowledge Graph Completion**  
   Knowledge graphs are rich vector representations of the world that can be leveraged for a wide range of applications. Given a knowledge graph, the goal is to predict new facts about unseen entities in it, either by interacting with them in some manner (say, by walking over their links) or by combining information from multiple knowledge graphs to form a composite model that is able to complete a given knowledge graph. To tackle this problem, existing works either treat the composition process and entity linking equally or rely on elaborate embeddings to capture the diversity of link structure. In this work, we argue that knowledge graphs should be formulated to implicitly encode the long-tailed relationships between entities. In particular, we propose a new distributed knowledge graph completion model that simultaneously performs entity linking and entity composition to tackle this problem, while accounting for both long-tailed relationships and rare entities. Our model is designed to scale to massive knowledge graphs that contain tens of millions of entities and hundreds of millions of relationships, with inference performance matching the state of the art while being substantially simpler and more efficient. Our model is based on a novel cascade of graph embeddings, each of which is designed to capture a particular type of long-tailed relationship between entities. Extensive evaluations on two benchmark knowledge graph completion datasets show that the proposed model achieves significant performance gains over the state of the art in a variety of settings.
33. **Breaking the Data Barrier: Towards Robust Speech Translation via Adversarial Stability Training**  
   In this paper, we propose a novel conditional-domain adversarial framework that exploits the hierarchical structure of natural language, which is crucial to ensure robustness. As demonstrated on Chinese-English and English-French language translation, our model is able to learn domain-invariant representations, i.e., the latent representations from both source and target domains are not conditioned on each other, in contrast to current methods that do not explicitly model this property. Experimental results show that our proposed model outperforms current state-of-the-art unsupervised and supervised approaches.
34. **Developing a Fine-Grained Corpus for a Less-resourced Language: the case of Kurdish**  
   This paper describes our contributions to the 2018 shared task on Automatic Generation of Information Extraction from Neural Machine Translation Systems. We integrated several approaches to improve language models and we collected three new low-resource Kurdish corpora. Our systems achieved the second best performance on the shared task of all languages.
35. **Pre-train, Interact, Fine-tune: A Novel Interaction Representation for Text Classification**  
   Many recent works have demonstrated the success of embedding-based models for text classification and, more importantly, embeddings learned from diverse types of interactions (e.g., traditional word-to-class relations, deep supervision, and high-level representation) have outperformed their single-type counterparts. The recently released Labeled-Aggregated-Transcript (LAT) dataset allows a large-scale in-domain interactive text classification experiment. While embedding models have been shown to outperform word-to-class embeddings in flat terms, we demonstrate that they are less accurate in a model-based fine-tuning context. While analyzing the extent to which models are capturing different types of interactions, we find that they are not focusing on the correlation between the two types of input (i.e., labeled and unlabeled) when aligning training and inference spaces. We propose a unified inter-embedding model that efficiently captures the interactions between two input representations, and build an interactive dialog model using it. Through extensive experiments on 10 different text classification tasks from 11 different domains, we demonstrate that our proposed model consistently improves upon alternative models. In addition to our empirical results, we empirically show that incorporating unlabeled text significantly improves word-to-class embeddings.
36. **Speech Recognition with Augmented Synthesized Speech**  
   This paper presents a new method for speech recognition based on acoustic model synthesis and data augmentation, and a semi-supervised technique to improve recognition performance. This method uses a Deep Neural Network (DNN) as the acoustic model. The synthesis method relies on a GAN (Generative Adversarial Network) trained by data augmentation. We augment the decoded speech signal with a sequence of synthesized speech frames at a rate of 2, allowing the DNN to generalize better to a given training set of real speech. Finally, we perform an unsupervised, empirical search that allows the DNN to be trained and experimented with datasets that are not available during the training process.
37. **Extreme Language Model Compression with Optimal Subwords and Shared Projections**  
   This paper proposes a text style transfer method to compress (in both content and structure) extreme language models. In the proposed method, we find effective ways to reduce the structural information in the LSTM layers and extract subword level word representations from the transformed input text. Then, we generate "distance features" of the subword representations to reveal the semantic relationship between the different words. Finally, in order to retrieve the original text from the subword level representations, we propose a mechanism of optimal projections to reconstruct the original text. By combining these features, we compress LSTM models by 1.67\% and achieve the state-of-the-art (SOTA) compression rate of 2.67\% on the public IMDB corpus. Our method is also applied to a set of large scale language modeling tasks.
38. **Learning A Unified Named Entity Tagger From Multiple Partially Annotated Corpora For Efficient Adaptation**  
   In this paper, we address the challenge of characterizing entity mentions in heterogeneous user generated content, which is more common in user generated social media streams. There has been a surge in tagging entity mentions as various data sources, such as social media content, blog posts, and news articles, have been used to identify mentions of entities in these sources. Current techniques either learn individual model specific models to capture differences in both source and target data, or rely on unlabeled target data and seek to optimize model models using document or domain labels as supervision. In this work, we tackle the first challenge and propose an unsupervised method to jointly learn a unified model across multiple source domains, which are effectively annotated and share the same level of accuracy. We show that by maintaining a shared entity mention representation, each model is equipped with a task specific entity tagging component. During the training process, the shared model learns to predict all possible entity mentions from the sources in an unsupervised fashion. The supervised model employs the fine-tuned model and attempts to predict all possible entity mentions from the domains with the same level of accuracy. We demonstrate that by leveraging domain level labels from multiple sources, we can achieve impressive results for both supervised and unsupervised tasks.
39. **Semi-supervised Text Style Transfer: Cross Projection in Latent Space**  
   Neural networks in semi-supervised learning have achieved good performance for image transfer. This is mainly due to the use of Generative Adversarial Networks (GANs), which are a form of image synthesis technique. There have been numerous attempts for a GAN-based style transfer approach. In this paper, we present a method to transfer the style of a given document into the image data. Our model is an adversarial network that takes a fixed-size image as input and is trained in the latent space of the document. Our training is very efficient, which means that large sizes can be fed into our network, and it also allows us to use a fast projection step to avoid overfitting the learned target function. We show the effectiveness of our method by showing that the document style can be transferred to a few images with good performance, even when only a small proportion of the images have some kind of labeled data.
40. **Improving Pre-Trained Multilingual Models with Vocabulary Expansion**  
   Many supervised machine learning (ML) methods require multilingual training data which is expensive to produce. We propose to use pre-trained multilingual network models (e.g. BERT) to improve multilingual translation performance. Our work follows an information-theoretic approach and explores some common characteristics of multilingual dictionary models. A naive expansion strategy of a bilingual dictionary is not effective as it heavily increases the bias of the dictionary, and fails to fully utilize the bilingual dictionary. We present a better way to expand the vocabulary of bilingual dictionary models by exploiting the fact that words appearing frequently in common dictionaries are also frequently appearing in different translations. We experimentally demonstrate the efficacy of our approach on English-French and English-German parallel data and show a considerable translation gain of 1.19 - 3.94 BLEU for English-French and 3.21 - 10.95 BLEU for English-German, respectively. Our source code is available at https://github.com/Parallel-Data-Lab/EnglishMT
41. **Monotonic Multihead Attention**  
   Attention-based sequence models have achieved high accuracy in many sequence modeling tasks. However, they typically introduce a large number of parameters, and their training becomes harder with multiple examples. In this paper, we propose a simple yet effective attention-based model that can learn better structure for the output by solving the conventional logistic regression problem. We call this model the Multihead Attention Network (MANet). We train a MANet using the maximum likelihood estimation algorithm, with the label of a subnetwork as the dependent variable. By jointly training two attention-based sequence models with different parameters, the estimated subnetwork's labels can be used to calculate the attention weights. We demonstrate the effectiveness of the MANet in three tasks: sentiment analysis, machine translation, and image caption generation. In all these tasks, the MANet can achieve state-of-the-art accuracy and a large number of parameters are not needed.
42. **Biomedical relation extraction with pre-trained language representations and minimal task-specific architecture**  
   Annotation tasks and the associated tasks for machine learning techniques have led to a renewed interest in automatically extracting statistical relations between biological entities from biomedical text. State-of-the-art methods are trained on a large and diverse corpus and require numerous annotated training examples. To overcome these limitations, we introduce a hybrid model, Biomedical relation model (BRM), that leverages sentence representations and minimal task-specific architecture. We compare our model with a standard Biomedical relation extractor, and experimental results demonstrate the effectiveness of our model.
43. **Improving RNN Transducer Modeling for End-to-End Speech Recognition**  
   In this paper, we focus on the analysis of RNN models for end-to-end automatic speech recognition (ASR). Different from the standard loss functions, we introduce a novel loss function called RNN Transducer Error Rate (RNER) loss. The RNER loss is designed to jointly optimize the speech recognition output and the Transformer Language Model (TLM) encoder outputs. Our empirical analysis shows that this new loss function leads to a better model representation, which improves the robustness of both the Transformer encoder and the speech recognition network. On the Wall Street Journal (WSJ) conversational speech recognition dataset, our new RNER loss leads to significant improvements (5.1%, 12.2%, 4.2%, 5.8%, 5.3%) over the original RNN based standard cross entropy loss. Our results show that RNER loss can reduce the error rate of the language model by 4% in the end-to-end ASR task without any performance degradation (generalization). The code is available at https://github.com/csenrl/RPIR.
44. **On the Importance of Subword Information for Morphological Tasks in Truly Low-Resource Languages**  
   This paper presents a data-driven analysis of how subword models are used to improve the results of morphological analysis tasks. We apply a number of subword models to several different morphological tasks in two truly low-resource languages, a Thai and a Polish language pair. This analysis provides a deeper insight into how subword information can improve morphological tasks. We find that subword features contribute more to neural models when they are learned together with the underlying morphology representations. We find that in a range of morphological tasks the best performance was achieved when only one hidden layer was used and in some cases more than two. In a classifier task we observed a drop in performance of the system as the amount of subword information in the training set increased. Finally, we demonstrate the benefits of subword information for pre-trained morphological models and compare it to model-agnostic features for a downstream task.
45. **Selecting Artificially-Generated Sentences for Fine-Tuning Neural Machine Translation**  
   Neural machine translation has shown remarkable progress recently. However, its performance suffers from the generalization gap between natural language text and generated text. The largest source of this gap is the use of various methods, including adversarial methods, to expand the vocabulary of translation systems. We propose the Naturalization from Sentence Expansion (NSE) framework, which takes the created synthetic text into account during training. We evaluate the efficacy of the NSE framework in a professional MT setup, consisting of professional and non-professional speakers, by evaluating the effect of different information sources on the translation quality. Experiments on the WMT'14 English-German and WMT'15 English-Russian translation tasks show that NSE provides improvements in the state-of-the-art model BLEU scores (4.9 and 4.1 in the best model, respectively). Moreover, we show that adding additional training data from the synthetic text improves the performance even further.
46. **DARTS: Dialectal Arabic Transcription System**  
   Dialectal Arabic, which is the mother language of the Arabs, is widely used in several Arabic-language social media networks. The dialectic based Arabic transcription system recorded at the FADEC workshop, which has been participated in by many researchers, including the development and usage of some end-to-end models. In this article we describe our work in creating a new resource for Arabic Arabic end-to-end dialectical transcription which is called DARTS. In this article, the end-to-end system that we built and used for Arabic Arabic end-to-end dialectical transcription is introduced.
47. **Aspect and Opinion Term Extraction for Aspect Based Sentiment Analysis of Hotel Reviews Using Transfer Learning**  
   Aspect Based Sentiment Analysis (ABSA) has been popular in business world for extracting sentiment and opinion of reviews which contain valuable information for business purpose and customer service. In this paper, we focus on aspect based sentiment analysis which requires extracting sentiment and opinion of reviews based on aspect. Although data for aspect extraction is available in large quantity and is of high quality, the problem of aspect and opinion term extraction is still not solved due to the high cost of annotating. This paper aims to propose a two-step approach for aspect and opinion term extraction, and model for aspect term extraction. First, we propose a new model for aspect term extraction, named TransSent, for extracting sentiment and opinion of reviews. The existing models use either hand-crafted features or additional knowledge of aspects. However, due to the high cost of manual feature engineering, these methods may suffer from high feature redundancy and provide poor performance. Second, we transfer existing existing state-of-the-art model TransSent from the domain of opinion classification and propose a novel domain-specific model called DeepASP for aspect term extraction. The proposed model is designed to have rich discriminative representations for aspect and opinion, thus achieving superior performance. Experimental results on several public datasets demonstrate the effectiveness of the proposed method.
48. **Fine-tune Bert for DocRED with Two-step Process**  
   In DocRED, the existing methods that use BERT, such as bertcenter, can reduce accuracy of DocRED. Nevertheless, these methods were derived by adding fine-tuning, and the resulting models perform poorly when they are deployed in production. To help accelerate the deployment process and improve the performance of DocRED, we propose to use an additional pre-training step to fine-tune bertcenter. The pre-trained models, namely fragment fragment-based approximations of the pre-trained BERT models, are easily deployed, and not require fine-tuning. For training the proposed models, we adopt a method, the Scaling-up-Matching-up approach, to find a small and self-adaptive subset of the training dataset for reuse. By this approach, we can (1) speed up training with the small training set and (2) effectively learn a better BERT model that is able to generalize better to novel documents. Our experiments show that the models trained with the proposed pre-training step in DocRED have better generalization and accuracy compared to the models trained with the existing pre-training steps. In addition, we show that it is easy to transfer the pre-trained model from one language to another using Transformer model pre-trained on a particular language.
49. **An Investigation into the Effectiveness of Enhancement in ASR Training and Test for CHiME-5 Dinner Party Transcription**  
   End-to-end automatic speech recognition systems, especially the end-to-end hybrid systems, are a promising approach to speech recognition. The goal of this work is to investigate the effectiveness of the recent encoder-decoder speech recognition system trained with the i-vector acoustic model for restaurant domain, which is a variant of CHiME-5 dataset. The model's evaluation is conducted on the developed character segmentation dictionary trained on i-vector. Experimental results demonstrate that the proposed system is successful in training the decoder with i-vector, and can also be applied to testing if the speech was prepared by a human or not.
50. **Improving Fine-grained Entity Typing with Entity Linking**  
   Entity typing is an essential task in knowledge base completion. While the existing works focus on entity linking and entity linking tasks separately, in this paper we propose a unified model for all of them. We fuse existing entity linking and entity typing methods to improve fine-grained entity typing by exploring their joint role in entity typing. Moreover, we present a unified entity linker which enforces the pairwise consistency between entities in multiple stages. Experiments on several data sets demonstrate the effectiveness of our unified entity linking and entity typing models, especially on entity linking.
51. **MinWikiSplit: A Sentence Splitting Corpus with Minimal Propositions**  
   We introduce MinWikiSplit, a corpus of 70,000 sentiment-oriented texts (1.4M sentences) generated from a collection of movie reviews on Amazon (Appear Now Now). Each sentence is split into three identical text chunks, each chunk being treated as its own sentence. The first chunk is split into three shorter chunks of individual words. The second chunk is split into two shorter chunks of independent words. The third chunk is split into two longer chunks of isolated phrases or concepts. Sentiment analysis is carried out on these chunks. The 3D Convolutional Neural Network (CNN) based classifier is trained on these three chunks of text to measure the sentiment of the extracted sentences. We also train a combination classifier based on sentence encoding techniques to measure the semantic similarity of each sentence. Results show that the three sentence splitting techniques result in comparable classification accuracy to human evaluators. With the training set of 71M sentences, we can efficiently mine 3D CNN representations of sentences for sentiment analysis. MinWikiSplit will facilitate research into sentiment analysis in general and other tasks such as sentiment classification, categorization and discourse analysis.
52. **Semantic Change and Emerging Tropes In a Large Corpus of New High German Poetry**  
   We present a large annotated corpus of English-German poetry spanning from the late sixteenth to the beginning of the eighteenth centuries. Our corpus contains over one hundred thousand poems written in New High German between 1598 and 1789 and which are currently categorized into three main semantic categories, borrowed, enriched, and private. The analysis of the corpus enables the study of the evolution of conceptual elements and they become available for domain-specific analyses when examined individually and in combination.
53. **DisSim: A Discourse-Aware Syntactic Text Simplification Frameworkfor English and German**  
   In this paper, we propose a contextualized approach for disfluency detection in German and English. The model consists of a tree-based decoder that performs selectional inference and an attention-based tokenizer. The disfluency model is trained by using English ASR transcriptions and disfluency lexicons for parallel corpora. We perform our experiments on a different disfluency dataset from the one used in the disfluency detection task of KDD Cup 2015 and compare the model to an state-of-the-art baseline model for German. Our experimental results show that the proposed model significantly outperforms the state-of-the-art baseline, and we achieve the new state-of-the-art in English.
54. **Decomposing Textual Information For Style Transfer**  
   In recent years, incorporating textual information into neural networks has achieved great success. Inspired by the success of style transfer, in this paper, we propose a novel unified approach that can synthesize text with given style. In particular, we treat each paragraph as a handwritten text, and match the style of the paragraph with the style of a given written text using a syntactic encoder. The synthesized text retains the semantic structure of the handwritten text, while the style of the synthesized text is smoothly adjusted with the given written text. Experimental results show that the proposed method is effective for document style transfer, outperforming existing methods by more than 5\% in terms of standard evaluation metrics.
55. **Overview for the Second Shared Task on Language Identification in Code-Switched Data**  
   In this document, we briefly introduce the three shared tasks on automatic natural language identification (NLI), and provide background and motivation. The first task is on short-text, the second one is on code-switching data, and the third one is on long-text. We provide the background and motivation of the shared tasks. We describe a few well-known and widely used NLI algorithms, and summarize the evaluation results. We compare the algorithms in terms of accuracy and complexity in various implementations.
56. **Creating a Large Multi-Layered Representational Repository of Linguistic Code Switched Arabic Data**  
   Current models of building a lexicon can only be used when the available lexicon is of a standard language and domain. As a consequence, linguistic code switched data can only be collected from a certain native Arabic population who, by knowledge, can tailor the target code. The most popular dataset used for this task is the Shapley multilingual dataset of source code that has been recently shared with the public. However, a complete lexicon, for each source language, needs to be built from scratch. In this paper, we address the question if and how large scale data acquisition can be performed from such populations. We propose a corpus, including over 3,000 annotated sentences from four different code switches languages. We also provide a variety of language-specific improvements that enhance the corpus. These improvements include statistical modelling of the clusters of sentences in source languages and parallel text generation, which can help even when the monolingual data is missing. Finally, we use attention based state of the art feature extraction techniques to demonstrate the performance of our proposed models for the multilingual task.
57. **WASA: A Web Application for Sequence Annotation**  
   We present WASA, a web-based system that learns new acoustic models from noisy sequences of time-stamped transcriptions. WASA exploits probabilistic features of acoustic events such as contour, velocity, density and stack to build a latent representation of an acoustic event. A user clicks on this latent representation to trigger a particular transformation on the embedding space. When the transformation is applied, the latent representation corresponding to the event is updated. A label corresponding to the transformation is attached to each time-stamp. WASA then collects maximum likelihood estimates of the transformation from unannotated speech in a random subsample of the training data. This method of deriving posterior inference weights is used to iteratively build a large-margin model using predictions from the previous models, before fusing a vector representation of the transformed embedding space. WASA also extends the scope of applicability of automatic sequence annotation by adopting the recently proposed deep models for end-to-end text classification. We demonstrate our system's application on a challenging commercial speech transcription corpus and show that it outperforms previous approaches in obtaining language model confidence scores in the clean source training corpus, as well as the acoustic model confidence scores in noisy speech.
58. **Part of speech tagging for code switched data**  
   The basic methods of speech tagging are based on time dependent statistics. Part of speech tags can be extracted from them using a number of techniques: lexical, morphological and some syntactic based. A supervised approach has been proposed for Polish, called syntax tagger (ST). It uses dependency between a pregroup of languages and a part of speech tagger.
59. **Improving Semantic Parsing with Neural Generator-Reranker Architecture**  
   Modern neural semantic parsers exploit top-down biases, but also focus on generating outputs that are orthogonal to the target syntactic structure. Such biased model therefore tends to overlook semantic variations that may be present at the top-down and bottom-up levels, and produce outputs that are too fluent for real-world usage. In this paper, we propose an approach to balance these two influences, and improve the quality of the generated text with reinforcement learning. Specifically, we use a recently introduced strategy, which is referred to as "neural generator-reranker architecture", to automatically and systematically balance the bottom-up bias, and the top-down bias via supervised learning. Furthermore, we train the conditional generators to provide new syntax-level features. Experiments on three tasks, including named-entity recognition, dependency parsing, and style transfer, demonstrate that this approach significantly improves the quality of both generated and accurate text.
60. **End-to-End Code-Switching ASR for Low-Resourced Language Pairs**  
   We propose a method for coding-switching and code-switching (CS) denoising that enables speech recovery in low-resourced speech sets in only one-shot training. Unlike previous CS denoising methods that only recover the CS waveform at test time, we also encode the CS spectrogram (representation of the CS audio waveform) and its average spectrogram coefficients. A convolutional neural network (CNN) model is then trained to map the CS spectrum to the spectrogram coefficients. To train the CNN model, we propose to end-to-end train it on both the CS spectrograms and the CS spectrum coefficients by constructing a monaural masked CS mask. Our model is trained end-to-end using our CS denoising methodology without any other unsupervised training objectives. On two speech restoration tasks with limited SNR, we observe that our end-to-end training leads to significant improvements in both clean and noisy speech sets, and our proposed method also achieves state-of-the-art performance.
61. **HateMonitors: Language Agnostic Abuse Detection in Social Media**  
   The problem of identifying the harassment or abuse that people subjectively experience is a hot topic of recent interest. Different algorithms and statistical models have been proposed to identify this type of language, and a lot of work has been done on machine learning and prediction of language patterns. However, most of these methods are not language agnostic, so they cannot be applied to abuse detection in social media. Moreover, most of these methods are based on the corpora that are generated by human annotators, which are largely curated by human agents, and this fact does not always carry over to automatic models. In this paper, we investigate methods for machine learning based detection of abuse in social media, without making any use of corpora curated by humans. We introduce the framework of HateMonitors, and investigate methods that are able to outperform human annotation performance. We show that the supervised methods are more robust to language variation, and are more useful for abuse detection. We also give experimental evidence of the effectiveness of a language agnostic model by comparing it to the supervised systems, when it is evaluated on datasets gathered from different corpora.
62. **Multi-Modal Citizen Science: From Disambiguation to Transcription of Classical Literature**  
   In this paper, we investigate the general problem of identifying the semantic positions of large texts of classical literature in order to extract their underlying semantic information. We propose a novel method for this task based on clustering techniques and a novel description language (DTD) for the annotation. Our main novelty in this paper is an approach to embed knowledge from multiple sources (e.g., lexica, grammars, dictionaries) into a common semantic space. We show that the DTD itself can be used as an efficient mechanism for building knowledge bases in order to facilitate the exploitation of other sources. We evaluate our approach on two classical datasets in terms of the multi-modal capability and the complexity of the solution space. We also show how to utilize the pre-trained word vectors for generating texts. The experiments show that, compared to using only classical textual information, our approach greatly improves the accuracy of retrieving relevant semantic information by delivering a 14.5% higher precision and a 35.3% higher recall on the 2-class semantic positions task.
63. **Named Entity Recognition System for Sindhi Language**  
   The domain of Named Entity Recognition is one of the important areas in Machine Translation (MT). Traditional techniques have shown superiority in handling agglutinative language but such techniques cannot be applied to the language of Sindhi because of the agglutinative nature of Sindhi language. The motivation for using agglutinative Sindhi language lies in the fact that people use Sindhi to write and write Sindhi language is spoken around the world. Therefore, a lot of Sindhi text is being published everyday in the form of newspapers and blogs and will be represented as the news in the form of newswire articles. This paper proposed a system for Sindhi named entity recognition. The proposed system is based on the agglutinative approach to the language of Sindhi. The training corpus consists of a Sindhi document, an input document and output document. Each document has a unique id such as school-id, department-id, code-id etc which is used to compute the output index. It also contains the collocations of the entity and their addresses and how to retrieve the information from a database by searching for the entity and query name respectively. In order to evaluate the performance of the proposed system, a dataset is collected from which the performance is evaluated and comparison with the traditional methods is made. The proposed system has been tested with 6 different classifiers, i.e., statistical, morphological, word embedding and agglutinative. The results show that the performance of the proposed system is equal to the performance of the traditional methods. The experiments also show that the high quality of the proposed system which has a low computational cost and long running time is desirable in case of agglutinative language like Sindhi.
64. **Towards Automatic Bot Detection in Twitter for Health-related Tasks**  
   With the advancements of the intelligent applications, social media platform, such as Twitter, has become a public platform of social media communication where users can exchange information and have discussions. This facilitates users to get involved in the social situation of other people. This unique communication allows users to be involved and to gain information which is beneficial to the society at large. For this reason, Twitter provides a platform to track the behavior of individuals from a range of different domains which are relevant to the global population. With the rapid growth of the service in developing countries, there is a need for automatic bot detection. This work proposes an application that detects bot accounts based on the content of the tweets and the conversational history of the user. The system focuses on analyzing the body of the messages to find out how the account tweets and what kind of conversational history the account has. The candidate tweets are then classified based on their nature and are further grouped by linguistic features such as emoticons, capitalization, and hashtags. Finally, a mention identification algorithm is applied to find mentions of the account in the messages. A dataset of ~2 million messages from 500 different accounts is gathered from this application. The efficacy of the proposed system is tested on this data and the superiority of the proposed system is measured.
65. **Towards Zero-resource Cross-lingual Entity Linking**  
   Automatic Cross-lingual Entity Linking is the task of inferring relations between entities based on their corresponding entity mentions in textual corpora. As the quantity of cross-lingual data grows rapidly, learning a language model is becoming the bottleneck. We propose a technique for zero-resource Cross-lingual Entity Linking that explicitly leverages the availability of parallel entity labels to derive a multi-task, multi-dimensional language model. This is achieved by maintaining multiple latent representations for the entity and its mentions in the lexicon, while predicting entity mentions and their corresponding label for the remaining words and the entity mention. Our model is trained in an end-to-end fashion, thus making it capable of automatically inferring all types of entity relations in the target language from any parallel corpora. We report large improvements over previous approaches on five cross-lingual knowledge graphs with disparate knowledge taxonomies.
66. **The Source-Target Domain Mismatch Problem in Machine Translation**  
   Although Neural Machine Translation (NMT) has recently shown remarkable performance on several language pairs, it has yet to reach the performance of Human-Experts. We refer to this as the Domain Mismatch Problem in MT. This problem remains an open challenge, even in cases where NMT achieves comparable or even better performance. We study this problem using a source-target language pair in the domain mismatch case. The source-target domain mismatch is based on the gap between the textual representations of the source and target data sets. In this paper, we show that the source-target domain mismatch can be easily modeled with a dictionary, allowing us to obtain a predictor using a language model which is trained on the target and the source language pairs only. We also propose a regularization term for this model that encourages sources to disambiguate between the source and target domains using their own data as much as possible. We evaluate our models on the English-German MUC dataset and find that the domain mismatch outperforms the word-misalignment problem, which is not solved by non-reconstructed source representations.
67. **Integrated Triaging for Fast Reading Comprehension**  
   We introduce a novel scheme to answer questions about a large document in a very fast read-through that is compatible with existing large-scale document summarization systems. Our design takes inspiration from the self-correcting and self-organizing map models. The answer is then used as context and weighted to show the most important entities. We call the combined system Integrated Triaging. It performs a large self-processing step (triaging) of the document and then aggregates the information coming from different steps to produce an answer. The benefit of our approach is that in each step Triaging provides extra information for improving the overall document summarization accuracy. To assess the performance of our approach we compared it against a state-of-the-art answer summarization system. Experimental results show that our integrated triaging system obtains competitive results.
68. **OpenNRE: An Open and Extensible Toolkit for Neural Relation Extraction**  
   Relation Extraction is the task of extracting all the semantic relationships between entities in a given context. The tasks of neural relation extraction are to facilitate the study of deep learning based systems and thus contribute to the improvement of relation extraction systems. Neural Relation Extraction is currently in an infancy stage and therefore is missing a few key components, such as top-down annotation and ontological facts. In this paper, we introduce OpenNRE, an open and extensible toolkit to help the community on their neural relation extraction research and development. We also describe the pipeline of OpenNRE to provide basic concepts of neural relation extraction, its unified interface, how to use it with various natural language processing tools, and various technical improvements. We also give a brief overview of the recent progress on neural relation extraction, such as introduction of the notion of paraphrase and the release of a new large dataset of 86K relation facts.
69. **Emirati-Accented Speaker Identification in Stressful Talking Conditions**  
   In this paper, we describe a speech recognition system for identifying people of Emirati descent, one of the fastest growing generations in the United Arab Emirates (UAE). This population is particularly underserved in speech recognition systems as they are much smaller than the non-Arabic majority population, and have a distinct accent. To address this shortcoming, we develop a feature extraction module and a novel method for building hand-crafted features for recognition. Our proposed model reaches an accuracy of 80% for acoustic tagging, using minimal feature set of hand-crafted features, including a latency of 0.06 seconds and retrieval latency of 0.04 seconds, without using specialised speech recognizers or feature extractors. Furthermore, our system is robust to non-standard speaking conditions and linguistic variation due to their location in the world. In the collection of test data, our system has been observed to perform best in noisy conditions.
70. **Self-Attention Transducers for End-to-End Speech Recognition**  
   This paper presents Self-Attention Transducers, a flexible set of self-attention-based neural network architectures for end-to-end speech recognition. A self-attention layer, referred to as self-attention transformer (STAR), provides a general mechanism for controlling attention to speech segments. By grouping speech segments into speech tokens, STAR is able to capture the global contextual information that is not captured by other types of attention mechanisms, such as self-attention. Such self-attention-based structures can learn to focus attention to segments or words in an utterance with which they are most compatible. STAR can also be trained end-to-end in an unsupervised fashion, thereby obtaining state-of-the-art results on both uni-modal and multi-modal datasets. By applying STAR to the widely used WOZ and Hub5 mid-level utterances, the improvements are up to 11.6% and 6.4% in WOZ and Hub5, respectively, when evaluated on the largest publicly available dataset, Omniglot.
71. **Fake news detection using Deep Learning**  
   In the social media era, fake news has been gaining popularity due to the spreading of misinformation, usually involving professional smear campaigns and appealing to people's emotions. Hence, significant efforts have been made to tackle this problem. In this work, we propose a method to detect fake news using machine learning techniques and conduct a case study for that purpose. We compare a variety of features with different information source types, embedding features extracted from different textual content sources, and dimensionality reduction techniques in order to detect fake news. We also propose an ensemble algorithm to automatically combine the output from various information sources and perform supervised feature selection. Our results show that: 1) using multiple information sources (blogs, social media, news) is the most efficient way to automatically detect fake news, 2) extracting feature embeddings extracted from different textual content sources is a promising approach to improve the detection performance. Our final dataset contains a subset of fake news articles with the labels of veracity and location. Experimental results show that our approach can correctly classify all of them, in both the veracity and location classifications.
72. **Semi-Supervised Neural Text Generation by Joint Learning of Natural Language Generation and Natural Language Understanding Models**  
   The recent deep learning-based natural language generation systems based on Generative Adversarial Networks (GANs) generate unstructured texts from textual inputs. These models are trained from scratch with the limited amount of manually annotated natural language examples available. However, while unstructured text generation models have been shown to achieve the state-of-the-art in automatic machine translation and text summarization, in natural language generation it is still difficult to generate text that captures the semantics of the original input, such as in text summarization. In this work, we present a semi-supervised neural text generation model that learns a representation of the input text using a language model, and generates a high-quality text by considering the meaning of its input. Our experimental results show that our model significantly improves the performance of both a baseline without model-based natural language understanding and a traditional unstructured text generation model in semantic-driven text summarization.
73. **Language-Agnostic Syllabification with Neural Sequence Labeling**  
   We present a method for alleviating the effect of feature transformation on language-agnostic topic models. Our approach is grounded in the neural decoder of Fisher, Gretton, and Handberg (2017) and applies to settings that are agnostic to the style of the encoder. The method exploits the flexible structure of the encoder as a feature transformation parameter and we prove how it is jointly optimised to obtain the best possible rescoring algorithm. Moreover, we show that a version of this method that uses soft labels and back-transformed words achieves competitive results in combination with weakly supervised alternatives.
74. **Augmenting Non-Collaborative Dialog Systems with Explicit Semantic and Strategic Dialog History**  
   Interactive dialog systems (IDSs) have been proposed as tools that enable humans to converse with computers about a range of problems, from managing hundreds of millions of credit card transactions to handling customer service. IDSs, however, have also attracted increased interest in the social sciences, as their benefits can be attractive to researchers who wish to improve their own work. In this paper, we attempt to provide a model for how to develop an IDS in the sense that we make a strong effort to identify an enriched version of an existing NCS model so that we can enhance its communication quality and both its explanatory and performance characteristics. The main aim is to enhance the identifiability of the generated responses, thus effectively identifying the seeds of an added sub-dialog. With this in mind, we experimentally evaluated several modern multi-task NCS models on two typical, but complementary datasets, measuring their ability to generate and to remember an explicit set of previous conversations. The learned latent constructs were shown to augment the extractive nature of the original model by preserving more precise and more explicit information about how its responses are generated.
75. **Tag-based Multi-Span Extraction in Reading Comprehension**  
   We introduce the task of multi-span extraction in the reading comprehension setting, which aims to select and use a multi-span representation for a specific answer-span. This is useful for applications that require modeling of more than one relation, for instance in relational question answering (referred to as SM-QA). We propose a new neural network architecture with a parameter sharing mechanism, which jointly learns to select and use a representation for each span and a classifier for the answer span. In experiments on two well-known multi-span datasets, we show that our model outperforms existing neural network architectures in all three tasks. Finally, we perform ablative analysis on the model's outputs and find that it captures the relation between selected spans, even if only from a single answer-span.
76. **Recent Advances in End-to-End Spoken Language Understanding**  
   Spoken language understanding (SLU) is the task of automatically understanding and providing useful responses to spoken dialogue. Traditional SLU uses hand-crafted feature representations. Most recent research works in SLU heavily depend on deep learning, leveraging deep neural networks (DNNs) and other models for effective feature representation. However, recent research shows that DNNs and other such models can perform poorly on a special kind of end-to-end SLU problems, in which no hand-crafted feature representations are used. This problem cannot be solved using conventional methods. This problem is mainly due to the behavior of DNNs. There are various ways to improve the performances of DNNs in SLU tasks, including incorporating the additional information contained in spoken conversation, and including additional modalities to analyze the model. This paper provides a detailed analysis of the problem of end-to-end SLU, analyzing the main strategies applied to it. A comprehensive evaluation of end-to-end SLU is performed on multiple popular datasets, with different levels of naturalness. Experimental results show that the proposed method reaches the state-of-the-art performance on several SLU tasks with a significant performance gain.
77. **Controllable Data Synthesis Method for Grammatical Error Correction**  
   The task of grammar correction usually requires extracting grammatical patterns from sentences in the raw text and then training a classifier to make corrections. However, due to the lack of proper manual constraints, large-scale training data has to be manually constructed. Since the quality of the constructed data can degrade significantly after it is used to train a classification model, it is vital to consider the best strategy to obtain the desired level of supervision in order to obtain the optimal model. The most widely used strategy, enforcing phrase-level supervision, requires more annotated training data than actual text. Thus, the human cost for obtaining sufficient training data can be very high. We propose a data synthesis method to enable the automatic creation of more expressive training data. Our method generates data by generating new sentences in the style of previously-observed sentences. In order to deal with the lack of explicit supervision and for practical purposes, we also rely on the prior knowledge in order to make full use of the syntactic knowledge available in an existing classifier. As a result, our data synthesis method can produce high-quality data with reduced human intervention. The proposed method has been evaluated by systematically comparing the learned model and that of conventional supervised learning. The results suggest that our approach is able to generate grammatical sentences with better quality than the conventional supervised training.
78. **A Dynamic Strategy Coach for Effective Negotiation**  
   In this paper, we propose a dynamic strategy coach (DScoach) to implement an effective negotiation strategy in a transaction. The proposed DScoach takes as input a transaction scenario and a payoff matrix for estimating the value and quantity of each outcome (or cost). From the information of payoff matrix, we generate a strategy consisting of two components, each component being a machine learning algorithm (such as SVM, linear SVM and a linear approximation) for estimating the value and quantity of an outcome of a transaction. We systematically evaluate DScoach through extensive experiments in negotiation and real-world trade applications. The experimental results show that our approach can generate better value and quantity of outcomes in negotiation compared to baseline.
79. **A Pilot Study for Chinese SQL Semantic Parsing**  
   The paper describes our work on the semantic parser for Chinese queries, i.e., a semantic parser which can parse sentences using concepts which are computed from database tables. We demonstrate our parser on the provided benchmark dataset of English queries (EnctuParsing), while participating in the 2nd IWSLT query-by-example/syntactic parsing challenge of 2016. The experimental results show that the semantic parser can achieve competitive performance compared with the baseline systems.
80. **Multi-Head Attention with Diversity for Learning Grounded Multilingual Multimodal Representations**  
   This paper presents a multi-head attention (MHA) with diversity (MHA-D) model for learning multimodal representations of language. The MHA-D model adaptively fuses bilingual information with multi-head attention (MHA) from multiple sentence layers to jointly learn language representations in a bilingual network. This model improves the performance of a single-head attention (SPA) model. The discriminative information of each frame can be obtained by ensemble of SPA-D model. Experiment results on two language modeling tasks show the advantages of MHA-D model.
81. **Lexical Features Are More Vulnerable, Syntactic Features Have More Predictive Power**  
   In this work, we use RNNs to extract morphological features of written Japanese sentences, and then use Support Vector Machines to perform state-of-the-art test-retest accuracy. We observe that compared to noun-modifier morphology, the forms of verb stems and the number of verb suffixes are more vulnerable to RNN's predictions. The experimental results indicate that the RNN-based feature extractors contain more predictive power and perform on par with noun-modifier models.
82. **Semantic Graph Parsing with Recurrent Neural Network DAG Grammars**  
   We present a novel model for semantic graph parsing which uses a recurrent neural network to model the representation of nodes from a given graph. Given an incomplete graph and a corresponding parse tree we first compute the consensus of the graph's DAG grammars and evaluate this parser's effectiveness for predicting the textual form of node labels. The subsequent parse is then obtained by using this model to predict a sequence of label combinations. We evaluate our model by parsing utterances from the RECOLA conversational dialogue corpus, and find that our parser produces valid label combinations, despite having access to an incomplete graph and only using a small amount of training data.
83. **Interrogating the Explanatory Power of Attention in Neural Machine Translation**  
   Attention is an effective mechanism to incorporate prior knowledge into neural machine translation (NMT). However, it has not been widely explored in NMT. In this paper, we demonstrate that attention not only boosts NMT interpretability but also improves the translation quality. More specifically, we investigate different ways to use attention in NMT, and report gains over baseline systems that do not employ attention. We also propose a unified model that utilizes both attention and attention mechanism. Experimental results show the effectiveness of attention on improving translation quality in both Hutter Prize based evaluation as well as empirical evaluations on the WMT 2014 English to German and English to French translation tasks.
84. **Simple and Effective Paraphrastic Similarity from Parallel Translations**  
   Paraphrastic similarity between a sentence pair can be defined as the likelihood of the two sentences being written by the same author. In this paper, we propose a novel approach to parallel paraphrase disambiguation which translates into an artificial intelligence task of disambiguating similar sentences in a conversation, with a goal of getting more accurate and effective paraphrase accuracy. First, we develop a differentiable adversarial neural network to translate a paraphrase from a non-parallel language to a parallel language and an error signal indicating which translation is incorrect. Second, we propose a parallel paraphrase similarity feature, which is defined as the product of a set of variables and is used in a parallel paraphrase similarity prediction task. Third, we perform the proposed parallel paraphrase disambiguation using different types of statistical metrics to evaluate the performance of the proposed model, which will give us a more fine-grained interpretation of the resulting disambiguation results. Results show that the proposed model achieves state-of-the-art performance of 78.17% in absolute accuracy on the actual test set.
85. **Automatic Fact-guided Sentence Modification**  
   With the growing computational power, it is becoming increasingly possible to learn effective sentence encoders. In fact, a novel phrase-based learning approach to automatic sentence modification has been proposed and shown to generate sentences with semantic improvements. Inspired by these results, we introduce an automated sentence modification system named FactTree. Our goal is to learn how to modify sentences to achieve the most semantic information in the original text while keeping its original linguistic information. To achieve this goal, we utilize a deep neural network framework based on an attention mechanism to jointly learn both the sentence structure and semantic representations. Based on the fact that a sentence does not have to be perfect in order to generate semantic information, we learn to exploit the non-perfect information in an automatic way by using attention mechanism. We evaluate the performance of our approach on two standard data sets and show that FactTree consistently outperforms state-of-the-art approaches, reaching the state-of-the-art performance on these datasets.
86. **DiPCo -- Dinner Party Corpus**  
   In this paper we report the results of our first study on the deployment of translation systems in the movie industry. We present a new dataset based on screenings from an English-French movie "DiPCo" which contains 68 million screenplays and 300 million hours of subtitles from the "French edition" of "The Little Prince". Given a movie script, we train a translation system based on a combination of bilingual lexicons, character embedding, morphological features and probabilistic models. We test our method on all of the above and report successful deployment of various systems with a 94% accuracy. The result on this corpus is quite promising and opens a promising path for future research in this area.
87. **The Universal Decompositional Semantics Dataset and Decomp Toolkit**  
   Understanding the meaning of natural language sentences (or phrases) is an important problem in natural language processing (NLP). In this paper, we present a new dataset of natural language sentences over 19 topics in order to gain insights into a better understanding of the meaning of natural language sentences. Inspired by the universal semantic role of a word, which plays an essential role in decoding the sentence, we introduce the Universal Decompositional Semantics Dataset (UDSD) to compare the meaning of the diverse categories in UDSD. We present the decomp toolkit to exploit the UDSD dataset, that provides an extensible grammar by which complex natural language sentences can be decomposed into their semantic components. We propose some examples and detailed analysis to understand the utility of the UDSD dataset in modeling complex sentences.
88. **A Critique of the Smooth Inverse Frequency Sentence Embeddings**  
   Natural language processing (NLP) research and applications have seen remarkable progress thanks to the introduction of new models based on neural networks that, at the time of writing, include several new methods for modeling complex syntactic phenomena like word order and distributional properties. This paper reviews and criticizes recent approaches to jointly learn an embedding of words and sentences. To this end, we first give a comprehensive review of all known embedding learning techniques, highlighting their properties, methods, and theoretical foundations. We then survey the neural architectures that use these embeddings, on the basis of an in-depth analysis of them for various syntactic phenomena. We provide an assessment of the quality of their outputs, highlighting the common problems that arise from such methods, including over-fitting and worse training samples. We also provide empirical results that confirm our claims and to suggest directions for future research.
89. **Embeddings for DNN speaker adaptive training**  
   This paper investigates the use of speaker embeddings in deep neural networks. In particular, we study how to embed DNN parameters (i.e. audio embeddings and corresponding spectrograms) in order to train a generative adversarial network (GAN). To this end, we propose two distinct methods for embeddings training. The first method relies on embedding embeddings directly in the input space of the DNN and is the target of this paper. The second method in this work relies on training a discriminator to discriminate between the hidden activations of the corresponding embeddings and those of the normal embeddings of the input features. The use of these methods leads to effective performance in automatic speaker adaptation tasks, namely, automatic speech recognition and end-to-end speaker diarization. The training of these methods is performed in an end-to-end fashion. Moreover, we investigate an alternative setup where the embeddings are embedded within a variational autoencoder (VAE). We experimentally show that this allows to use speaker embeddings in an effective way in those tasks, which hitherto have been out of reach for conventional methods.
90. **Regressing Word and Sentence Embeddings for Regularization of Neural Machine Translation**  
   Automatic pretraining of neural machine translation systems has been shown to substantially improve their performance. However, the regularization effect of pretraining remains under-researched. In this paper, we describe a new regularization method, called DRT (Regression via Synonym Translation), which is able to regularize the model by using the reverse direction of source and target word embedding space. In addition, to overcome problems of dependency on auxiliary variables, we employ a variant of RNNs (i.e., residual networks) to make our model effective on the target side. Our experiments on three German-English language pairs show the effectiveness of the proposed regularization method.
91. **A Closer Look at Data Bias in Neural Extractive Summarization Models**  
   Neural extractive summarization models using long short-term memory (LSTM) networks have been recently proposed and shown to produce state-of-the-art results for some of the news summarization tasks. Their use of recurrent neural networks (RNNs) to encode their input documents allows the training of these models by back-propagation. This training allows these models to learn meaningful representations of sentences which improve the state-of-the-art results. However, when the training dataset is biased, such as the Newsroom dataset and the TAC KBP dataset, there is a very real chance that the model learns biased representations in the learnt latent space. We show that in these datasets the extracted sentences from LSTM models are biased towards the annotator's preferred context and consequently those extractive sentences generated by the LSTM model do not contain the true information. In particular, LSTM networks trained on the Newsroom dataset exhibit strong bias towards information and context that do not reflect the content of the original documents. Furthermore, when training the LSTM models on the TAC KBP dataset, the same trained LSTM models are not biased towards the same information that LSTM networks trained on the Newsroom dataset and on the TAC KBP dataset yield. These biased training sets can result in in-group bias and out-group bias. Consequently, the automatic extraction of information from extracted sentences based on the LSTM network is not as accurate as the information from extracted sentences that have been trained using non-biased training sets.
92. **Acoustic Model Adaptation from Raw Waveforms with SincNet**  
   Neural network acoustic model adaptation methods attempt to extend the network models to learn new tasks using previously seen data. While the performance of many of the adaptation methods depends on the nature of the adaptation task, usually the data distribution used for the adaptation task is unknown. In this work, we show how to obtain a direct benefit from data-driven adaptation methods. Our method, called SincNet, has the strength of using all of the available data within a given adaptation task in order to obtain an improved adaptation performance. Experiments on the acoustic modeling tasks for Arabic speech recognition (ARSA) and English speech recognition (ESR) show that SincNet outperforms other adaptation methods. We also compare SincNet with baseline methods on the acoustic modeling task for English speech recognition and show that SincNet's performance is substantially better than the baseline models in both tasks.
93. **Incremental processing of noisy user utterances in the spoken language understanding task**  
   The state-of-the-art spoken language understanding (SLU) systems are mostly based on feature extraction algorithms and they are static. In this work we present an incremental SLU system (Aspl{\"a}l {\"o}v{\"a}kst{\"o}v{\"a}) and show how to apply incremental processing strategies into these systems. Moreover, we show how a two stage system can be used in which an embedding layer and an SLU layer are used together. Experiments with the available benchmark datasets (DECREE, SNLI, BSNLI) show that Aspl{\"o}v{\"a}kst{\"o}v{\"a} algorithm is not only significantly faster than the dynamic processing strategies, but also it offers comparable accuracy of a static SLU system.
94. **Non-native Speaker Verification for Spoken Language Assessment**  
   This paper introduces a technique for automatic speech recognition (ASR) of native speakers for spoken language assessment (SLA). The technique incorporates non-native speaker features, such as Wav2Vec, so that it can be applied to a standard SMT system, such as GloVe, and achieve a 97% AER rate for ASR. A comparative study is conducted between the method and a standard SMT method in three domains of commonly used ASR, audio-only (dialog) ASR, and video-only ASR. In the audio-only domain, the method achieves a 95% AER rate in a case study involving 22 native speakers, whereas in the video-only domain the method performs at a higher level than the standard SMT system.
95. **Type-aware Convolutional Neural Networks for Slot Filling**  
   In this paper, we present a novel instance-level slot filling model based on convolutional neural networks (CNNs). Given a slot candidate from a pre-allocated search space, the model generates a query embedding that encodes the slot candidate. In addition, a new type-aware segmentation is performed on the candidate embedding to classify the slot into a given type. Experiments on two large-scale datasets show that the proposed model achieves significant improvements over the current state-of-the-art models for slot filling.
96. **Essentia: Mining Domain-Specific Paraphrases with Word-Alignment Graphs**  
   We present a neural network-based approach for automatically extracting text paraphrases from a given sentence. The system works for domains like News, Tweets and tweets. It assumes access to semantic entities and mentions (e.g. Person X, Organization), and works from the local (word level) to global (entity level) level. The model explicitly distinguishes the target word in the original paraphrase from the source of the paraphrase. It learns to optimize the alignment between the local and global word-alignment graphs by employing different types of word alignments. The alignments are learned by optimizing the Viterbi loss between the obtained aligned phrase and the target word. The extracted paraphrases are then processed by a reranking step, obtaining high-quality sentences. Experimental results show that the proposed model obtains more accurate paraphrases compared to existing state-of-the-art models.
97. **State-of-the-Art Speech Recognition Using Multi-Stream Self-Attention With Dilated 1D Convolutions**  
   In this paper, we propose a novel multi-stream self-attention-based approach for speech recognition. Different from conventional models which are limited to just a single stream or multi-channel self-attention, we introduce a multi-stream convolutional self-attention model with a new self-attention-based decoder to capture multi-scale contextual information simultaneously. We first design a dilation layer in the model to incrementally extract features from both the lower and upper speech channels, and then create another dilation layer to fuse the features from different channels. Finally, a convolutional self-attention mechanism is used to merge multiple representations from different channels. By performing the proposed approach on the Switchboard corpus, we achieve a recognition rate of 64.4%, which is significantly higher than conventional models (e.g., GloVe-20) on the WSJ-20 test set. Our model can naturally learn multi-scale contextual information through combining different features from different channels.
98. **BillSum: A Corpus for Automatic Summarization of US Legislation**  
   This paper introduces a corpus of text documents with legislative bill titles, and documents describing underlying legislation. The corpus contains 7,463 full text documents (reports, articles, executive orders, or quasi-reports) and 2,256 summary documents. The corpus was obtained from the US Congress (congress.gov), and was manually processed and labeled by committee-members. This paper describes the data collection process, process of taxonomy extraction, and annotation of the corpus. The corpus is an important resource to be used as a benchmark for automatic summarization of US legislation. The corpus allows for the training of an NLP model for summarizing texts about US legislation. The method to generate summaries is described and the corpus is further analyzed to understand the characteristics of summaries. The dataset is available online at cmp.ualberta.ca/billsum/.
99. **Learning to estimate label uncertainty for automatic radiology report parsing**  
   The paper proposes an approach for the automatic extraction of radiology report parser labels from a set of radiology reports. The parsing algorithm exploits the similarity between the sentence and label space and learns an estimation of the precision of label labels from the support vectors produced by the parser. The precision of the label labels is evaluated by the error probability of a baseline parser. Experimental results on a large training set show that the approach consistently produces high precision and can successfully infer new label labels for new radiology reports at significantly lower cost than previous approaches.
100. **Improved Word Sense Disambiguation Using Pre-Trained Contextualized Word Representations**  
   This paper presents a novel approach to provide improved word sense disambiguation (WSD) for English and Japanese. Our approach is based on two methods: (i) using co-occurrence of pairwise related terms as a source of context, and (ii) building new word vectors using a maximum inner product approximation of the features from a classifier. The co-occurrence terms are derived by using WordNet with a pre-trained word representation. Using a large corpus, we show that even the word vectors trained using context information get more accurate than the ones trained using only external semantic resources.
101. **Machine Translation for Machines: the Sentiment Classification Use Case**  
    Machine Translation has become an important computational tool for natural language processing. In recent years the number of available machine translation systems has multiplied by several orders of magnitude, and existing machine translation systems are experiencing delays and quality problems. In this work, we applied SMT on a large-scale sentiment classification task in order to compare the translations produced by different different machine translation systems and to identify the performance degradation faced by different machine translation systems. We assessed the quality of the translations produced by SMT for sentence-level, phrase-level and query-level evaluation. The results of this work were based on large-scale corpora (more than two billion words) from different sources. The results show the negative impact of short-cut heuristics in SMT and a lack of consistency between the results produced by different systems.
102. **Dialogue Transformers**  
    In this paper we introduce the first dialogue system with 3D translation capabilities which exploits a recently developed novel translation model. Our approach, which we refer to as Dialogue Transformers, represents a cross-modal sequence of 3D joint-perspective images as an unordered sequence of depth images with each frame being spatially-aligned across all depth views and allowing each frame to be represented as a 3D joint-perspective image. We demonstrate the generalizability of our approach with a user study using Mechanical Turk, and show how it significantly improves performance when compared to baseline methods that perform 3D pose regression on 3D views but fail to take advantage of the appearance information.
103. **When and Why is Document-level Context Useful in Neural Machine Translation?**  
    We demonstrate that document-level context in neural machine translation (NMT) has distinct effects on end-to-end translation performance, and that using document-level context within NMT is not only useful but essential to achieve higher translation quality. On top of an ensemble of several single-model and beam-search based multi-model models, we evaluate the impact of document-level context with baseline beam-search and language models, and with a BERT-based translation model as well. The experimental results show that with more document-level context, the BERT-based model achieves significant improvements, significantly outperforming the single-model and ensemble models.
104. **Robust Semantic Parsing with Adversarial Learning for Domain Generalization**  
    As a domain expert has more domain knowledge, it can enhance the overall generalization capability of the model. However, domain generalization for semantic parsing is still challenging, due to an untrustworthy training dataset. To handle the problem, this paper proposes a semantic parsing model with adversarial learning for domain generalization. Specifically, we first cast semantic parsing as a cross-domain word-level adversarial learning problem, and explicitly formulate the domain generalization as a joint semantic-adversarial training process. We then make the observation that the domain generalization can be made more robust by reducing the discrepancy between our word and input embeddings, as well as applying the adversarial learning model to ensure that word embeddings are able to distinguish the image from the textual content, given the input images. Extensive experiments on benchmark datasets show that the proposed model can achieve a significant performance gain over the baseline, without using any additional data or features. We also demonstrate the benefits of using adversarial training to promote domain generalization.
105. **Analyzing Sentence Fusion in Abstractive Summarization**  
    Neural abstractive summarization models are known to produce summarizing summaries by inducing a fusion between the second and the first sentence(s) of the input document, as has been shown to be effective for various NLP tasks such as sentiment analysis and question answering. Despite the empirical success, how to automatically fuse the outputs of these models with different types of features remains an open problem. In this paper, we propose a way to perform sentence fusion in an end-to-end way. It generates the final summary by fusing the sequence of sentences and weighting the fusion weights according to the content similarity. We do the generation of sentence fusion in two different settings: end-to-end and supervised. For the supervised setting, we estimate the quality of summaries via the Pearson correlation between the first and second sentence (two-sentence) fusion weights. For the end-to-end setting, we estimate the summarization quality directly based on the input sentence and the sentence fusion weights. Experiments on a standard benchmark datasets demonstrate that the proposed framework outperforms the current state-of-the-art methods by a large margin and is able to produce competitive and sometimes even better-quality summaries.
106. **Multilingual End-to-End Speech Translation**  
    In this paper we present an end-to-end speech translation system for multiple languages. Our system is based on the LSTM (Long Short-Term Memory) models with a novel Bi-LSTM (Bi-LSTM) network. Instead of treating each language independently, we allow for multiple language combinations through multi-stage learning, by training the same LSTM models for different languages separately. To the best of our knowledge this is the first end-to-end speech translation system which allows for multiple language combinations. In order to minimize the number of parameters and the number of bilingual embeddings, we train only two-layers of both Bi-LSTM and Bi-LSTM using large bilingual corpora and translate at almost the same quality as using a single-layer. Experiments on the TIMIT and WSJ datasets indicate that the system can cope with arbitrary language combinations.
107. **VOnDA: A Framework for Ontology-Based Dialogue Management**  
    In a real-world collaborative environment such as an online collaborative platform or a social chatbot, having a good conversational agent requires many complex features and mechanisms. Such systems need a fair number of services that help users perform basic tasks. However, achieving these services, especially in an open domain, is often challenging. A natural approach to this problem is to have an ontology that explicitly represents the end-user's needs. In this paper, we present VOnDA, a dialog system that automatically generates a human-readable and maintainable ontology. We have already integrated our ontology into the popular LTbA software and have seen significant improvements. This article discusses the various design decisions and techniques adopted to build a good and self-explanatory ontology for a spoken language understanding system.
108. **Grammatical Error Correction in Low-Resource Scenarios**  
    We consider the problem of morphologically erroneous utterances in English. To do so, we define a problem domain and first perform a literature review of the literature on the translation of grammatical errors into normal translations. We then present three systems which utilize tools from the Machine Translation (MT) literature: BERT-translated, MT-TAMER, and a simple unsupervised system. In all three systems, we take a (primal) language model (LMM) as input and report statistics about the normalized text, across all the three different MT systems, and comparing those statistics with existing normalized information. The goal is to assess the extent to which these systems can distinguish between grammar errors in generated normal text and normal text, and the impact such errors have on MT systems.
109. **Application of Low-resource Machine Translation Techniques to Russian-Tatar Language Pair**  
    This paper explores the impact of preprocessing pre-processing methods in low-resource machine translation for Russian-Tatar language pair. More precisely, our approach consists of two phases. Firstly, we create a dictionary of pretrained word-embeddings. Afterwards, we define the mapping between the domain-word embedding space of Russian to the domain-independent embedding space of Tatar. Next, we use two normalization techniques, contrastive lower-bound clustering and word co-occurrence ranking to build a post-processing dictionary of pre-processing. Then, we are able to use any pre-processing method to transform Russian-Tatar word-embeddings to Tatar-language word-embeddings. Experiments show that, the use of pre-processing methods significantly improves the translation quality of Russian-Tatar language pair for all translation directions.
110. **A Survey of Methods to Leverage Monolingual Data in Low-resource Neural Machine Translation**  
    Despite recent progress in neural machine translation (NMT) systems, many of them still struggle to scale to massive monolingual corpora. This problem is often attributed to the difficulty of parallelizing monolingual data, or by the lack of appropriately sized monolingual corpora that would assist in training neural NMT models. These problems might be alleviated if translation resources could be transferred across systems or domains, in order to effectively leverage additional monolingual data for training. In this paper, we provide a comprehensive survey of methods that address the problem of leveraging monolingual data for low-resource NMT. We begin by summarizing the existing methods, then discuss the shortcomings of the existing methods and focus on ways that existing methods might be improved, by addressing these limitations. We then review recent published work that addresses the problem of cross-lingual transfer and propose ways to leverage multi-lingual sources in order to leverage additional monolingual data for boosting the performance of low-resource NMT systems. Finally, we discuss potential challenges in scaling neural NMT models for languages with low amounts of monolingual data.
111. **Neural Zero-Inflated Quality Estimation Model For Automatic Speech Recognition System**  
    In this paper, we introduce a Neural Zero-Inflated Quality Estimation (NZE) model to automatic speech recognition (ASR). Unlike prior works, the NZE model does not need to estimate the residual difference of non-zero zero (ZZ) acoustic parameters or propose the optimal weight of the quantization filter bank, as it merely outputs the aggregated loss of the joint model. To this end, we develop a neural network-based architecture, in which a subnetwork is trained to learn the aggregation of the residual error maps and residual errors. We further demonstrate the NZE model by training it on the mixed hidden Markov model (HMM) of the noisy Mandarin speech data. Experimental results on the Oulu-CASIA data sets show that our model improves ASR performance compared with state-of-the-art methods. The NZE model is free of complex inference and can be trained end-to-end, making it suitable for practical applications.
112. **Extracting UMLS Concepts from Medical Text Using General and Domain-Specific Deep Learning Models**  
    The United States National Library of Medicine's UMLS system is a gold standard for systematically summarizing large collections of medical texts. However, as modern digitized documents continue to grow larger, existing resources for summarization become inaccessible, limiting the extent to which practitioners can extract high quality ideas from the data. We present a novel approach to efficiently extracting UMLS concepts from documents, i.e. to create abstractive summaries that cover as much of the document as possible. We do so by developing a deep neural network to exploit a large collection of facts in document abstracts. We then train a domain-specific neural model to generate abstracts that are specifically targeted at the identified concepts. We show that our system can find many UMLS concepts in the abstracts that have not been previously used as examples. Our empirical study confirms that these extracted UMLS concepts are helpful for decision support.
113. **Identifying Nuances in Fake News vs. Satire: Using Semantic and Linguistic Cues**  
    Internet users have become more familiar with Fake News (i.e., articles that are either intentionally or mistakenly fake), but there is no universal definition of fake news that can be applied to all fake news. Therefore, it is important to identify which fake news are suitable for a given news article in order to determine if it can be taken seriously, as well as to identify which news articles are fake news. Identifying fake news is important not only to improve news reading and consumption, but also to promote the spread of inaccurate news and spread of misinformation online. Although various features have been used to identify fake news, such as social media data and article features, there are other factors that are critical for distinguishing real news from fake news. In this paper, we analyze the various characteristics of fake news and identify various kinds of fake news from news articles. We show that many fake news use false claim facts to describe news articles, thus differentiating fake news from real news. In addition, we analyze the social factors that play an important role in identifying fake news. We introduce FakeNewsReading, which is a concept used to measure how well a news article is read by the users, as well as for predicting the probability of fake news readers to read a news article, as a novel way to analyze fake news. To examine the performance of FakeNewsReading, we construct a Fake News Reading Task (FNRT) based on Amazon Mechanical Turk data collected from fake news websites and evaluate two different models to address this task: (i) a gated graph model and (ii) a bag-of-words model. Finally, we experimentally demonstrate that FNRT can be used to evaluate fake news readers and that fake news reading performance can be used to predict the probability of fake news readers to read a news article.
114. **Abstractive Dialog Summarization with Semantic Scaffolds**  
    The rise of knowledge graphs and data mining have changed the way humans interact with information. In addition, due to the ability to summarize data, machines can be more efficient and effective when providing context. Our work addresses the task of automated abstractive dialog summarization from open-domain customer service domains. We propose to leverage data to summarize conversations into a full executable document. We perform automated reasoning by relying on principles of SQL - enriched with rich semantic information such as ontologies and entity descriptions. We build an extension to Extractable Argument Structure (A2S) and Semantic Tableau Analysis (STA) that incorporate this enriched data, leading to a much richer insight. We discuss several scenarios that follow the semantic tableau paradigm, including abstractive or semi-automatic dialog summarization, and consider various approaches for building such an extension. In several evaluation experiments, we observe significant improvements in terms of both qualitative evaluation and human evaluation of automated dialog summarization, especially when documents are clustered by semantics.
115. **BookQA: Stories of Challenges and Opportunities**  
    Recent advances in both academic and industrial areas have resulted in a stream of book-based question-answering (QA) datasets for the past few years. These datasets provide rich in-depth details of how questions are posed and answered, thus offering an ideal opportunity to explore novel challenges and opportunities arising in such dialogues. However, a question-answering task based on human-generated short stories is extremely challenging. These stories usually present a combination of the following elements: (1) remarkable insights that provide novel information and insight into the story, and (2) qualitative inconsistencies in words, phrasal expressions, and their meanings. We argue that for a QA task based on human-generated short stories, it is especially important to discover and reason over these challenging elements, and analyze their effects to help improve the quality of short stories. Therefore, we present an end-to-end sequence to sequence (Seq2Seq) model for automatic inference of these elements from dialogue history, semantic information of the story, and knowledge about current questions. To the best of our knowledge, this is the first such model to automatically extract these elements from dialogue history. We also present a benchmark for the generation of dialogue history for QA tasks based on these elements. We evaluate the quality of generated story passages using open source resources, and show that automatically estimating element types can provide significant improvement.
116. **Exploiting BERT for End-to-End Aspect-based Sentiment Analysis**  
    Aspect-based sentiment analysis (ABSA) is the task of inferring whether a sentence expresses the underlying opinion of the speaker. Since sentiment analysis is an aspect-based task, one of the crucial challenges is how to effectively leverage models based on contextual information (e.g., the context of the sentence). To this end, we propose a novel BERT based system to exploit contextual information. In our model, BERT is directly adopted to learn more sophisticated attention to the sentence aspect contexts. In addition, instead of treating aspect contexts as attention to individual words, we treat aspect contexts as a continuous semantic space which is not affected by word attention. Extensive experiments on a benchmark dataset show that our model significantly outperforms the state-of-the-art methods, even in settings where contextual information is absent (e.g., in settings with no explicit aspect words or with explicit attention only to target words). Moreover, the attention mechanism with aspect contexts leads to faster convergence and better results.
117. **Cracking the Contextual Commonsense Code: Understanding Commonsense Reasoning Aptitude of Deep Contextual Representations**  
    Recent years have witnessed the strong performance gains of deep learning on many NLP tasks. However, it remains challenging for the same model to be able to provide superior performance when it is used in NLP tasks that require commonsense reasoning. One solution that is gaining increasing attention is to build a model that is able to learn semantic meanings of the objects using context as a privileged information. However, the existing approaches rarely have taken advantage of the context information related to objects' classes. The absence of contexts in these approaches will be detrimental to the models performance. In this paper, we analyze the performance of deep contextual representations in commonsense reasoning tasks, and design two new models, MCTS-C and FOCS-C, with the purpose of boosting the performance of existing models. Moreover, we show that the learned representations are more informative than the representations that are built from manually defined class information. Finally, we introduce a new benchmark called CSEMail (Context Semantic Embedding in Knowledge Base), which is constructed from several English and Spanish commonsense datasets.
118. **The merits of Universal Language Model Fine-tuning for Small Datasets -- a case with Dutch book reviews**  
    We present an application of domain randomisation for language modelling when the learning method is used for fine-tuning a language model trained with large datasets. Our main contribution is to introduce and evaluate a universal language model fine-tuning approach. This approaches allows any language model trained with large enough corpora to be reused, and thus enabling our use of a single model for all domains. This gives rise to a state-of-the-art in out-of-domain prediction of English to Dutch, with two other similar models being used for comparisons.
119. **Hierarchical Multi-Task Natural Language Understanding for Cross-domain Conversational AI: HERMIT NLU**  
    In this work, we describe HERMIT NLU, a high-performance end-to-end neural NLU system for conversational AI. HERMIT NLU, which runs on top of bidirectional encoder-decoder neural networks with attention mechanisms, is a multi-task NLU system which takes both categorical and relation-valued input into consideration for entity recognition. We introduce a novel hierarchical attention mechanism that allows HERMIT NLU to learn well-aligned triplets. Specifically, HERMIT NLU first merges the common base triplets by using a hierarchical self-attention mechanism, and then integrates the individual task-specific triplets by utilizing attention mechanisms. Experimental results show that HERMIT NLU outperforms a state-of-the-art multi-task system with a substantial margin, especially on multi-domain datasets.
120. **DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter**  
    BERT is the state-of-the-art language model and remains one of the largest publicly available languages. It is powerful in several important areas of natural language processing. It is highly efficient: it can be trained from scratch to build a language model from scratch in less than 10 minutes on a single machine and is an order of magnitude faster. BERT can model thousands of letters in a sentence, and it is also an effective memory component for many NLP tasks, including machine translation, query expansion, phrase selection and co-reference resolution. Yet, there is also evidence that BERT is less interpretable than previously hypothesized. We show that this is a result of the use of more restrictive word embeddings, not necessarily deeper representations. We propose DistilBERT, a distilled version of BERT. DistilBERT is 40% smaller, 1.3x faster and 6x faster than BERT, while maintaining or even improving the top-1 accuracy on several language modeling benchmarks.
121. **A CCG-based Compositional Semantics and Inference System for Comparatives**  
    We present a compositional semantics for preference relations that allows for interpretations in a compositional way and answers inference queries about preferences. The semantic theories we describe are based on the Metagrammar Coherence Minimization principle of Fenchel-Finkel. They are based on the notion of the intersection of a set of bundles, together with some compositional modal operators. We demonstrate the utility of our approach through an example in the context of relational sentiment analysis.
122. **Neural Word Decomposition Models for Abusive Language Detection**  
    Abusive language detection (ALDe) is a crucial component of cyberbullying prevention systems. Neural word decomposition models, which are based on the principles of word formation and word decomposition, are very effective for learning fine-grained discriminative features for ALDe. However, existing neural ALDe models do not perform well on datasets containing offensive language and low word counts. In this paper, we present a general-purpose neural ALDe model (Bi-Word ALDe) that enables neural models to learn powerful features with a high degree of generalization. The word decomposition is estimated on the last hidden layer of the network during training, in contrast to existing neural models that employ latent decomposition. We also study several activation functions for the Bi-Word ALDe model, and investigate the importance of the lateral connections between hidden layers. Our experimental results on the Amazon EmoEmotional Emotion Corpus show that the Bi-Word ALDe model consistently outperforms existing neural and non-neural baseline models and is more generalizable than previously proposed methods. In addition, we propose a new cross-genre ALDe model, which is based on cross-genre and cross-domain fusion methods. This model also consistently outperforms other cross-genre and cross-domain fusion methods.
123. **Linking artificial and human neural representations of language**  
    Abstract-nouns, such as'man' and 'woman', are essential properties for natural language understanding and building natural language interaction systems. However, their relation with conceptual relations such as'man and women' or'man and robot' is not well-understood, since the two relation types typically depend on each other. In this paper, we propose a neural network based architecture for the automatic automatic generation of a conceptual tree from a collection of abstract-nouns. The model is based on the Encoder-Decoder architecture, which exploits a hierarchical decomposition of the input context. We show that using the proposed model we can generate both abstract nouns and concepts with respect to abstract nouns, and that using the best abstract nouns are able to achieve the best performance. In addition, we show how the proposed model can automatically find connections between abstract nouns and conceptual relations, and how the proposed model can be used for connecting abstract nouns and syntactic relations.
124. **Types for Parallel Complexity in the Pi-calculus**  
    With the increasing volume of data we observe more and more success in processing a larger set of data in parallel, an attempt to model a more general computational task as a Boolean function whose output is in some interval. We model this problem by defining an interval-valued expression, called interval type, which allows for the computation of combinations of different computational tasks. These functions, which are called mixed type functions, allow us to model conditions on the input that make all the computation we need, or can avoid, parallel. In this paper we present a formalization of the problem of computing or avoiding various types of partition in parallel processing, a basic step towards developing general parallel computing models.
125. **Modeling Color Terminology Across Thousands of Languages**  
    Automatic color terminologies are important, but non-English resources are still scarce and disparate. This paper aims to facilitate resource discovery for color terminology across thousands of languages. We use a cross-lingual framework to address four common challenges in color terminology discovery: (1) language specific, (2) resource specific, (3) scaleable, and (4) applicable. Our framework identifies publicly available terminologies from the Clio corpus and uses their synset attributes to train a color term embedding and select a subset of the synset for training. Additionally, we use them to predict color terminologies for specific languages using a cross-lingual transfer learning approach. We find that including language specific resources improves the effectiveness of our color term embedding. Additionally, the transfer learning approach increases the scale of color term discovery. Our results show that our cross-lingual framework can potentially benefit over existing methods that leverage shared terminologies. We also provide some interesting insights on resource usage. Finally, we show that our cross-lingual transfer learning approach is applicable to English and Chinese, which are the two most difficult languages to achieve color terminology discovery.
126. **Data-Efficient Goal-Oriented Conversation with Dialogue Knowledge Transfer Networks**  
    Knowledge-based dialogue (KDB) has achieved great success in recent years. One of the most important challenges is to infer the intention of an autonomous agent, based on a limited amount of data. Previous methods of KDB based on feature extraction have been unsatisfactory in exploiting a large amount of conversation history without sacrificing the data efficiency. In this paper, we address the data-efficiency problem in KDB based on neural networks and propose a data-efficient dialogue model. Our model, named Dialogue Knowledge Transfer Network (DKTN), automatically learns the knowledge transfer parameters from an in-domain dataset with the goal of solving KDB in the target domain. We show that our model achieves up to 50% improvement in the data efficiency compared with the existing methods.
127. **Topic-aware Pointer-Generator Networks for Summarizing Spoken Conversations**  
    In order to provide flexible and abstract processing of multi-turn dialogues, various model-based summarization methods have been proposed in the past decade. One promising technique, called topic-aware approach, uses a summarizer to adaptively select content that is most informative from a corpus of naturally occurring dialogues. The summarizer often processes all utterances in a given corpus independently without considering their context and relying on external resources. To handle the long-tail corpus and tackle problems of sentence selection, it has been proposed to explicitly model the granularity of context in the model by setting the topics in the conversation prior to word selection. However, such model introduces the selection of words that are highly dependent on the context which cannot avoid the over-approximation effect. In this paper, we propose a new model, named pointer-generator network, by explicitly modeling the sentence selection in the model. By properly encoding the topic structure in the pointer network, it could reduce the over-approximation effect by including more frequent words in the sequence of words. In particular, we introduce topic-aware selection over the last word of each word in the selected sequence with a relevance vector. Then, we optimize the selection of words with a Pointer-generator based objective function, and gradually gather more context over the encoding process. The proposed framework could boost the performance of the summarizer and obtain a more flexible sentence selection. The experiments conducted on three large datasets with respect to each task demonstrate the effectiveness of the proposed model.
128. **Mapping (Dis-)Information Flow about the MH17 Plane Crash**  
    Translating a text to the spoken language of a target language using Machine Translation (MT) to improve the quality of the translation makes use of several resources, and thus influences the overall quality of the translation. However, due to the data restriction, resources for the translation are limited. The main contribution of this paper is to introduce the map of the MH17 plane crash to the MHRA, and apply the mapping to obtain the translation confidence maps for the target language. The use of machine translation, particularly MT, makes it possible to get an effective multi-source MT system that operates at 24/7 for only a small budget.
129. **Towards Understanding of Medical Randomized Controlled Trials by Conclusion Generation**  
    The Demographic Health Survey (DHS) has been employed to study the health effects of multivariate statistical models by generating the summary statistics. Although some studies have been presented, the literature lacks the ability to understand the results. In this study, we show that the conclusion of the randomized controlled trials can be predicted by the outcomes of the trial, with the aim of better understanding the findings. We explore the method of feature selection and the ranking algorithm and report the results of the experiments performed to predict the final score on the outcome.
130. **Modeling Confidence in Sequence-to-Sequence Models**  
    Neural machine translation (NMT) models have achieved state-of-the-art performance in machine translation (MT) tasks. However, the majority of NMT systems are susceptible to over-translation caused by the lack of training data, which limits the translation quality. The lack of translation quality is mainly caused by a neural mismatch between the input and target sentence statistics. This mismatch arises from the fact that the semantic relations between the words in a sentence are only recognized after translating the input. To overcome this problem, this paper proposes a confidence regularization scheme that forces the NMT system to produce aligned sentence statistics. In this work, we model the decoder state as a Gaussian distributed Gaussian process. The proposed normalization scheme can be applied to any NMT model and the output of the decoder can be treated as a sample from the input statistics. Consequently, the confidence in the NMT models can be naturally modeled by the consistency measures. We perform experiments on the CoNLL-2003 English-to-German task and the WMT14 English-to-French task to verify the proposed normalization scheme in NMT tasks. The proposed confidence regularization successfully improves NMT models by 10.7% on the CoNLL-2003 task and 3.3% on the WMT14 task.
131. **DialectGram: Detecting Dialectal Variation at Multiple Geographic Resolutions**  
    This paper addresses the task of identifying and clustering data from different domains where one language is spoken at multiple locales. A key challenge here is that most of the available data comes from the so-called Inland North America dataset, where word counts are higher than in other bilingual datasets. We introduce the DialectGram, a data set of various dialects for the Inland North America, where a large number of speakers of the same language may be observed. The goal is to detect the clustering effect of multiple speakers of the same language, where we expect the languages that share common languages to be spoken close to each other and languages with different languages to be distant. We also introduce a technique that allows us to infer the topological relations of a clustered distribution, based on which we make a novel call for improved clustering techniques. For this we model the language-of-language dependency between micro-topographic information, a rough marker for micro-topographic shifts and a robust language-information representation, and use statistical methods to infer a mapping between micro-topographic shifts and language-of-language changes. The result is an extensive evaluation of different approaches to different language-of-language mapping problems. The results show that a combination of clustering techniques and language-of-language mapping is the best way to deal with language change. We report some of the important effects of dialect clustering, which include the removal of language discontinuities between languages as well as the acquisition of new vocabulary.
132. **Tanbih: Get To Know What You Are Reading**  
    Finding interesting snippets in a text is a complex task for machine learning models. Recent works address this challenge by exploiting latent topic features based on bidirectional information between topics and sentences. The ability to distinguish informative topics from non-informative ones is crucial for a good summarization. In this paper, we propose an end-to-end neural network for text summarization that leverages such information. Our model, called Tanbih, obtains the topic knowledge from sentence pairs. We further use the knowledge to introduce two alternative latent topic representations to each topic-word pair. Using these representations, we train the topic models with word and sentence embedding vectors, respectively. We show that Tanbih outperforms previous methods in terms of generated meaningful summaries on three widely used data sets.
133. **Template-free Data-to-Text Generation of Finnish Sports News**  
    The number of available sports news articles is growing rapidly with the spread of the modern and ever-increasing internet. However, to be helpful and effective, these articles need to be well-structured and free of redundancy. Moreover, each sports event needs to be categorized accurately. Most news articles however are highly unstructured, and generate by a simple text extraction step with minimal filtering. In this work, we investigate a different data-to-text generation method based on deep neural networks that converts an existing sports news article to a natural language text, by treating it as a movie script. We have developed a novel technique called skeleton-based recurrent neural networks, with multiple image-based attention mechanisms to automatically extract a sequence of semantically meaningful content from the article. With an additional attention mechanism over the latent sentences, we trained a model on a small amount of manually-labeled sports news articles to infer the latent sentences corresponding to the same events. We evaluate the performance of the model with multiple text generation datasets in news analysis. We demonstrate that our model is able to generate reasonable stories for sports articles, which appear to be well-structured and free from redundancy.
134. **Multi-level Gated Recurrent Neural Network for Dialog Act Classification**  
    Dialog act classification is a common task in natural language processing. There are two significant challenges to this task: 1) dialog act labels are usually noisy and difficult to obtain. 2) the human understanding of a dialog act is highly subjective and consists of complex logical inferences. In this paper, we propose a multi-level Gated Recurrent Neural Network (G-RNN) framework to tackle these challenges. The core of our framework is a combination of bidirectional long short-term memory and long short-term memory. In order to effectively model temporal dependencies, we propose a multiple-level Gated Recurrent Unit (G-RU) module for each level of the Gated Recurrent Unit (GRU) to model both long and short-term dependencies. We also introduce two novel strategies to enhance the performance of our proposed models. Experiment results on two public datasets, that are annotated by experts, show that the proposed models perform significantly better than several strong baselines.
135. **Talk2Nav: Long-Range Vision-and-Language Navigation in Cities**  
    We present a new approach to building dialog systems that has the ability to automatically learn about long-range contexts, where a conversation takes place. Given an input image, our method generates a sequence of sentences describing an unknown environment such as a street. Our method, which we call Talk2Nav, extends previous method of deep learning to learn which words are likely to be next to a source phrase and to predict the resulting semantics. In this work we use a self-supervised deep language model to predict what would happen in a scene where a source phrase is in the input image. Our experiments in production cities show that the proposed method is effective for long-range vision-and-language navigation. In the evaluation in the small amount of training data, our method outperforms previous methods both on accuracy and efficiency. We also extend our method to simulate navigation scenarios and evaluate it on cityscapes. The results show that the proposed method can predict what would happen based on the input image.
136. **A Machine Learning Analysis of the Features in Deceptive and Credible News**  
    The prevalence of social media platforms has allowed the emergence of innovative methods for distribution of fake news (false information) and internet rumor. News editors usually adopt automated methods for categorizing news articles into credibility or credibility-distrustive categories. However, these algorithms are not always accurate, because each publication might have its own characteristic features that are relevant for the reader to know its reliability. In this study, we analyze the characteristics of fake news and related news through a machine learning-based approach. First, we study the characteristics of articles based on their respective structure, the article titles and the associated tags. We focus on the articles of two widely used internet news domains, The Mirror and Snopes. Then, we perform a statistical machine learning analysis based on word embeddings and N-grams to build predictive models of articles with regard to the implied reliability of both fake and credible news. We report the results obtained for the benchmark experiments for fake and credible news.
137. **Keyword Spotter Model for Crop Pest and Disease Monitoring from Community Radio Data**  
    In agricultural areas, agricultural yields depend on early and timely detection of crop pests and diseases. Crop monitoring technology can assist the field managers by predicting the onset and severity of crop diseases. To collect such data, the field manager deploys citizen-powered radio transceivers which record the farmers' speech (static) and generate the recordings of captured audio signal. In this paper, we present the in-depth review of keyword spotting technology for monitoring crops in rural communities. A comparison between agricultural yields predictions from a crop monitoring system and traditional broadcast sources is presented. The effect of using different models for crop monitoring and data acquisition and processing on yield predictions is also discussed. Additionally, the potential limitations and possible improvements of crop monitoring approaches are discussed. The major performance factors in the field are also examined for further analysis.
138. **On the Limits of Learning to Actively Learn Semantic Representations**  
    We show that active learning, a well-studied problem in the machine learning literature, can be seen as a natural unsupervised extension of standard active learning to latent variable models. We derive a novel theoretical connection between active learning and variational inference, and show that both active learning and variational inference can be interpreted as special cases of semisupervised learning. The aim of this work is to open a gap between active learning and existing approaches in latent variable models. We show that standard active learning can be translated into a standard dynamic programming formulation, and that the same dynamic programming algorithms can be applied to latent variable models, even though the latent variable models do not have explicit goal functions, unlike the case of the standard active learning. We propose a general approach that allows us to use different active learning algorithms for different latent variable models. Our algorithm, active reconstruction (AR), is a generalization of active learning to latent variable models. We demonstrate the scalability of AR on the task of inferring goal structures from free text.
139. **How Transformer Revitalizes Character-based Neural Machine Translation: An Investigation on Japanese-Vietnamese Translation Systems**  
    Transformer, an end-to-end model, has shown great improvements on many sequence-to-sequence (seq2seq) tasks including machine translation. However, there is still room for improvement in Chinese-Vietnamese Transformer, since the exact model is not revealed and no prior knowledge about the target language is given. This paper conducts experiments on Japanese-Vietnamese translation, and the benefits of using the Transformer model are studied. We investigate the different ways to pre-train the seq2seq model, including shuffling the initial steps and using incremental pre-training steps. We report some preliminary results of Transformer on Chinese-Vietnamese translation and discuss the problems of them.
140. **Joint Diacritization, Lemmatization, Normalization, and Fine-Grained Morphological Tagging**  
    The article aims to investigate the role of diacritization in named entity recognition. Diacritization is an automatic procedure that uses statistical information to re-classify an entity's content based on its distance to the model's last modified hypothesis (LMH) which is at the core of named entity recognition. This paper explores the use of diacritization in complex, heterogeneous, noisy, and rare named entity recognition tasks. In particular, we propose two different systems to jointly perform diacritization and lemmatization. The first system is based on smooth distributions, while the second uses unimodal distributions. In the absence of word alignments, we propose a two-step feature generation strategy which yields reliable features on entity pairs at the first step and disentangled representations at the second step. The evaluation of our proposed method is carried out on two different datasets (EWTN and Stanford COCO). Results suggest that jointly training de-registered (mismatched) features on entity pairs using unimodal distributions (without word alignments) offers a competitive performance compared to state-of-the-art systems.
141. **Text Level Graph Neural Network for Text Classification**  
    Graph Neural Network (GNN) can be regarded as an application of Graph Convolutional Neural Network (GCNN) in a data-driven way. The term graph neural network is used to emphasize the modularity of the two approaches and more importantly to discuss the relationship between GNN and GCNN. In this paper, we propose a novel framework, named text level graph neural network, to learn a unified graph representation with an additional domain randomization, capable of implicitly modeling text content for text classification. Specifically, a conditional graph generator is utilized to produce text-level representations from text graphs using a novel end-to-end conditional GNN module to learn representations by the output of the generator. Finally, the proposed framework is efficiently applied to text classification task with a simple local recurrent architecture on large-scale dataset. Experimental results demonstrate the superiority of the proposed approach on three standard text classification datasets and three real-world text mining tasks.
142. **Hate Speech in Pixels: Detection of Offensive Memes towards Automatic Moderation**  
    Understanding offensive online content plays a pivotal role in prevention and mitigation. This paper presents a large-scale crowdsourced dataset for analyzing offensive memes towards automatic moderation systems, and devising effective, efficient methods for detection. We analyze a comprehensive set of meme tags derived from a popular web service, MemeGenerator, and thousands of images on Reddit to construct a scalable, high-quality dataset. The dataset contains offensive content in the form of free text, images, and video clips that are annotated with salient features about the content. We also propose several actionable insights from the data for preventing and mitigating offensive content, such as: 1) learning from the initial offensive sub-populations and improving their discriminative power by leveraging strong baseline models and gradient boosted trees, 2) training a model to automatically classify a new offensive sub-population with just a few labeled examples, and 3) using image saliency and variation of the object classification label to devise a decision policy for detecting new offensive sub-populations.
143. **Design and Use of Loop-Transformation Pragmas**  
    Loop-transformation pragmas (i.e., functions that transform subcircuits into each other) are a useful tool for automated reasoning about loops. In this paper we propose a methodology to design and use loop-transformation pragmas. This methodology exploits existing operators for defining heterogeneous subcircuits and constructs a set of analogous operators for defining loop-transformation pragmas. We illustrate our methodology with two examples: for a relational network this methodology provides a completely automatic means to define loop-transformation pragmas and for a combinatorial cellular automaton the methodology provides the ability to generate loop-transformation pragmas with specific levels of complexity.
144. **Domain Differential Adaptation for Neural Machine Translation**  
    Domain adaptation for neural machine translation (NMT) has gained much attention recently. However, the target domain should be translated in a constrained environment which usually leads to word-level translation, which does not share the common characteristics between source and target domains. Although the domain discrepancy has been defined for a longer time, there is not much research work about the relationship between the domain discrepancy and NMT performance. In this paper, we introduce the domain discrepancy as the complementary factor between source and target domains and investigate the impact of different types of domain discrepancy. We also present a novel domain adaptation method based on the novel asymmetric domain discrepancy measure that can align different domains with a common counterpart while retaining the specific differences. Our empirical experiments on Chinese-English translation task show that our proposed method can efficiently remove the word-level domain discrepancy.
145. **Named Entity Recognition -- Is there a glass ceiling?**  
    In the modern era, many successful text recognizers are built on a pipeline approach, which encodes the text input into a vector representation and then backpropagates the features forward to predict the corresponding scores. On the other hand, recent studies have demonstrated that the quality of word vector representations does not translate well to the quality of the classifier performance. To examine the question whether there are distinct limits on the performance improvement achieved by the word embedding feature vector and the model structure itself, we perform feature ranking on multiple text and model architectures on three corpora. We discover that in many cases, the feature ranking technique leads to poor word embedding classifiers, which suggests that a simple feature selection procedure is not always adequate for evaluating the overall performance of the word vector representation.
146. **Why Attention? Analyzing and Remedying BiLSTM Deficiency in Modeling Cross-Context for NER**  
    BiLSTM is a widely used architecture for sequence-to-sequence learning, with state-of-the-art performance in the NER task. It generalizes to different unigram embeddings, and its latent state dimension can be estimated by maximizing a standard cross-entropy loss. It is also quite flexible, able to incorporate additional context information. In this work, we investigate how the encoder of a BiLSTM encoder-decoder network works when trained on word-level data, to obtain high performance in sequence-to-sequence NER tasks. We investigate what kinds of factors affect performance and observe low state-of-the-art performance in the popular task of cross-context sentence retrieval, but with a standard baseline that ignores any input context. We observe the surprising phenomenon that although adding LSTM encoder can improve performance, the resulting model is also slightly worse in the retrieval task. We found that the model is not effectively using the context information in the context-sensitive embeddings. We observe and analyze the impact of several factors, including the attention mechanism, on the performance of the model, and propose a new alternative approach to obtain improved performance. We experimentally validate our proposed approach on three data sets, achieving an improvement over the current state-of-the-art on the cross-context retrieval task.
147. **Measuring Sentences Similarity: A Survey**  
    Sentence similarity is a relatively recent field of NLP research and yet, is widely recognized as a useful first step to understanding the semantic properties of sentences. Over the past few years, it has developed into an active research area with promising results. However, there is still much room for further research in the area of measuring sentence similarity. In this survey, we present a systematic overview of different existing research in measuring sentence similarity in different domains and provide an in-depth comparison of metrics and evaluation procedures. Also, we summarize the main challenges in measuring sentence similarity and suggest promising directions for future work.
148. **Capturing Argument Interaction in Semantic Role Labeling with Capsule Networks**  
    An important task in building fine-grained discourse relations is to estimate the semantics of what follows in discourse. Asking questions about the role of each argument in a conversation is a challenging task, because it requires inferring the argument role of every discourse segment, including the incorrect ones. This work presents a novel method for capturing the argument interaction between two arguments in a conversational context. Our method builds on a previously developed capsule network to detect and match roles and conversations by constructing a user-driven, multi-label, multilingual, and multilingual semantic knowledge base. The resulting knowledge base is then used as a corpus for learning Capsule Network (CapsN) model parameters, which is then used to train a fully convolutional model on unlabeled sentences. Extensive experiments on two public discourse datasets (UCF-QNRF and WSJ) show that our method outperforms a recent state-of-the-art semantic role labelling model, which has been used as the baseline on both datasets. Finally, we provide empirical evidence that our proposed method also is able to capture discourse structure that is present in noisy conversational data.
149. **The Query Translation Landscape: a Survey**  
    Big data applications demand efficient answers to complex queries. This requires either solving (or optimizing) a problem involving several (possibly conflicting) evaluations of queries, or in some cases solving the corresponding optimization problem for multiple evaluation sources. In this article, we present a survey of the landscape of a general class of optimization problems called query translation optimization, and study their structure, convergence behavior, and optimization algorithms. We focus on a general class of adversarial queries, and investigate optimization algorithms that can solve them. We then present some extensions of these results, particularly in the context of multilayer feedforward neural networks, and review algorithms for global optimization. We survey a range of research topics in these areas, and argue that we need to be careful not to overinterpret results in the terms of specific types of neural network architectures. Finally, we present a general case-study where we show how different neural network components help to achieve different tasks and analyze how the design choices make a difference.
150. **Adversarial reconstruction for Multi-modal Machine Translation**  
    Multi-modal Machine Translation (MT) is a technique in machine learning which leverages both language and vision modalities to generate high quality translation outputs. While most of the existing works pay more attention on the prediction for one modality, we argue that both the source and target information should be leveraged in order to improve the quality of machine translation. In this paper, we propose a novel approach for adversarial training for Multi-modal MT. Specifically, given a pair of target and source sentence, we ask our model to learn the distribution of the two modalities in the output space, and thus to improve translation quality. Furthermore, by employing the adversarial training mechanism to improve multi-modal MT performance, the generator suffers from catastrophic forgetting, which can be alleviated by adding an extra classifier layer. Experiments on multiple datasets show that the proposed method improves the translation quality in multi-modal MT settings, compared with the baseline model.
151. **A Case Study on Combining ASR and Visual Features for Generating Instructional Video Captions**  
    One of the challenging tasks in computer vision and language processing is to learn the mapping between text and visual information. We explore the different ways in which the two modalities can complement each other when generating information describing a given domain. We argue that the results obtained with a language model can also be leveraged for generating visual captions. The visual channel is a richer, richer and more expressive source of data. Visual features such as color, object and text classification are well studied, and in the case of a language model, capturing long-range semantic information (global properties). However, in this paper, we take a more local view and discuss the choices of visual features. We evaluate various aspects of the different features on a Visual Genome dataset and discuss their potential usefulness in a video captioning task.
152. **SMArT: Training Shallow Memory-aware Transformers for Robotic Explainability**  
    The capacity of any model to explain itself can lead to improper or improper interpretation. Imitation learning models, which are typically based on shallow neural networks, are commonly explained by showing how they extract features from the state vector and interpret them in a way that preserves important information for solving the task at hand. However, this process is slow and memory intensive, especially on tasks that require a detailed, multi-step explanation, e.g., video analysis. Furthermore, existing implementations of these models are often supervised, which does not accommodate the limitations of models that need to explain themselves. To this end, we present SMArT, a novel approach to train agents that can rapidly and efficiently extract and elaborate explanations from high-dimensional sensory input. To train such a model, we use a recursive algorithm that learns to synthesize features and explain them with appropriate length, and apply the learned method to the state vector to learn an information-theoretic representation for the state space. In order to illustrate its capabilities and how it may be useful for other tasks, we use the state vectors of a simulated annealing robot to show how to detect pendulum contacts and start a chain reaction for a trampoline swing-up, and also how to infer the state vector of the robot's hardwired video sensor from the state vector of its low-level video stream.
153. **Rekall: Specifying Video Events using Compositions of Spatiotemporal Labels**  
    In this paper, we consider the problem of inferring a video descriptor from a set of keyframes that represent a (possibly long) sequence of events. For example, if the video is of a street scene captured by a drone, this task will produce a sequence of stereo images from left to right followed by an event. Such events are temporally similar, but might not be captured at the same moment, for example due to motion blur. In this paper, we propose a method, called Rekall, that uses a semantic model to describe the spatial distribution of the events and a neural network that predicts the temporal context of each event. We train the model using a large set of synthetic, noisy data with a multi-label loss function and propose a data augmentation method that improves the consistency between the events' predictions and the actual events in the video. The model is trained on the coined Global Event Dataset (GED), which is a collection of videos with event labels that can be classified using standard metrics. Experiments using Rekall on the GED show a significant improvement over the state-of-the-art on three datasets with more than 200 events. Furthermore, we show how Rekall can be used to enhance synthetic video datasets by a factor of more than 200.
154. **Improving Neural Machine Translation Robustness via Data Augmentation: Beyond Back Translation**  
    The recent success of Neural Machine Translation (NMT) is based on a powerful encoder-decoder architecture. However, it is still difficult to train NMT models robustly to adversarial examples. In this paper, we propose a novel data augmentation method to improve robustness of NMT models for adversarial examples. More specifically, we dynamically create a large-scale and diverse set of adversarial examples with different uncertainty in its source and target sentences. To avoid catastrophic failure in NMT robustness, we further propose a model adaptation scheme, which embeds learned confidence of source and target sentences into model transformations. Experimental results on three benchmark datasets show that our proposed data augmentation approach improves robustness of NMT models, especially for adversarial examples generated from target and source sentences with different uncertainty. In addition, experiments on large-scale neural machine translation problem demonstrate the effectiveness of the proposed data augmentation method.
155. **MaskParse@Deskin at SemEval-2019 Task 1: Cross-lingual UCCA Semantic Parsing using Recursive Masked Sequence Tagging**  
    We present our work in Phase 2 of SemEval-2019 shared task UCCA-Semantic Parsing, where the UCCA semantic parsing task is composed of 7 subtasks. We participated in the shared task UCCA-Semantic Parse (S2P) in English, Spanish and French. This shared task was organised as part of the SemEval 2019: Language and Vision Shared Task. The UCCA semantic parsing task asks us to predict a sequence of abstract concepts (e.g. "bed" in "phone"), based on a textual description of a natural scene (e.g. "a house in a hill"), where there are many unseen concepts and the expected sequence does not contain all the concepts. The UCCA semantic parsing task is a challenging task as it requires the ability to detect complex structures (e.g. wall and ceiling) and is inherently multi-modal, i.e. it requires an integration of linguistic, visual and information from other modalities. We introduce the Recursive Masked Sequence Tagging model, which takes as input an example of a word combination that occurs frequently in a given context, and annotates the tokens in the given context with their associated labels. The experimental results show that our model is able to achieve promising performance compared to previous approaches on UCCA semantic parsing.
156. **Controllable Sentence Simplification**  
    Automatically creating abstractive summaries of texts is a fundamental research problem for Natural Language Processing (NLP). We propose a mechanism for automatic sentence simplification, by modeling, instead of solving, the challenging scenario of learning from an insufficient dataset. We identify five training pairs of sentences in the abstractive domain, and evaluate them individually and in a joint optimization framework to learn the sentence simplification model. For the unified framework, we introduce a new distillation strategy to decompose the problem into more involved subtasks, which could be solved by a centralized neural network or a distributed ensemble. The framework is validated with machine-generated data. Results show that our approach improves upon the baseline system by 7-10% in both English-language and English-universal domains.
157. **BERT for Evidence Retrieval and Claim Verification**  
    In order to offer a new level of model flexibility, word embeddings have been shown to be one of the most promising candidate models for the retrieval and claim verification of controversial claims. They give more expressive power than word embeddings do, and hence are more useful to rank the claims, and thus will be more suitable for use in active learning, where there is a need to rank multiple alternatives and define a consensus ranking. A recent paper gave evidence that BERT, by itself, is a promising candidate model for evidence retrieval and claim verification. In this paper we summarize the experiment presented in that paper, reporting the different methods proposed and evaluating their results. Moreover, we evaluate the usefulness of the different versions of BERT and show that BERT with few pretrained words produces comparable results to other state-of-the-art models while achieving much better generalization to unseen cases.
158. **Adapting a FrameNet Semantic Parser for Spoken Language Understanding Using Adversarial Learning**  
    Textual context plays a critical role in both automatic speech recognition (ASR) and spoken language understanding (SLU) models. To this end, many approaches have been developed to extract multiple source frames from a speech audio stream, and align them to form a semantic parse. However, standard statistical models, such as a neural network (NN), usually suffer from high computation complexity. A recent line of work to improve NN learning algorithms used adversarial learning to boost the performance of learned neural networks. However, it is still an open question how to exploit adversarial learning in spoken language understanding, as only 1/3 of the semantic parses are available to evaluate. Therefore, in this work we propose an alternate approach to training a frame network with adversarial learning. We extract only the spoken labels for a pseudo-label (NND) and employ adversarial learning to fit the data distribution. In our experiments we evaluate two variants of our proposed method: i) normal NN training, and ii) adversarial training. The results show that our proposed approach is able to adapt a frame network for SLU at the same cost as trained NN, and outperforms competing approaches.
159. **Towards De-identification of Legal Texts**  
    There is a growing need for systems to help law enforcement agencies manage a law enforcement database that stores thousands of pages of legal documents. Such a database can be useful for law enforcement to investigate unsolved cases, locate victims of crime, examine witness accounts of crime, conduct investigation and traffic analysis of suspected drug use, abduction and child abduction cases. The growing number of cases poses a huge burden to law enforcement. To overcome the problem, various solutions have been proposed to tackle this problem by (a) classifying a document into a known category or category of documents (b) generating a vector or hash code to represent a document and (c) storing the documents in a different database. Several approaches have been proposed in the literature to deal with these problems. In this paper, we make a case for a study to seek for the mechanism which allows to eliminate the use of these solutions. We propose a novel algorithm, which is able to remove the data vectors which are used in several popular hash functions. We also discuss the impact of the proposed algorithms on the accuracy of the remaining documents. We applied our algorithm to the National Crime Records Information System and we compare the results with those obtained by using a similar algorithm from a previous work.
160. **HuggingFace's Transformers: State-of-the-art Natural Language Processing**  
    We demonstrate the efficacy of running convolutional neural network on HuggingFace using deep learning models for multi-instance classification. The HuginFace system was inspired by the Hugin model of Bartok, Jayasingha, andppa [2] [3], which is a bridge between image-level and word-level representation of an image. We created a network using Hugin's WordNet and features from Facial Expression Recognition(FER) Competition [4] data set. While the large dataset available with the competition did not allow us to run as deep as what currently in production systems, we were able to achieve a mean Error Rate (MER) of 13.2% and a MAP of 10.5% using a single model in the test set. We also show that a few very simple decisions could significantly reduce this cost, such as reducing the input size from 1024 x 1024 to 496 x 496 [5] and adding a small mask for each input word instead of the standard weighting system. Finally, we use a novel technique of writing up code using the CNN architecture we built and show it on a prototype.
161. **Knowledge Distillation from Internal Representations**  
    Learning representations that can capture both high-level and low-level aspects of data, is a key aspect of representation learning. Current deep learning methods typically focus on one type of such structure, and in particular, linear projection in Euclidean spaces. Despite promising results, there is currently a lack of systematic overviews of different sources of information for representations, and different sources of loss functions and regularization methods, which are in turn potentially important for representations that can capture both high-level and low-level aspects. In this paper, we provide a comprehensive review of methods for representing data, including methods that transform high-level features into low-level features, and data-specific methods that select features to capture one aspect. We study how these approaches perform, and discuss their underlying assumptions. We first review approaches for linear projection for representing data, and review the properties that support the representation. We then review methods for transforming features, and show that transformations generally take into account different aspects of the data. We also show how previous work addressing transformations can be applied to other methods. We discuss a few specific applications, including data translation, representation transfer, and partial domain adaptation, and highlight their challenges.
162. **Do People Prefer "Natural" code?**  
    Computer code is intuitively associated with several factors. First, it is visually similar to images: its pixel composition is similar to that of an image. Second, code is concise: a single line code is also short and concise. Lastly, code's association with information such as inheritance and typedefs is intuitively accepted by humans. Here, we systematically collected and analyzed over 2000 tags associated with programs. We compared tags associated with plain, simple, and complex programming languages to more abstract programming languages: defined types, strongly typed, and typedlambda. We evaluated various aspects of human judgment, such as the sentiment and the order in which tags are selected, by having humans review tags according to several criteria. We also show that the highest-rated tags are consistent with the classical use of natural languages, namely according to the authors and the semantic meanings of the particular language.
163. **The Daunting Task of Real-World Textual Style Transfer Auto-Evaluation**  
    The generation of natural language content is at the core of many applications in real-world scenarios, e.g. automated blog writing, semantic role labeling, and automatic image annotation. The availability of large-scale labeled datasets, however, has been limited in comparison to recent years and provides no guarantees on the robustness of the data in terms of quality and diversity. In this paper, we investigate the challenging task of real-world textual style transfer where the training and evaluation sets of different tasks (e.g. text reconstruction and natural language generation) share a common theme. We propose a framework based on Adversarial Networks (AN) and a classifier for comparing two datasets (one labeled and the other unlabeled), and evaluate the performance of several deep learning models for the semantic role tagging task (Stacked Autoencoder, Bidirectional Gated Recurrent Unit and Wasserstein GANs) and for the text-to-image generation task (self-attentive auto-encoders). Our results suggest that neural models are the most effective for both tasks.
164. **One-To-Many Multilingual End-to-end Speech Translation**  
    This paper explores end-to-end speech translation, i.e. the task of translating speech utterances into their respective native languages from source language, without the need for parallel data and transcription. A generalized bottom-up approach, named video-to-text prediction model, is developed for the task. The deep neural network is used to capture the temporal correlation in videos and sentences, which help the model achieve the superior performance. The proposed model can not only translate within a single language pair, but can also translate multiple languages in parallel. Experimental results on multiple languages, like English-Italian, English-Japanese and English-Chinese, demonstrate that the proposed method achieves better translation quality than the baseline models. Furthermore, there is no need for parallel data and transcription in the proposed approach.
165. **An Interactive Machine Translation Framework for Modernizing Historical Documents**  
    This paper presents an interactive machine translation framework that can produce modern translations of historical documents. A statistical process is used to take advantage of the corpora of the old and the new translations, using simple structural features extracted from the documents. Experimental results indicate that the approach presented here provides better translation quality than existing works on the same historical documents, but with a faster processing time.
166. **In Search for Linear Relations in Sentence Embedding Spaces**  
    Sentence embedding has been proven to be effective in several NLP tasks. However, most existing word-level attention models are binary, which have limited capacity to model long-distance dependencies (i.e., discourse relations). In this paper, we propose a novel word-level attention model with a hypertree structure, which can model the long-distance dependencies of sentences in a common space. This model is referred to as Parallel Linear Dependency Network (PLDN). Our model introduces an additional skip-layer node at the root of the tree, which is a proper decoder node, to capture the long-distance dependencies, and a local-layer node at the leaf node to attend the most salient parts of the sentence at each node. Moreover, we extend the PLDN model to the sentence embedding space by introducing the factorized attention layer, which can predict latent representations of words based on a specific dependency relationship. We further provide a dataset, with human-annotated sentence relation labels, to evaluate our model and compare its performance with existing state-of-the-art models. Experiments on a wide variety of real-world benchmarks demonstrate the superiority of our model.
167. **Look before you Hop: Conversational Question Answering over Knowledge Graphs Using Judicious Context Expansion**  
    Conversational Question Answering (CQA) typically involves a domain expert creating the queries from a domain corpus and then selecting relevant answers from the retrieved answers. The constructed domain ontologies typically do not allow one to query for a specific answer from the set of retrieved results. A natural way to address this problem is to incorporate the retrieved results into the knowledge graph of the domain expert during the CQA model training phase. However, existing CQA models either do not leverage the retrieved results or rely on heuristic approaches to integrate the retrieved results into the knowledge graph. In this paper, we propose a method to extend existing CQA models to incorporate retrieved results. Specifically, we propose the Context Aware Knowledge Extraction model that is capable of inferring the context information for the retrieved results. Extensive experiments are conducted on two benchmark datasets. The empirical results demonstrate that our proposed approach outperforms the current state-of-the-art approaches on both datasets. We also conduct user studies to explore the impact of our proposed method.
168. **CONAN -- COunter NArratives through Nichesourcing: a Multilingual Dataset of Responses to Fight Online Hate Speech**  
    This paper describes the emergence of a multi-lingual crowdsourcing pipeline for analyzing large volumes of hostile text, as a novel and scalable platform for tracing and amplifying cultural and political violence, and as a means to mitigate its potential impact. Our work bridges widely disparate research communities with the context of a newly activated alt-right website called CONAN, and manifests new understandings of far right narratives, strategies, and messages. The production of this large, diverse and multilingual corpus -- more than 13M long Reddit posts -- coincides with an increased focus in this field on adapting social media platforms and analyses to the specificity of multiple languages and sources of sentiment. The proposed model and platform now runs in 26 different languages and employs over 14 thousand annotators with more than 86 million saved professional ratings.
169. **Linguistically Informed Relation Extraction and Neural Architectures for Nested Named Entity Recognition in BioNLP-OST 2019**  
    We report results on the NER task in the BioNLP-OST 2019 Shared Task, which assessed the performance of relation extraction systems for detecting and identifying named entities. We choose three ranked state-of-the-art approaches for relation extraction: Ensemble Weakly Supervised Meta-Model, Merge Topic Models with Neural Word Embeddings, and Deep Metric Learning. We analyze them on a newly generated bio-nationally annotated dataset consisting of WikiContentID reviews for Amazon products. We observe that the neural-word embeddings models yield the best overall result (top 5) with a perplexity of 0.999, using the supervised setting, while our recently proposed framework (weakly-supervised neural meta-model) yields a perplexity of 0.997. The results indicate that models equipped with a syntactic parsing and higher-order topic modeling component (developed in our company) perform better than simpler neural models, a principle based on assuming language models to be quite good.
170. **Generating Highly Relevant Questions**  
    In this paper, we explore the problem of generating high quality, high relevance questions using image captioning systems. In image captioning, the task is to generate captions that accurately describe a given image. A question is a word that contains a question sentence and the image (or video) to describe. We present a set of novel evaluation metrics for this task that cover a number of aspects including readability, specificity, coherence, relevance, co-reference and partiality. We also present several strategies for selecting images to generate questions. We test these strategies in the context of two recently released image captioning models, named MonaCaption and MaskRNN. We test the two models on the standard publicly available ImageNet captioning dataset and find that adding question generation to existing models yields considerable gains in performance. We also experiment with two different scoring strategies for question generation which demonstrate that combining multiple metrics can yield an average increase in performance of up to 10%.
171. **Riposte! A Large Corpus of Counter-Arguments**  
    The level of sophistication of existing argumentation theory is difficult to describe with quantitative terms. This paper presents a new dataset of 100k counter-arguments, consisting of 40k texts in total and more than 10K arguments with hyperlinks (this is a substantial increase over previous datasets) and an additional 2.6k arguments with text snippets. The data, designed and curated by the propositional framework, can be used for statistical machine learning tasks as well as for other purposes, including text summarization, machine learning for dialogue systems, and a meta-learning approach.
172. **Perturbation Sensitivity Analysis to Detect Unintended Model Biases**  
    We study the sensitivity of a learned classification model to perturbations in the training data. Specifically, we consider the scenario where an adversary can inject a specific number of small perturbations into the inputs of a dataset. We give an algorithm for assessing the degree of sensitivity and when the problem is solvable. We demonstrate empirically that our algorithm is significantly more effective than existing approaches that assess the degree of adversarial behavior for individual perturbations.
173. **BHAAV- A Text Corpus for Emotion Analysis from Hindi Stories**  
    With the growth in the Internet and people's usage of digital media, an increasing number of articles have been published daily in various domains. These have not been accompanied by indices for public emotions in the texts. The ease with which digital media is published and spread like wildfire makes it possible to acquire a massive amount of information that may go unnoticed. A novel approach is being proposed here to classify emotions into different subcategories. A text corpus is built to categorize the stories into different categories. This corpus was collected from Hindi News sites and two separate classifiers are proposed for the purpose of the project. The results obtained show that both the classifiers are capable of accurately determining the emotions of texts.
174. **Exploring Hate Speech Detection in Multimodal Publications**  
    We explore the automatic detection of hate speech in multimodal media. We consider articles in four different platforms: social media, online newspapers, political blogs, and online magazines. We evaluate five different state-of-the-art approaches and compare them in terms of accuracy and cost. Our experiments show that articles in social media (78%), online newspapers (81%) and online magazines (80%) are not sensitive to hate speech.
175. **Novel Applications of Factored Neural Machine Translation**  
    The goal of factored neural machine translation (FNMT) is to obtain significant improvements over the monolingual baseline by translating both the source and target sentences to a factored representation that makes it easier to optimize for translation performance. This paper introduces the factored sentence translation problem, and analyses its well-posedness as a type of factored NMT that allows simultaneous training of both source and target sentences. We illustrate the factored approach on the task of article summarization in language-independent newswire applications. In experiments, we show that this approach achieves substantially better performance than a strong baseline model.
176. **Assessing the Efficacy of Clinical Sentiment Analysis and Topic Extraction in Psychiatric Readmission Risk Prediction**  
    Patient discharge is a standard method for studying the effects of mental health conditions. It is an effective way for identifying patients with psychiatric conditions, such as depression, anxiety, and bipolar disorder. However, it is a time-consuming and tedious process for health care organizations to conduct. This study aims to investigate the effectiveness of sentiment analysis and topic extraction in identifying high-risk patients. A case study of clinical readmission risk prediction in the Department of Veterans Affairs (VA) hospital system was created. The approach involved the following steps: (i) using words, phrases and descriptions in mental health literature in the abstracts, in clinical notes, and in job interviews for various diagnoses, (ii) designing classification algorithms using a topic-based topic model, (iii) creating factor graphs by clustering across the data and analyzing them in terms of clinical attributes (measured by a bag-of-words model), and (iv) implementing a supervised learning model using autoencoders to predict whether a patient will be readmitted to the VA hospital. For this study, a diagnosis level predictive model based on WordNet was constructed to predict whether a patient will be readmitted to the VA hospital. The application of an SVM classifier using the F-measure score to predict readmission risk was also performed. The analysis demonstrated that supervised topic models can effectively predict patients' future readmission. This study demonstrated that topic models can provide important diagnostic and prognostic information on mental health conditions.
177. **Efficient Semi-Supervised Learning for Natural Language Understanding by Optimizing Diversity**  
    In this paper, we address the problem of extracting useful semantic information from a large amount of unlabeled text. It is widely recognized that humans are able to gain a better understanding of natural language if they can easily access diverse sources of information. However, the existing state-of-the-art semi-supervised methods often face a challenge in finding the best unsupervised method for a specific task. In this work, we propose a novel approach to improve the performance of the unsupervised models on the semantic relatedness task, by incorporating distributional information for predicting the future distribution of each unsupervised instance and by simultaneously predicting the distribution of the unlabeled data. We achieve this by combining neural architectures and optimization techniques to obtain efficient representations for each text and exploiting this to design unsupervised classifiers that extract meaningful semantic information. We evaluate our approach on several well-known datasets, and show that it significantly outperforms other state-of-the-art models for the semantic relatedness task.
178. **Multi-label Categorization of Accounts of Sexism using a Neural Framework**  
    The world of online social media is brimming with hate speech, sexism and racism and an increasing proportion of these practices is taking place behind closed doors. A crucial aspect of online social media is its impact on women. While existing work focuses on statistical gender bias and fails to effectively capture attitudes of men and women toward these aspects, we present a new methodology for classifying hate speech, sexism and racism from online social media. The key contribution of our approach is to utilize a neural language modeling model to solve a multi-label categorization task and subsequently aggregate gender and racial hate speech labels for further analysis. Our empirical study of 1,000,000 comments from Twitter shows that a neural language modeling model can successfully detect and identify the prevalence of sexist language on Twitter in order to discriminate against a group of accounts and classify it in a less discriminatory manner.
179. **Controllable Sentence Simplification: Employing Syntactic and Lexical Constraints**  
    We introduce a sentence simplification method that allows the user to specify various heuristics, such as sentencesplits and consecutive simplification, which determine which parts of the input are the input's input. Our approach is implemented in a model that implements a reduced-order parser that in turn efficiently produces an approximate simplification. We perform experiments on four standard corpora, including newswire and academic text, for both automatic and human evaluation, showing that our method is highly effective, especially for sentences that are highly repetitive and not grammatically correct. The simplicity and flexibility of the method make it suitable for novel fields and domains, such as medical transcription and editing Wikipedia articles.
180. **Multilingual Question Answering from Formatted Text applied to Conversational Agents**  
    Multilingual question answering (QA) is a classical task in natural language processing. As a consequence, many QA systems rely on fixed-size sentence representation that limits scalability and flexibility in question answering. In this paper we develop an in-memory representation for QA that can handle large sentences, by fusing a fixed number of sub-sequences and employing a heuristic. Our system, which is named Spark-QA, makes use of a dynamic neural seq2seq architecture that exploits contextual dependency to capture temporal dependencies in the extracted sub-sequences. To solve the sparsity issue due to short sentences, we propose an attention mechanism that operates on the sentences at the level of words, where a single word can be assigned to all sub-sequences. Furthermore, to adapt to queries from unknown users, we propose a language-invariant metric to identify unknown queries and a question composition model that considers the semantics of each word in the question. Extensive experiments on two benchmark datasets demonstrate the effectiveness of our approach compared with the state-of-the-art multilingual QA approaches.
181. **RC-QED: Evaluating Natural Language Derivations in Multi-Hop Reading Comprehension**  
    The task of multi-hop reading comprehension focuses on understanding a series of clauses or sentences using both knowledge and language processing approaches. Over the years, various authors have proposed various neural models to handle this problem. However, their evaluation of these models is severely hampered by their high computational complexity and the fact that multi-hop reading comprehension is a challenging task. In this paper, we address these limitations and present a benchmark for evaluating natural language derivations in this task. Our evaluation benchmark includes 26,371 multi-hop sentences and receives different evaluation metrics. We study three neural models, namely, bidirectional recurrent neural network (BRNN), attention-based neural network (ABNN), and tag-based neural network (TNN), on this benchmark. We find that the models do not perform equally and are therefore evaluated separately. Furthermore, we propose a novel concept called RC-QED, a new metric for multi-hop reading comprehension evaluation. We show that RC-QED can be used to choose the best model for the task, which is particularly useful in light of the inherent small sample size of the training data.
182. **Automatic Quality Estimation for Natural Language Generation: Ranting (Jointly Rating and Ranking)**  
    With the increasing popularity of spoken dialog in various applications, there has been increasing interest in automatic generation of natural language. In this paper, we present an evaluation framework for automatic quality estimation for natural language generation. The evaluation framework consists of three steps: (1) learning a vector space representation of the utterances by extracting vocabulary words, (2) training a classifier for predicting the quality score for the utterances, and (3) evaluating the quality score of the generated utterances. We performed experiments on the Chinese restaurant domain and the English-French service domain. Our experiments indicate that a powerful method is introduced in the first step for ranking quality scores automatically. The second step of our framework can be considered as the complement to the first step and can improve the quality scores of generated utterances. We conducted experiments on the well-known MALICON corpus, and we found out that our system can give reasonable results for both the English-French service domain and Chinese restaurant domain.
183. **Zero-shot Dependency Parsing with Pre-trained Multilingual Sentence Representations**  
    Zero-shot dependency parsing aims to distinguish text segments in different languages based on knowledge of the sentence structure. Recent advances in zero-shot dependency parsing involve leveraging monolingual embeddings to learn embeddings for the marginal distributions of languages, i.e. given a sentence, the marginal distribution is provided as a cross-lingual vector. In this work, we show that even if these marginal distributions of languages are obtained from the same source, we can extract new information through the shared space embeddings between different languages. We call this new source language "Source-Target Cross-lingual Embedding". In particular, our method learns to exploit shared representation space between a target language and its source language to optimize the model for zero-shot dependency parsing. To alleviate the low-resource scenario and build a consistent global parser, we propose a multi-stage approach and use an automatic zero-shot dependency parser by integrating the pre-trained sentence representations with the embedding of source languages. Our extensive experiments show that this new source language improves zero-shot dependency parsing results compared to previous zero-shot and multi-task parser based on general-purpose sentence embedding.
184. **Acquisition of Inflectional Morphology in Artificial Neural Networks With Prior Knowledge**  
    In this paper, we investigate whether prior knowledge about what to learn about inflectional morphology improves the end-to-end learning performance of an LSTM-based neural model. Our work shows that multi-task learning with syntactic supervision can be beneficial to learning inflectional morphology. Our results show that prior morphological knowledge can significantly improve LSTM performance without affecting accuracy on inflected morphological properties.
185. **Neural Generation for Czech: Data and Baselines**  
    We introduce the Czech Language Generative Adversarial Network (CLGAN), a model which learns disentangled representations for both meaning and sentence structures. Unlike prior work, the CLGAN is fully unsupervised and unsupervised adversarial: it learns to generate sentences without explicit supervision in the form of source sentences, and simultaneously learns disentangled representations of meaning and sentence structures. We train CLGAN on a large Czech corpus that contains phrase-based and sentence-based labels. Our experimental results demonstrate that CLGAN successfully disentangles both meaning representations and sentence structures, and produces more abstract, higher-level language.
186. **How Does Language Influence Documentation Workflow? Unsupervised Word Discovery Using Translations in Multiple Languages**  
    In many multilingual document analysis applications, a human-in-the-loop process is needed for construction of task-oriented reference documents. We propose a novel deep learning model for automatic data representation and comparison. The model learns the latent representations from an untranslated collection of documents, without any supervision. Through experimental results we show that the model achieves a superior performance over strong baselines. In addition, the model learns the language-invariant latent representations. We further show that the model performs in accordance with the specification of a human-in-the-loop process that facilitates cross-lingual comparison.
187. **Multi-Task Learning for Conversational Question Answering over a Large-Scale Knowledge Base**  
    Conversational question answering (CQA) over a knowledge base (KB) is a fundamental problem in artificial intelligence and natural language processing. Many state-of-the-art models based on multi-task learning (MTL) suffer from data sparsity and inefficient inference. Motivated by this observation, we propose a novel MTL model for CQA. Our model combines information from both conversational discourse and the knowledge base using a joint vector of word embeddings and embeddings from knowledge base. As a result, our model can effectively incorporate information from the dialogue history and the current topic in an efficient manner. Furthermore, by utilizing an attention mechanism, our model is able to make intelligent use of available data to learn more meaningful vectors. Experiments on two public datasets show that our model significantly outperforms the current state-of-the-art CQA models.
188. **Keyphrase Generation: A Multi-Aspect Survey**  
    Keyphrase generation (KG) is a fundamental task in NLP, involving selecting a few representative sentences with the most important phrases (e.g. 'the weapon has two unique faces'). Various deep learning models have been proposed for this task, and a number of them have reached state-of-the-art performance. In this work, we review the different aspects of KG and analyse their connection to other NLP tasks, summarizing the current state-of-the-art models. We discuss the limitations and challenges of these models, summarizing their strengths and weaknesses. Finally, we provide a detailed comparison of the various KG models, based on both objective and subjective results.
189. **Group, Extract and Aggregate: Summarizing a Large Amount of Finance News for Forex Movement Prediction**  
    This paper aims to perform a large-scale analysis of financial news (news articles), describing the attributes of news articles and classifying news articles into groups. In particular, the subject of our study is to predict the movements in stock prices of a group of banks, banks with different trade activity, or large financial institutions. To study this topic, we have built a system to extract news articles with a pre-defined class. This class has four features: share price, political, economic and time. In order to aggregate the extracted news articles into a summary of a group, we have developed a few types of aggregation mechanisms. Then, we have evaluated the results obtained with the aggregated summary by finding out the active and influential groups, and the correlation coefficient among different groups. The result shows that the aggregated summary is more influential than a simple summary, in terms of accuracy, by two orders of magnitude.
190. **Emotion Recognition in Conversations with Transfer Learning from Generative Conversation Modeling**  
    Conversation is an important component of social interactions. But for real-world applications such as customer service, public speaking and fashion recommendation, there are practical challenges in building and running a good conversation model. This is because this kind of application poses new challenges such as: (1) a large number of speakers with many emotions in conversations, (2) highly conversational situations. In this paper, we tackle the challenge of emotion recognition in conversational scenarios, aiming at building conversation-adaptive models with the desired characteristics. This can be considered as a multi-task learning problem, in which both emojis recognition and conversations generation are treated as well as complementary tasks. We describe our solution and benchmark the performance of both in various combinations of multi-task learning and transfer learning. We provide experimental results using two different sets of datasets, and also an analysis of the individual components involved. We demonstrate that our model outperforms baseline methods and other state-of-the-art methods on both of the two datasets.
191. **Statically Detecting Vulnerabilities by Processing Programming Languages as Natural Languages**  
    This paper presents a system for automatically detecting vulnerabilities in computer programs. The proposed system extends existing system for vulnerability detection by incorporating the notion of efficient sub-language parsing. This extension enables the ability to process programming languages as natural languages. The proposed system handles information retrieval and execution traces as non-redundant unstructured documents and reuses intermediate results to generate an accurate list of vulnerabilities in the list. The approach is evaluated using vulnerabilities of popular embedded and off-the-shelf computer programs. The objective function of the proposed system is to identify the sources of possible vulnerabilities and to mine the vulnerabilities in the source code files. Each source code file is processed through a regular expression whose termination produces the list of vulnerabilities. The efficacy of the proposed system is shown in the way it has been able to find high-risk vulnerabilities of three widely used embedded programs: Siri, Bitbucket Web Server and Responsible disclosure. Results show the exploitation efficiency of the proposed system.
192. **Context-Gated Convolution**  
    Context-modulated convolution is a powerful tool for coarse-grained object detection and classification, by using both fixed (e.g. pixel-wise convolutions) and contextual (e.g. moving window-wise convolutions) constraints, which have not been fully explored. We propose Context-Gated Convolution (CGC), which encodes object category context with a separate gating layer, during CNN training. CGC maintains a multi-level context feature representation, referred to as SAC-pooled context feature, at different spatial locations of an object. Specifically, context-modulated convolutions are used to capture spatial context information of object and, therefore, fine-grained object class detection and classification is conducted on the SAC-pooled context feature. When bounding box predictions are predicted with a specific context region (e.g. for instance, background region), object's identity is transferred to this SAC-pooled context feature. Therefore, the same image class can be reliably assigned with multiple instances in different contexts simultaneously. Extensive experiments on two large-scale object detection datasets demonstrate that our CGC model achieves a new state-of-the-art performance (5.4\% absolute gain over the previous best).
193. **From the Paft to the Fiiture: a Fully Automatic NMT and Word Embeddings Method for OCR Post-Correction**  
    Pre-trained word embedding models are widely used in language modelling and neural machine translation. In this paper, we propose an end-to-end trainable pre-processing method for OCR post-correction. Our method first generates the supervision signal from the beginning and then applies an artificial neural network to a unigram input and combines the modeled features with the noisy pre-correction. We tested the proposed method on three different language pairs: Italian-English, German-English and French-English. The result shows that the proposed method improves the score of OCR post-correction by 16%, compared to the conventional methods.
194. **SmokEng: Towards Fine-grained Classification of Tobacco-related Social Media Text**  
    Social media text represents a new source for collecting qualitative scientific research on a vast number of issues. It is also an effective communication channel for discussing the behaviors, environment, and health of people who follow public health issues such as tobacco use. In this paper, we focus on fine-grained classification of social media posts, such as those containing tweets, journal articles, and other posts in the forum of the Tobacco-Related Forums. To address this task, we propose a novel and comprehensive dataset of English tweets tagged with the 13 contextual tags of smoke status and smoking frequency as well as 2 extra categories of text, namely tobacco+gate (i.e., tobacco and tobacco related content) and non- tobacco+gate (i.e., non- tobacco related content). We describe the dataset construction and our methods for training classifiers on it. Experimental results show that using the more semantic Tobacco-Related Forums data, a state-of-the-art MOS classifier, requires only 11% and 29% training examples to achieve a 75% accuracy, using a naive application of the self-training strategy on the non- Tobacco+gate data.
195. **VAIS ASR: Building a conversational speech recognition system using language model combination**  
    Conversational speech recognition (CRSR) is a challenging task with substantial real-world applications. One of the main requirements of CRSR is to build a natural conversation from a short chat or a few key-phrases extracted from the conversation. In this paper, we will show how to build a robust conversational speech recognition system from just two keywords and a small corpus. The key is to employ language model combination with fully convolutional neural networks (FCN). A phoneme key-phrase is first clustered to form a small training set and the generated speech will then be used as a train set in the CRSR system. The key is to learn a set of high-quality speech segments from the training set and then combine them with the speech features for CRSR. In this work, we propose a novel scheme called Conv-VAIS (Convolutional VAisual Recognition) to generate the background speech using a large number of phonemes and the key-phrase of the previous frame. Furthermore, we propose a new language model called LSTM Language Model (LM) that can be adapted and trained from the human speech without any human interaction. Our CRSR system achieves an accuracy of 96.42% and an F1 score of 87.19% on the LibriSpeech 2011 test set.
196. **VAIS Hate Speech Detection System: A Deep Learning based Approach for System Combination**  
    In this paper, we introduce a new variant of an existing hate speech detection system which performs Twitter sentiment analysis as part of its classification capabilities. The training data for this system is provided by Twitter itself and includes posts that were deemed hate speech by the author of the posts. We introduce a two-stage neural network trained with different pre-processes (redundancy reduction and word embedding) that perform task of detection. We compare our system with a state-of-the-art vanilla deep learning-based system for tweet sentiment analysis. Our experiments show that the use of pre-processed data has resulted in substantial improvement in the final classification performance and, on average, a 12.2% relative error rate reduction.
197. **A Research Platform for Multi-Robot Dialogue with Humans**  
    We present a multi-robot dialogue system based on the minimalistic Open-domain User Interface (UDI) systems. An increasing number of applications of such systems in interactive systems with humans requires that a suitable input method be provided to the system. Traditional text-based methods either provide a full solution, but lack an integrated user interface to which humans can react, or depend on a large feature set with a fixed vocabulary that humans are not directly able to apply. In this paper, we focus on an alternate and highly realistic scenario, where a robot and humans do not have a direct linguistic input method, but have access to a framework for a certain specific task. We present how to construct a multi-robot dialogue system from a minimalistic UDI framework. We outline the modular components of our multi-robot dialogue system, discuss and compare their interaction and comprehension capabilities, and describe a user study. We also present how to integrate multi-robot dialogue with a standard dialogue application. We present an experimental evaluation of the system and user evaluation of the integrated system.
198. **VATEX Captioning Challenge 2019: Multi-modal Information Fusion and Multi-stage Training Strategy for Video Captioning**  
    In this paper, we introduce a new multi-modal video captioning challenge and describe the task design. We provide baseline results using recent video captioning methods. Using a VOT dataset, we investigate methods that fuse the contents of individual frames into a single caption. We introduce a novel approach, which combines multiple information fusion models that utilize a scale model, a depth model, a multiscale model and a multi-stage training strategy to obtain a video caption that is robust to various lighting, view angle and occlusion, and thus easy to describe by humans. Moreover, we propose a novel multi-stage training strategy to accelerate the training of different modality fusion models. Extensive experimental results validate that the proposed multi-modal fusion model can significantly improve the performance of single modality models. The dataset used for the challenge is made publicly available at https://github.com/VOT-Dept/VOTEX-Challenge-2019
199. **Progress Notes Classification and Keyword Extraction using Attention-based Deep Learning Models with BERT**  
    It has been recently shown that the generalization error of BERT in text classification tasks is significantly less than the error of other variants of BERT. In this paper, we propose a BERT-based method for classifying article titles (often called as "progress notes"), which obtains better results compared to the best performing methods trained on the same datasets. Specifically, we introduce a simple hierarchical attention mechanism to model sentences and extracts their important sentences. Then we use a deep learning model with bidirectional residual connections as a base model, to describe the contents of the articles. To train the model, we collect about one million labeled articles from Wikipedia, automatically crawled from various sources, and annotate them by mining keyphrases. Experimental results show that the proposed method achieves state-of-the-art performance in all evaluation tasks.
200. **Estimating post-editing effort: a study on human judgements, task-based and reference-based metrics of MT quality**  
    The quality of Machine Translation (MT) is a very important topic within the MT research community. This paper presents the motivation of this work and its main results, which provide quantitative evidence on the post-editing effort (PE) of a system. Through a case study we try to improve existing performance measures by utilising several techniques in combination with common human judgements, such as bLEU and METEOR. We also provide data-driven insights, aimed at understanding the impact of various techniques on the quality of MT. Overall, we demonstrate the feasibility of our proposed approach.
201. **Interpretable Text Classification Using CNN and Max-pooling**  
    Automatic text classification methods use data distribution to classify text. This data distribution can be assumed to be parametric in that, from each sentence in the training data, we have access to the probability that a class label will appear in the test set. This was demonstrated in an experiment at Penn Treebank (PTB) and another at the University of North Carolina (UNC) on six text classification tasks: five newswire (WT) and four articles (published in arXiv:1710.06319, arXiv:1811.04245). In the existing approaches, we must manually learn these distributions, which impose additional burden on the classifier. We present a method to achieve interpretable text classification without requiring these distributions. Our approach uses a convolutional neural network to learn both the distribution of the training data and the probability distribution for each class label. We leverage the character-level labels (i.e., the class labels) of each sentence to train the network and minimize overfitting. We perform an extensive empirical evaluation of our approach with the existing state-of-the-art text classification models, and find that the proposed method outperforms existing methods on six different text classification tasks and on six additional datasets, for both large and small datasets.
202. **Updating Pre-trained Word Vectors and Text Classifiers using Monolingual Alignment**  
    As WordNet is becoming increasingly accepted by the non-native English speakers, we are facing a growing need for improving text classification accuracy using an adapted version of WordNet. While pretrained word embeddings have been used for their exceptional performance, they are computationally intensive and thus may perform at the expense of better modeling of the semantics of the word as some traditional unsupervised models. This raises a question: can we obtain a linear transformation between input word embeddings and predictions and use it for improving classifier performance? In this paper, we show that it is possible to obtain such a transformation through monolingual alignment in a model trained on the restricted English WSD dataset and in a post-training technique. We experimentally evaluate on multiple data sets and evaluate different models for word alignment in the context of text classification. We show that without any pretraining, the monolingual alignment provides a significant boost in classification accuracy on several datasets.
203. **Whatcha lookin' at? DeepLIFTing BERT's Attention in Question Answering**  
    In recent years, BERT (Alexey Ivanovichvichovichovich Kuznetsov) became the first Deep Learning model for question answering. It yields strong performance in a number of tasks, such as answer selection. In this work, we present a new novel attention mechanism that significantly improves the performance of the state-of-the-art BERT model on three benchmark datasets, i.e., TRECVID MED 2005, SNLI, and Amazon Alexa Prize. We present a detailed analysis of our method with respect to BERT's attention models and show that these attention models are indeed relevant to text classification. Furthermore, we propose a new way to introduce novelty to text classification models by embedding the context information of the question in a new attention model which we call Dense Context Attention Networks (DCAN). We show that our new attention model outperforms previous BERT attention models, especially in the cases where the context is ambiguous.
204. **Tell-the-difference: Fine-grained Visual Descriptor via a Discriminating Referee**  
    Recent research has shown that the Visual Question Answering (VQA) task of obtaining a sentence image from an image query can be cast into a classification problem by transferring knowledge extracted from a human-written template and unstructured text sources. In this work, we propose a novel discriminative label learning approach based on a fusing of a pre-trained language model and a visual question answering (VQA) model. This allows us to separate the clues for the answer extraction from the language model and fusing of the two models into a coherent coherent answer by maximizing a score reflecting the degree of the discriminating quality of the image-question pair. In addition, we demonstrate the capacity of our proposed discriminative approach to improve the performance of VQA and obtain a large improvement on the state-of-the-art.
205. **Mapping Supervised Bilingual Word Embeddings from English to low-resource languages**  
    The morphological analysis of English can benefit from translation information, which is not available in many low-resource languages, such as the seven languages of sub-Saharan Africa. In this work, we use a fully supervised method to generate low-resource morphological annotations of English words. These are related to a pre-trained word embedding of a new language pair, which we use as input to a statistical parser to produce an embedding vector in the target language. We show that this approach results in improved accuracy and fewer ambiguities on the MNIST dataset in terms of percentage of correctly recognized words.
206. **Restoring ancient text using deep learning: a case study on Greek epigraphy**  
    We restore ancient texts (images) to their original high-resolution state by using a new tool called Deep Text Restoration. The restoration algorithm consists of three steps: 1) a data preprocessing step, which applies a smoothing technique to remove noises, and a preprocessing step to preprocess images; 2) an image inpainting step which inpaints the hidden patterns of the missing text; and 3) a text restoration step which inpaints the hidden patterns of the restored text. In each step, a convolutional neural network is trained to predict the effect of each operation on the image and to output the restored image. This last step consists of two main steps: the text prediction and the restoration. In the text prediction step, the text is predicted by a pre-trained convolutional neural network. In the restoration step, the restored text is reconstructed from the hidden patterns of the restorer. In the experiments, we compare our method with three recently developed methods: a gradient descent method, a statistical softmax method, and an autoencoder. All experiments are performed on 300-year-old manuscripts with preservation results which are comparable to those of recent papers.
207. **Unsupervised Question Answering for Fact-Checking**  
    With the recent proliferation of fact-checking websites, a new question-answering task emerges: fact-checking. Answering factual questions about an unannotated, abstract document with incorrect factoid sentences is difficult, but it is possible to perform fact-checking by querying the document for question-answer pairs. Existing work in this task has focused on question-answering for fact-checking web pages, and question-answering for named entity recognition, but question-answering for fact-checking is emerging as an important unsupervised reading comprehension task. While previous work is concerned with question answering with a text-based pre-processor and entity recognition with a named entity recognizer, in this paper we evaluate a fully unsupervised method to construct accurate and human-readable question-answer pairs for fact-checking from unannotated fact-checking web pages. We show that this approach is superior to existing state-of-the-art systems in question answering for fact-checking on standard evaluation datasets. We also demonstrate that our approach can be combined with a general, easy-to-use reading comprehension system, making it a powerful reading comprehension task for Fact-Checking Web pages.
208. **Right-wing German Hate Speech on Twitter: Analysis and Automatic Detection**  
    Recent years have seen an unprecedented increase in the number of right-wing extremist and racist Twitter accounts. Despite the fact that these accounts openly espouse hateful views and participate in publishing hate speech, it is not yet clear how these accounts are influencing public discourse. In this work, we propose a graph-based method to automatically detect and analyse the use of hate speech in German. Our graph-based approach takes the textual content of an account and their position in a user-generated lexical tree as inputs. This allows the model to generalise to new accounts and handle cases in which existing detection methods fail. We present a large-scale, manual annotated corpus of right-wing hate speech, which is publicly available. We evaluate different neural classifiers based on word embedding and perform results on the corpus. We present results on automatically detecting specific accounts and use them to analyse discourse in a new text. Finally, we validate the classifiers based on Twitter data using a secondary text and show that a top-1 classifier over 60% of the instances fails to detect accounts with over 5K retweets and 800 followers.
209. **Efficiency through Auto-Sizing: Notre Dame NLP's Submission to the WNGT 2019 Efficiency Task**  
    The WNGT 2019 Efficiency Task has focused on designing efficient data structures and algorithms for Natural Language Processing applications. The purpose of this paper is to describe Notre Dame's submission to this task. The submitted model consists of a dictionary plus multiple sets of words, each with associated tuples. The Dictionary Addressing component includes a block of words designed as an efficient locality sensitive hashing scheme to enhance compression of a dictionary. We develop a system that predicts word tuples from the sentence and use the dictionary on top of these tuples. We test the model on the output of the Corpus and show that we are able to outperform other state-of-the-art models.
210. **FewRel 2.0: Towards More Challenging Few-Shot Relation Classification**  
    This paper presents a new methodology to perform few-shot relation classification by combining techniques from bag-of-words (BoW) and few-shot learning (FSL). Previous approaches to few-shot relation classification generally employ either BOW-based approaches or FSL approaches on an aligned noisy text corpus. We show that combining these two approaches produces significantly better results than either of them alone on a public dataset of around 7,000 relation pairs.
211. **Comprehend Medical: a Named Entity Recognition and Relationship Extraction Web Service**  
    With the fast development of Knowledge Graph, the number of entities in the knowledge base has increased significantly. Meanwhile, the characteristics and relationships among the entities have gradually been exploited to better understand and identify the entities. As a result, the data representation and classification in medical domain can benefit from entity-based features. In this paper, we propose a web service that could extract entities from the medical text. It can be used to identify the concepts and relations of medical entities from the text, and can be utilized for clinical prediction tasks. To achieve this, we propose a novel model which takes the entity names as the raw input and conducts entity recognition and relation classification, respectively. Finally, it performs relation extraction from the extracted entities. In this model, the entities are assigned to an ontology called SOA (Syntactic Ontology) or DB (Domain Data) model. An LSTM model is used to represent these entities as contextual variables, and the sentences are fed to the model as the input. Extensive experiments on public datasets, such as MIMIC-III, ECML-PKDD, FCE, XING-KB, ICDAR2013, and XML-NHR have shown that our model outperforms the baseline method.
212. **Iterative Delexicalization for Improved Spoken Language Understanding**  
    Delexicalization, also known as tokenization, plays an important role in speech recognition. Delexicalized speech reduces the mismatch between speech and natural language, which is essential for improved speech recognition. In this paper, we propose a tokenizer that iteratively converts between the general mixed-English lexicon (GEL) and the unsupervised language model (LMs). The tokenizer, which is implemented as a neural network, is trained by a sequence labeling task, i.e., recognizing the sequence of tokens in the GEL as tokens from the LMs. Experiments on two standard benchmarks indicate that the tokenizer, as well as the machine learning model, achieved significant improvements over the state-of-the-art.
213. **Facebook AI's WAT19 Myanmar-English Translation Task Submission**  
    Machine translation of modern academic texts is often considered a mature field with a well-established network of skilled professionals, who spend thousands of hours on carefully hand-selecting the vocabulary, sentence structure, sentence style, and word order to obtain the desired output. In contrast, nowadays there exist fewer and far between professional translators, driven mainly by a modest number of patent-encumbered foreign patent-license applications for legal translation and the ever-increasing demands of business clients and legal experts. To make translation more affordable for all translators, we present Facebook AI's submission to the WAT19 workshop on Myanmar-English translation. Facebook's submission to the workshop is comprised of a hybrid machine translation system leveraging a neural machine translation based machine translation system as a backend neural machine translation system, a relatively simple code change detection system, and hand-designed filtering functions. The purpose of this paper is to investigate how we might best make Myanmar-English machine translation accessible to non-specialist translators. To this end, we describe our submission along with a detailed experimental analysis of our system using data provided by the organizers of the WAT19 workshop. Facebook has provided all training data, trained models, and all pre-processed data that is publicly available for reproducibility.
214. **On the Importance of Word Boundaries in Character-level Neural Machine Translation**  
    Character-level neural machine translation (NMT) suffers from poor performance for different languages, particularly for Japanese, due to the inconsistent word boundary between different characters. We observe that the proposed word-level attention mechanism can alleviate the problem by constraining the input word to a small region, by directly replacing the region-maximum value with a score value, according to the generated target sentence. We empirically evaluate our model on different character-level NMT systems (ciphertext, kernelized encoder) on the WMT'14 Japanese-English test set and the WMT'14 Chinese-English test set. Results show that the proposed word-level attention mechanism can improve translation performance by around 10 BLEU points.
215. **Language Identification on Massive Datasets of Short Message using an Attention Mechanism CNN**  
    Human language identification is a challenging problem in machine learning, since the goal is to recognize human's utterances from natural language text and compute classifiers to distinguish between each human utterance and its corresponding language, whereas most machine learning algorithms focus on classification methods which only consider information in texts as input. In this paper, we present an Attention Model on Massive Language Dataset for short message identification. Attention Model is a transfer learning method from sentiment analysis to natural language processing. The focus of attention mechanism is on detecting out of vocabulary words which are important to be identified. We use phrase-based language modeling method to find out relevant phrases of the sentence. A Word2Vec model is used to calculate character vector of each individual word, then uses attention mechanism to identify words that are similar with a certain word. Experimental results show that the proposed method outperforms other machine learning techniques.
216. **NumNet: Machine Reading Comprehension with Numerical Reasoning**  
    We present a system named NumNet, that can be trained end-to-end, but whose inference algorithm is fundamentally different. NumNet relies on a powerful representation of reasoning process as a numerical sequence; we call such sequence 'number sequence'. With the help of 'number sequence' we are able to optimize a series of symbolic algorithmic and statistical tasks to efficiently answer a series of queries. For each task, we introduce a specific layer of our neural network that takes numerical representation as input. The layer accepts numerical representation of a question and outputs a numerical sequence of 'number values' as an answer. Experimental results show that our numerical sequence reasoning system can generalize to many arithmetic questions with complicated underlying reasoning, and can scale up to single-machine inference with order of $100,000+'s of data, which are far beyond current end-to-end models. The NumNet system (https://github.com/deepmind/numnet) can be applied to different domains of machines (big data) and also shows great potentials to build a reading comprehension system with complex reasoning (deep reasoning) and time consuming large scale training procedures (forecasting and inference).
217. **FacTweet: Profiling Fake News Twitter Accounts**  
    Fake news is a major problem in modern society, affecting individuals and institutions alike. Numerous methods have been developed to detect fake news in social media. However, this task remains challenging, owing to various types of online content, and language barriers. Current online detection methods rely on automatic feature extraction and rule-based methods, such as statistical machine translation, and linguistics. These methods are not well suited to different applications, such as an investigation of online advertising, or the analysis of online advertising performance. Recently, the use of computational linguistics has become attractive in fake news detection. This paper introduces FacTweet, a statistical machine translation-based text-to-speech neural network-based system designed to identify fake news tweets. Experiments on a real dataset of real news events demonstrate the effectiveness of the proposed model. Furthermore, a set of 11 Twitter accounts with a combined 10.1 million followers is used to create a large-scale fake news dataset. As an alternative to the conventional computational linguistic approach, the paper proposes a novel text-to-speech neural network-based approach, which outperforms both statistical machine translation and linguistic approaches in terms of accuracy, speed, and robustness.
218. **Aligning Cross-Lingual Entities with Multi-Aspect Information**  
    This paper presents a new approach for aligning cross-lingual word embeddings, which aligns pairs of vectors regardless of their semantic proximity. The core idea is to exploit an existing cross-lingual semantic similarity graph and adapt it to new target languages (either in latent space or shared-latent space) through a specific strategy. This strategy aligns such pairs of vectors in a shared latent space. To validate the effectiveness of our approach, we analyze several downstream tasks using word vectors that have been learned from a shared cross-lingual semantic similarity graph. We show that our approach yields similar or better performance than state-of-the-art baseline methods, while having a low time and space complexity.
219. **Text2Math: End-to-end Parsing Text into Math Expressions**  
    A key challenge in high-level natural language processing is inferring the abstract meaning of a given text. Classical, compositional methods often fail to attain good predictive accuracy in this challenging problem. We propose a Neural Network architecture based on recurrent neural networks to classify and compose math expressions of arbitrary length. Our approach achieves state-of-the-art accuracy in this task on the Extended Yale B dataset, while being more efficient than all other methods using simple function approximation.
220. **MIMO-SPEECH: End-to-End Multi-Channel Multi-Speaker Speech Recognition**  
    Despite the recent advances in automatic speech recognition (ASR) research, a real-world practical MIMO system would have to overcome several design challenges. In this paper, we introduce MIMO-SPEECH, a new approach for end-to-end multi-channel multi-speaker speech recognition using a recent MIMO-SPEECH layer that offers both end-to-end and channel-level independent feature extractors. An ASR system that leverages these two-dimensional (2D) feature extractors directly would be capable of recognizing several languages with one model, enabling its use for language-dependent applications (e.g., advertisements, border control, video messaging, voice-controlled cars). We demonstrate the ability to achieve speech recognition on two large data sets: a test set consisting of nearly 20 hours of real-world Mandarin telephone conversations, and a second one comprising spoken conversations from Diverse Technologies, a voice-controlled phone application. We also evaluate our approach on the DUC 2008 Mandarin competition task. We achieve substantial recognition accuracy improvements over state-of-the-art MIMO-SPEECH-based models. The obtained results, as well as the proposed methods, will be made publicly available.
221. **Answering Complex Open-domain Questions Through Iterative Query Generation**  
    We address the task of automatically answering complex open-domain questions, especially the most common type of open-domain questions, in the domain of health informatics. Given a question, our goal is to find the most relevant ontology from a large knowledge base and generate a set of formal and ontological representations to answer the question. In this work, we define a new generic open-domain question answering model, based on a ranking approach of the (potentially incorrect) ontology representation and a direct seq2seq-based distillation of the question representation into the ontology representation. We present a competitive baseline model trained with a manually annotated dataset from the CLIO 2018 contest, and demonstrate the effectiveness of the proposed method on two real-world datasets: MovieQA and CAMH-QA. Experimental results show that our model outperforms the competing methods both in accuracy and robustness. We hope that the current state-of-the-art results in the QA domain would motivate other researchers to develop similar models for other domains.
222. **Detecting Machine-Translated Text using Back Translation**  
    Automatically detecting machine-translated text is an interesting research problem because of the difficulty of different languages, the variety of technical methods and the difficulty of computing the reference translations. Machine translation is a popular research topic and techniques for evaluating the quality of machine-translated texts are widely used. In this article, the problems are introduced and analyzed in detail. There are several approaches proposed to deal with machine-translated texts, including: (1) utilizing the parallel datasets, (2) using the random forests method and (3) using a statistical model based on human judgments. A thorough evaluation was performed on the English-to-German and English-to-French machine translation tasks and the results demonstrate the effectiveness of each of the proposed approaches. Furthermore, an extensive comparison was performed by using different back translation methods as proposed in this paper. This comparison revealed that the accuracy of the statistical back translation method improved the most for both the English-to-French and English-to-German machine translation tasks.
223. **Imperial College London Submission to VATEX Video Captioning Task**  
    In this paper, we describe the task of recognizing human actions for video captioning. We present the work of a team of Imperial College London (ICL) researchers in this project. To advance the field of video captioning, it is essential to adequately capture information needed for generating the logical description of a video. This paper provides an overview of the challenges and methods used in this work. We describe the sentence-level video captioning task using two benchmark datasets as a case study. We then report on our ICL team's contributions to this task. In particular, we introduce our video-to-word model based on word embeddings and compare it to a recurrent neural network based on recurrent gated recurrent units. Furthermore, we introduce the CorreInt video-to-video model and present an evaluation of our proposed method on the public benchmark dataset.
224. **Using Whole Document Context in Neural Machine Translation**  
    Much of current Machine Translation (MT) systems are based on neural machine translation, relying on pretrained word embeddings. However, there is little understanding of how the internal context in a sentence contributes to the translation of the whole document. To address this problem, we evaluate the effect of sentence context on the accuracy of neural MT systems. We propose a novel neural MT system to aggregate the context of the sentence, which we call TenseNet, for improving the performance of MT systems. TenseNet consists of two components: 1) a background attention model, which performs sentence-level attention and 2) a notion of context, which is computed based on the learned network. We evaluate our system on four MT datasets, including standard datasets and two new datasets. In the standard datasets, our system outperforms current state-of-the-art systems. Our model shows promising improvements on two new datasets.
225. **Towards Annotating and Creating Sub-Sentence Summary Highlights**  
    In this paper we present a method to generate automatically a simple summary for new sentences containing complex sentences. For example, we might be presented with a complex sentence that is implicitly included in the summary of a previously generated long text. Our proposed method, called Acoustic Summarizer (AS), can be used to generate automatic summaries for new sentences containing sentences that are inherently difficult to form or create. Our main contribution is the annotation and annotation of the corpus, which has been used to create summaries for many of the existing data. A second contribution is the definition of a simple evaluation criterion for manual evaluation of AS and automatic evaluation of abstractive summaries. Based on this, we have created a new dataset for automatic evaluation of structured, unconstrained abstractive summarization. Finally, we have evaluated AS by producing summarizations of selected retrieved articles from the encyclopedia-building site Wikipedia. The automatic evaluations suggest that AS is capable of generating summaries that are of comparable quality to those produced by humans.
226. **BUT System Description to VoxCeleb Speaker Recognition Challenge 2019**  
    This paper introduces theBUT System for VoxCeleb 2019 Speaker Recognition Challenge.This system was used to extract a personalized training set of 15,500 speaker attribute labels, which were then used to train a 4-layer VGG 16, LSTM and Auto-Encoders network. The output of these 3 classifiers is a vector of normalized words and character positions.We also present a novel ensemble approach, which was successfully used to achieve a score of 64.6% on the official evaluation task.
227. **Fine-grained evaluation of German-English Machine Translation based on a Test Suite**  
    Syntactic tagging, which characterizes how each noun class in a sentence is tagged by a morphological analyzer, is used to optimize the performance of neural machine translation (NMT). Automatic evaluation of grammatical tags is often conducted through examining texts written in both languages, but in general such evaluations do not take into account the variety in writing styles between languages. We present an approach for evaluating NMT systems based on the coverage of test phrases. We evaluate machine translation systems in multiple languages and test on a single test phrase. This helps us avoid direct comparisons between languages, which are difficult in many settings, but also suggests best practices to avoid errors in automatic evaluation. We use a data set of 300 English--German sentences and describe the method. We find that BLEU 4.1 scores are lower for English than for German, but the highest BLEU scores are obtained for German, rather than English.
228. **Linguistic evaluation of German-English Machine Translation using a Test Suite**  
    The paper describes a new automated evaluation methodology for Natural Language Processing (NLP) on German-English Machine Translation (MT), where the evaluation is carried out through a test suite of translations chosen from a corpus of European languages. Three methods were used to evaluate the quality of the German-English MT systems: precision, recall, and F1 score. The precision method was used to compare the output of the MT system to a reference translation. The recall method compares the input sentence's translation to a reference sentence. The F1 method gives the ratio of input to target sentences in which the MT output is higher than the reference sentence.
229. **Bridging the Knowledge Gap: Enhancing Question Answering with World and Domain Knowledge**  
    This paper proposes a novel Knowledge-augmented Neural Machine Translation (Kaldi) framework that enhances the accuracy of the automatic translation system with additional knowledge by introducing knowledge-aware cross-domain translation. We first develop a state-of-the-art knowledge-augmented sentence encoder based on generative adversarial networks (GAN) and its information fusion module to fully understand the entire text and achieve good translation performance. Then, we extend this deep model to have two specialized modules: a global (Supervised Attention module) and a local (Focus Map module) to achieve attention attention or attention-based learning, respectively. Based on these two components, Kaldi is improved with additional world knowledge by integrating knowledge-aware training and inference methods. Experiments conducted on multiple benchmark datasets demonstrate that our method significantly outperforms the state-of-the-art methods in both sentence translation and entity linking, which shows the effectiveness of the knowledge-augmented model.
230. **Evolution of transfer learning in natural language processing**  
    We investigate the evolutionary constraints of transfer learning in a neural machine translation system. This paper shows that overfitting makes the transferable features propagate more gradually and hence generate better translation results. Using improved features we do not need more transferable parameters to reach the same level of performance and consequently the learning time is reduced. This study leads to a new understanding of transfer learning in machine translation systems by pointing out the necessity of selecting parameters to enable the propagation of useful features in the system. We also propose a new way to estimate the parameter optimisation which allows for scalable parameters selection in the presence of large and fast-to-train neural networks.
231. **Generating Challenge Datasets for Task-Oriented Conversational Agents through Self-Play**  
    In this work, we propose a novel approach to generate open-domain conversational agents with structured user-agent interactions. We describe a method to acquire actionable task-oriented knowledge from user profiles for acquiring task-oriented dialogic knowledge. Through a variety of evaluations and an ablation study, we show that our approach can outperform current state-of-the-art techniques by significantly improving the task-oriented conversational agent user interaction.
232. **Content Enhanced BERT-based Text-to-SQL Generation**  
    We consider the problem of machine-to-machine translation (MTG) with auxiliary objectives that improve the monolingual BERT model. Unlike prior work, which relies on a monolingual BERT model and more importantly monolingual semantic embeddings for the same, we target improving the translation performance without using any semantic embedding information. We propose to use the embeddings learned by the monolingual BERT model itself as a source of supervision. However, instead of feeding BERT source tokens to the decoder, we target to learn encoder parameters with the target language in a "micro-encoder". We compare the effectiveness of our proposed architecture on the CoNLL-2014 dataset with a state-of-the-art BERT-based text-to-SQL generation method, and show that our architecture provides a consistent $5.1\%$ and $4.1\%$ gain, respectively. Furthermore, we also use our proposed micro-encoder architecture to improve the translation accuracy of a BERT-based supervised sentence-to-SQL task.
233. **BERTRAM: Improved Word Embeddings Have Big Impact on Contextualized Model Performance**  
    In this paper, we present BERTRAM, a general framework to improve contextualized word representations. BERTRAM extends existing word embedding techniques to extend word representation space in order to better capture contextual semantic relationships, and obtains significant improvements on several benchmark datasets. The proposed BERTRAM can be embedded in existing word embedding models with minimal modification, and thus provides a significant generalization of existing methods. BERTRAM improves the accuracy of state-of-the-art contextualized models on four benchmark datasets, with both large absolute and large relative improvements.
234. **Transformer ASR with Contextual Block Processing**  
    Recent studies in automatic speech recognition (ASR) suggest that contextually-encoded feature extractors (CEFs) might be a promising approach for improving ASR. However, the scale of these CEFs is usually very large, which makes them difficult to be embedded within a high-capacity encoder-decoder network, which is typically used for the recognition task. To overcome this limitation, we propose to use an attention-based transformer network for ASR, which can model multiple contexts at the same time and thus render massive feature extractors. Experimental results demonstrate that our approach can improve the ASR results significantly in speech recognition tasks, for both open-domain and conversational languages.
235. **Meemi: A Simple Method for Post-processing Cross-lingual Word Embeddings**  
    Cross-lingual word embeddings have recently become a popular method for exploiting the distributional properties of words across languages and for learning distributed word representations. However, the literature lacks a full characterization of how well cross-lingual embeddings perform on this task. To this end, we present Meemi, a simple method for post-processing cross-lingual word embeddings by learning the distributions of both source and target embeddings to fit a normal distribution. Meemi is inspired by a similar algorithm known as MRF, which has been used for smoothing over discrete distributions in standard word embeddings. We empirically evaluate the performance of our method on the two tasks of POS tagging and dependency parsing. We find that Meemi outperforms other methods by an average of 1.84 F1 points on POS tagging and by 2.5 F1 points on dependency parsing.
236. **Why can't memory networks read effectively?**  
    Neural networks that allow retrieval are known to be successful in learning visual features from large, complex datasets. We investigate how the network works to perform this task. We find that in the retrieval task, the networks continue to read information after it is fully processed, rather than storing it as part of a reconstruction process. Furthermore, as we observed in the case of digit recognition, this comes at the expense of higher memory consumption.
237. **Fine-grained evaluation of Quality Estimation for Machine translation based on a linguistically-motivated Test Suite**  
    The paper presents a comparison of machine translation quality estimation systems from machine translation (MT) evaluation tools and human evaluators. Machine translation quality estimation, also known as quality assessment, is considered to be an essential component of any MT systems. However, the evaluation tools have their own shortcomings. In particular, the evaluation tools are mainly useful for machine translation (MT) quality assessment. However, these quality assessment tools are not yet able to be properly adapted to the MT tasks. To overcome these shortcomings, the paper introduces a linguistically-motivated, large-scale, parallel test suite for MT quality estimation. The paper describes how the system outputs of the English-German (E-G) MT evaluation tools and human evaluators are differently represented on this test suite. As a result, the paper proposes a set of appropriate quality estimation tools for MT evaluation, which are used in two MT tasks. Furthermore, a linguistically-motivated, quality assessment test suite is also developed and used to evaluate three MT systems. The proposed method has been experimentally validated by using this linguistic-motivated test suite.
238. **Question Classification with Deep Contextualized Transformer**  
    Due to its recent success, deep convolutional neural networks (CNN) is gaining increasing attention in many applications, including sentiment analysis, image classification and other tasks. Most of the existing works focus on improving a single CNN architecture, which sometimes results in sub-optimal solutions, especially when dealing with large-scale image datasets. In this work, we propose a novel method for training CNN models to improve their ability to learn semantically meaningful features that characterize real-world images. We propose a novel network architecture with shared layers that contains a dual encoder to create shared contextualized embeddings that are adapted at different levels for specific tasks. We then combine the model with a sequential generation task to produce a high-quality image that can be understood from the context. We evaluate the proposed method on multiple public benchmarks and show that our model outperforms state-of-the-art baselines. Moreover, we show that the obtained representations can be directly exploited by other applications such as semantic image retrieval and text classification.
239. **Explainable Authorship Verification in Social Media via Attention-based Similarity Learning**  
    Research has shown that authorship verification (AV) is an important component of identity-based social media services. The existing methods for social media verification rely on a series of hand-crafted features. Despite their quality, the hand-crafted features remain the cornerstone in practical AV systems, which may not be suitable for textual content. In this paper, we present a new attention-based mechanism for AV that is well-suited for text-based authorship verification. We also introduce an attention model with self-attention to enhance the text-based authorship verification results. The attention model is optimized via a reinforcement learning algorithm. In addition to the deep and attention-based attention model, we also present two reinforcement learning techniques, which can enhance both accuracy and robustness of our model. Experiments on a real-world dataset demonstrate that the proposed method significantly outperforms the baseline methods in terms of accuracy and robustness, and shows significant advantages in terms of effectiveness and interpretability.
240. **Marpa, A practical general parser: the recognizer**  
    Over the past two years, the MVW parser has emerged as one of the major success stories of parser construction. In this paper, we present a simple neural-network-based instantiation of the Marpa parser, the core of which is a bag-of-words predictor network. In a first pass, the predictor focuses on learning an acoustic-phonetic parser from data, using a simple encoder-decoder architecture. We then use the trained parser to produce a parse, which is improved from a preliminary parser that has been trained using synthetic input patterns and normalizations. An algorithm to calculate the disambiguation cost of a target word given a parse produced by the Marpa parser is also presented. We show that this cost is an important metric for evaluating parser construction and for creating an effective grammar, showing its utility in the grammar evaluation task. Finally, we use the Marpa parser as a baseline for improving results on syntactic analysis.
241. **Cross-lingual Parsing with Polyglot Training and Multi-treebank Learning: A Faroese Case Study**  
    Cross-lingual parsing is a practical yet challenging task in cross-lingual natural language processing, since it requires the ability to translate low-resource languages to high-resource ones. In the Faroe Islands, only a small part of the data for the first language is available in English. To tackle this problem, we propose to map the first language into a high-resource Faroese language by translating the data in that language. Our translation model can be used with a novel multi-treebank method for multi-lingual word representation learning. Our proposed method reaches 93.52 F1 for cross-lingual parsing with this method and with a cross-lingual distributional model. With the multi-treebank model, we reach 92.81 F1 and with a linguistic information model that implicitly takes into account linguistic similarity, we reach 90.21 F1.
242. **PLATO: Pre-trained Dialogue Generation Model with Discrete Latent Variable**  
    In this paper, we present a data augmentation technique to address the recent challenge of training large neural encoder-decoder neural dialogue models, using pre-trained BERT and deep Q-network (DQN) models, for low-resource speech recognition. Our approach is to introduce a discrete latent variable to the DQN encoder, which is trained to be discriminative with high confidence for in-domain utterances, and then outputs the dialogue state as discrete random variables with continuous marginals in the decoder. By using discrete latent variables, the large DQN model can be trained with less data, which may accelerate the training progress. On the Penn Dialog Corpus, we have improved state-of-the-art frame-level error rate by over 10% with 4x less data, which is related to the improvement of high-level dialogue features. For in-domain utterances, our experiments have shown that using the discrete latent variables greatly reduce the dialogue state dimensionality, which is important for transferring knowledge from distant domains, where many speech recognizers are not available.
243. **LibriVoxDeEn: A Corpus for German-to-English Speech Translation and Speech Recognition**  
    We describe LibriVoxDeEn, a new, publicly available corpus of German-to-English text aligned to an English Wikipedia, augmented with natural acoustic speech sequences. This corpus was built for the NIPS 2018 European Machine Translation workshop and consists of speeches in German, English, Italian and Portuguese, broadcast on Youtube and a pay-per-subscription stream. We collected the text from the YouTube video, adding annotations to align the text to the Wikipedia based transcriptions. We evaluated a wide variety of neural machine translation models and standard speech recognition models, as well as neural abstractive models. We found that the neural abstractive models outperformed their counterpart in the turn-taking tasks, with an absolute gain of 13.5%.
244. **Topical Keyphrase Extraction with Hierarchical Semantic Networks**  
    Keyphrases are words that have a large impact on the meaning of the document. To help extract topical keyphrases from large text documents, we train a bidirectional word2vec model to characterize the semantics of the text. We use this model to label text documents by topical keyphrases. In addition, to exploit knowledge in the knowledge graph, we employ gated-attention to address the problem of not being able to detect in an efficient way the topical keyphrases. Based on these two knowledge sources, we generate an ontology from the texts, which is combined with a global contextualized embedding. We present experimental results showing that this method is very effective for topical keyphrase extraction from large text documents.
245. **Using a KG-Copy Network for Non-Goal Oriented Dialogues**  
    One of the most significant tasks in human-computer interaction is to induce cognitive activities, such as memorization, forgetting, and learning. Based on the dialog system description in the most recent AI literature, a KG-copy network architecture was proposed to solve this problem. This architecture introduces copies of the conversations from the previous dialogs to cover the span of the dialogue with the goal of contributing to the generation of new memories. The copied content is used to produce new answers as well as to generate copy dialogues that will contribute to the generation of memory. In this paper, we propose a modification of the KG-copy architecture that utilizes only these copy memories. We discuss in detail the new architecture and its benefits in the domain of computer-assisted speech recognition. The approach has been evaluated on a new benchmark with 50 real human-computer conversations. The results show that the proposed KG-copy architecture significantly improves on the baseline.
246. **BIG MOOD: Relating Transformers to Explicit Commonsense Knowledge**  
    Many real world applications of neural language models, including named entity recognition and machine translation, involve being able to accurately infer new knowledge. Existing works for these applications often model the source and target models as separate modules, and propose explicit knowledge transfer rules which specify the conditions under which new knowledge can be inferred. Such methods are usually manual, expensive, and may involve human judgment. In this paper, we study the problem of automatically inferring knowledge from any syntactic category of a source sentence, and with no explicit knowledge transfer rules. Instead, we leverage the structure of a sequence of conditional generative models to compute new "structured" knowledge embeddings, which provide a natural way to infer new "data objects" as defined in our knowledge transfer rule. Furthermore, in order to speed up the process, we propose a statistical non-parametric approach that allows us to seamlessly incorporate additional syntactic categories of the source sentence, as well as additional domain knowledge. We evaluated the performance of the model on a knowledge base completion task, the task of translating Wikipedia articles, and a synthetic text sentence completion task. Experimental results show that, compared to baseline models, our approach reduces the size of the model by up to 36% and reduces the error rate by up to 14%.
247. **SetExpan: Corpus-Based Set Expansion via Context Feature Selection and Rank Ensemble**  
    Modern artificial neural networks (ANNs) usually use an exponentially large set of learning parameters to obtain a good prediction. In this paper, we introduce SetExpan, a fast method for expansion of ANN sets. SetExpan utilizes context information on the input data in a principled way to expand the input space. Context information describes global context information, which is the same when we observe all images in the training set, and local context information, which is different for different features. Additionally, SetExpan extracts context features using the problem-specific shape information that is available in the ANN training dataset. The experimental results on several benchmark ANNs demonstrate the usefulness of the proposed method.
248. **Sticking to the Facts: Confident Decoding for Faithful Data-to-Text Generation**  
    Dealing with real world data varies with time, domain, and storage density. There are several reasons for this. One is the actual data are unstructured and noisy, which limits the applications of current NLP techniques. A more fundamental challenge is that text are often richly annotated with factual information, but unreliable annotations are hard to come by for more modern datasets. Besides, these datasets are diverse and often small enough to enable efficient training of pre-trained models. In this paper, we propose a confidence-aware re-ranking technique that preserves both the truthfulness of the input and the confidence of the output given a seed set of facts extracted from the database. Furthermore, we discuss a novel context distilling approach that is designed to facilitate the generation of more fluent, accurate, and transparent text. The effectiveness of our approach is demonstrated on the public Ubuntu Dialogue Corpus.
249. **Automatic Post-Editing for Machine Translation**  
    Post-editing refers to a task of improving the quality of machine translation output by assigning different versions of the same translation to different sentences in the source and the target languages, following the guideline of SMT by assigning both versions to each sentence pair. Unfortunately, there are a few effective approaches for automatic post-editing of machine translation. In this paper, we present a novel approach for automatic post-editing. Our approach is based on two encoders that are designed to work together. Firstly, the encoders are presented with source sentences, where each encoder assigns multiple translations to the source sentences. Secondly, a set of post-editing rules are dynamically computed by aggregating the outputs of the encoders with respect to a target sentence pair, such that the rule is only applied once, at the beginning of a sentence. Extensive experiments on Chinese-English translation show that our approach significantly outperforms a number of baseline methods, including those employing costly SMT rules, with more than 10 BLEU points on average.
250. **An Improved Historical Embedding without Alignment**  
    In this paper, we improve the representational power of the HE* dataset by aligning it with information on historical events and keywords within the dataset, as well as a recently introduced selectional view of events. Our embedding allows us to use natural language processing algorithms to link new questions to existing ones, without access to either data or queries, and without the need to use external word embeddings. We found that the HE* model represents tweets within a text region that aligns with this selective view of the text. This reveals a great advantage of our data alignment approach, as it provides strong theoretical guidance that state-of-the-art representational learning methods lack.
251. **Using Local Knowledge Graph Construction to Scale Seq2Seq Models to Multi-Document Inputs**  
    Sequence-to-sequence models have emerged as a state-of-the-art approach for speech recognition. Typically, these models are trained on large amounts of data, requiring significant human effort to annotate. While large amount of data provide effective data for training acoustic models, they often make the generation of the acoustic model more difficult since auxiliary information from the source data is not included. In this work, we propose to use the graph topology information of a lexicon to solve the problem of data sparsity and improve the performance of the sequence-to-sequence model. In particular, we employ a heuristic method to extract knowledge graphs from the input data using a local knowledge graph construction approach, and incorporate it into the inference process of the seq2seq models for multi-document inputs. Experimental results on two datasets show that our approach can improve the quality of the generated text significantly, achieving state-of-the-art results on multi-document transcription tasks.
252. **Controlling Utterance Length in NMT-based Word Segmentation with Attention**  
    The encoder-decoder neural machine translation (NMT) framework is shown to achieve impressive improvements over a competitive baseline encoder-only model in machine translation of discourse structure. We observe that NMT models modify their source code in a subtle yet effective manner. This modification results in shorter outputs containing more semantic content. We observe these features to improve performance on discourse parse tasks. We evaluate our models on German-English (German-English MT), English-Chinese (English-Chinese MT) and English-French (French-French MT). The improvements obtained by all models are between 0.2 and 1.6 BLEU points.
253. **Model Compression with Two-stage Multi-teacher Knowledge Distillation for Web Question Answering System**  
    Most of existing state-of-the-art web question answering systems learn rich representations of words and sentences based on various different knowledge distillation methods. However, most of them suffer from slow decoding process which results in poorer performance of the systems. In this paper, we propose a novel strategy to compress the knowledge into a separate transformation matrix to better fit the network parameters while preserving the expressive power of the underlying knowledge distillation model. Two-stage knowledge distillation (2-KD) strategy is proposed to ensure the compatibility of the distillation model with the target model. First, the low quality words are separated from the good words in the knowledge distillation model. Then, a two-teacher knowledge distillation model is developed to aggregate the knowledge distillation results into a single teacher model. Experimental results show that the proposed 2-KD strategy is able to significantly compress the knowledge from a large-scale knowledge distillation model which is trained on the same dataset. By doing so, the one-shot model is trained for new knowledge from scratch, and has a faster decoding speed without the knowledge distillation model suffering from the degradation of the performance.
254. **Concept Pointer Network for Abstractive Summarization**  
    This paper proposes a novel semantic vector space model (C-CVM) for extractive summarization, which learns and disentangles the intrinsic representation and document semantics via a hierarchical disentangled convolutional neural network (CNN). C-CVM can be easily integrated with any existing CNN architectures for large-scale summarization. To evaluate C-CVM on a recently-proposed benchmark dataset, we propose a novel and challenging abstractive summary generation method (C-COG) with the same semantics as the benchmark. Extensive experiments on both our novel benchmark dataset and C-CVM suggest that C-CVM is capable of capturing semantic patterns in text, can better generate abstractive summaries than other abstractive and extractive models.
255. **Entity Summarization: State of the Art and Future Challenges**  
    We analyze how the state of the art of sentence-level entity summarization systems perform, taking advantage of evaluation data based on the DUC datasets. We find that individual systems have slightly different performances in terms of their evaluation outputs, but have generally reached the same levels of performance. We observe a similar trend for machine learning systems, indicating that good performance is an essential component of the training process. In summary, the proposed evaluation metrics and data collection techniques are highly relevant and will help systems to advance and to compete.
256. **End-to-End Speech Recognition: A review for the French Language**  
    The French speech recognition (SR) is of global interest. With the help of recent breakthroughs in sequence-to-sequence (seq2seq) speech recognition, this is the most time-efficient language to which automatic speech recognition can be applied. Most successful SR systems however employ speech enhancement techniques that extract good speech transcriptions, which is implemented as modules. The main advantage of end-to-end trainable modules is that they do not need on-the-fly adaptations of speech characteristics or decoding procedures. As a consequence, SR systems that adopt module-based training have the same final performance, which shows how simple training methods give good results.
257. **Keyphrase Extraction from Scholarly Articles as Sequence Labeling using Contextualized Embeddings**  
    Keyphrase extraction (KPE) is a fundamental task for extracting topic areas of scientific articles. However, existing KPE methods focus only on identifying keyphrases from a paper. The types of keyphrases may vary significantly from article to article. Moreover, most of the state-of-the-art KPE methods are designed for monolingual data and fail to adapt for a language pair such as English-Arabic. We propose to address these issues by first developing a novel model for KPE from a collection of article abstracts by leveraging context information from the title and abstract. Then, using two types of contextualized embeddings, we design a multi-task neural network to solve the KPE task simultaneously for all languages that are in a common language family. Finally, the multi-task network can transfer its knowledge across different languages. Our method can achieve the state-of-the-art results on both English-Arabic and English-Vietnamese KPE tasks and improves the results on English-Vietnamese as compared to a number of baselines.
258. **Natural Question Generation with Reinforcement Learning Based Graph-to-Sequence Model**  
    Natural Question Generation (QG) involves the question generation process with linguistic and logical structures. Previous studies have shown that neural attention models have been successfully applied to tackle this task. However, the original graph-to-sequence models, which directly generate sentences, have not been explored in this paper due to their high requirement of knowledge graph to build a question-aware model. This paper proposes a novel graph-to-sequence model to generate the questions with specific contents with graph attention mechanism. Furthermore, we propose a new QG benchmark to measure QG model, which consist of the questions from Wikipedia and a list of English Wikipedia nodes. Experimental results on the proposed benchmark dataset show that our model outperforms several baselines, while it is easily interpretable and easy to be generalized to other tasks.
259. **MonaLog: a Lightweight System for Natural Language Inference Based on Monotonicity**  
    Monotonic rules are non-monotonic, and allow an answering rule to be instantiated in the context of an answer. Thus, they allow the system to perform monotonic inference. This paper presents a novel information-theoretic parameterization for monotonic rules. The parameterization is based on the Monotonic Condition Object (MCO), which quantifies the probability of a conditional probability of a valid answer. Thus, we can compute the maximum possible value of a MCO for a given answer using the maximum of the decomposition of the answer set and the parameters of the optimal answer-generating rules (via a combination of the answer set and its optimal answer-generating rules). Additionally, we present a novel formulation of monotonicity, which offers several theoretical insights into the parameterization. The experimental results show the effectiveness of the proposed system on several standard data sets, and demonstrate the strength of the proposed mechanism when compared to other state-of-the-art results for single-target, contextual answer set reduction.
260. **Adversarial Attacks on Spoofing Countermeasures of automatic speaker verification**  
    Automatic speaker verification (ASV) is the gold standard in verification technologies. Despite the fact that ASV is usually unreliable, it is very useful for reducing the costs of e.g. audiovisual teleconferencing systems. So far, spoofing attacks were not studied as possible alternatives to secure ASV systems. In this work, we study the effectiveness of ASV spoofing attacks on current ASV countermeasures and the effect of deep neural network (DNN) based countermeasures on ASV spoofing. Moreover, ASV spoofing using white-box and black-box attacks are analyzed, based on the extracted DNN features of ASV training and test data. We use a freely available ASV spoofing dataset which includes 12 attacks of various types that are aimed at extracting speech features corresponding to ASV spoofing. We used a supervised learning approach to extract features from DNN models of ASV countermeasures and compared them with ASV spoofing using white-box and black-box attacks. Results show that as expected, ASV countermeasures are not robust against all the attacks. This robustness is more surprising and surprising because the ASV spoofing attacks were discovered before ASV countermeasures were proposed. By the end of the analysis, an open discussion on the robustness of ASV countermeasures against ASV attacks is provided, both from a theoretical and practical point of view. This analysis illustrates that the ASV countermeasures are a little bit more robust against many different types of spoofing attacks. In addition, by an experiment with a large bank of ASV spoofing attacks, we show that DNN based ASV countermeasures are more robust than traditional ASV countermeasures against the given spoofing attacks.
261. **Findings of the NLP4IF-2019 Shared Task on Fine-Grained Propaganda Detection**  
    In this paper, we describe the ensemble of NLP4IF 2017 shared task winners "message intent classification and detection for fake news" and their participation in the PropOrNot 2018 diagnostic subtask, which builds on the work of Camacho et al. in its proposal. In this work, we propose a model that uses lexical and structural features derived from a statistical phrase graph (SPG) to identify the context of a given text fragment as a propaganda piece. We evaluate different models and ensemble strategies and evaluate their ability to generalize on a training set of corrupted news articles as well as on the PropOrNot diagnostic subtask. Our results suggest that simple phrase classification and individual ensemble methods are sufficient for this task. Additionally, we present a text anomaly detection method that uses vector representations derived from an SPG to identify suspicious sentiment and identify manipulated pieces as propaganda. Finally, we release the SPG for research purposes.
262. **Trouble with the Curve: Predicting Future MLB Players Using Scouting Reports**  
    Quantifying future performance is a challenging problem in both baseball and other sports, particularly in the modern era when big data and sophisticated techniques such as machine learning are the state-of-the-art tools in the analytics game. While there is a well-established research community devoted to predicting the future outcomes of individual players, forecasting the entire future trajectories of players in the same professional league remains challenging. In this paper, we combine statistical learning and machine learning to predict the major-leaguer trajectories in the three major pro sports. Our model is a conditional deep generative neural network that uses free-text scouting reports. The model is trained using a set of simulated players and then evaluated using historical data of players from a decade. Compared to traditional ensembles, our approach achieves more stable and more stable performance.
263. **Byte-Pair Encoding for Text-to-SQL Generation**  
    We present two important advances for the task of generating SQL queries from text. The first one is that we successfully discover and exploit semantic regularities between the query and the database schema, and encode the query based on the schemas with all possible permutations of columns and rows, while the second one is a new byte-pair encoding algorithm that efficiently handles the noise introduced by the table structure. We evaluated our methods using a realistic benchmark of 9, cp40k, and 6, SQL-Core. Our experiments demonstrate that our methods can outperform the state-of-the-art on all datasets, but our byte-pair encoding approach can also handle table-influenced noise more effectively than existing methods.
264. **Good, Better, Best: Textual Distractors Generation for Multi-Choice VQA via Policy Gradient**  
    Multi-choice reading comprehension tasks such as Visual Question Answering (VQA) have become increasingly popular in recent years. The problem has received much attention from the literature and a number of techniques have been proposed to address it. Existing models typically work by solving several regression problems on the passage-level but require a large number of documents for training. On the other hand, much of the recent literature has focused on documents produced by a single system trained on small-scale datasets. In this paper, we propose a new model to learn an attentional mechanism to handle the huge text size in the VQA. We first employ an attentional reader that extracts sentence-level features from the passage-level data. Then, we apply several policy gradient methods to train the model to improve the answer prediction quality. We evaluate our approach on the multi-choice VQA dataset released by TREC 2017 and the results show that our model outperforms the state-of-the-art models significantly and is comparable with a machine learning model (via the AUC-ROC metric) trained on smaller text.
265. **Deep speech inpainting of time-frequency masks**  
    Deep learning has been shown to be a promising technique for solving difficult speech inpainting tasks. However, in real applications, one needs to deal with noisy speech which is very challenging for conventional approaches. One alternative to provide a more effective solution is to use the power of neural networks to learn and improve the model performance. In this paper, we present a joint time-frequency learning method to reconstruct speech inpainting results from weakly aligned speech in a deep neural network. The proposed method is trained to reconstruct speech inpainting results by minimizing the reconstruction error using the FFT signal. In addition, to further enhance the reconstruction quality, a novel audio feature pooling and high-quality speech normalization are proposed to increase the reconstruct quality and robustness of the reconstructed speech. Experiment results with three different datasets demonstrate the effectiveness of our proposed method for generating realistic speech inpainting results.
266. **Representation Learning for Discovering Phonemic Tone Contours**  
    Accurate modeling of the fundamental frequency contours in speech remains an important issue in speech processing. Inspired by the well-known transition-based work of Chen et al. [2009] on character-level encoding of spectrograms, in this paper we present an analysis of the frequency contours in continuous speech by formulating a high-order mixed integer linear program, representing a complete contiguous spectral tract along which all phonemic tones can be separated. On one hand, we analyze the application of Gaussian Mixture Models (GMM) to the challenging task of segmenting time-varying continuous spectrograms into identifiable phonemic tones, from the spectrogram of a time-frequency graph. On the other hand, we also show how various wavelet transforms such as the logarithmic transforms of the transforms of ICA and WCA (used in phonemic-based music modeling) can be useful to break down the problem of reconstructing speech waveforms into phonemic features. By considering phonemic tone decomposition as a discrete-time Markov Random Field problem, we have also derived an efficient approximation of the policy gradient algorithm that, inspired by the alternating direction method of multipliers, can be easily computed by solving the corresponding discrete-time Markov Random Field. In order to demonstrate the merits of this approach, we compare it to another Markov Random Field approach, the Decision Tubes (DT), that uses the semi-Markov property of the DT (since DT considers discrete-time dynamics). Experimental results demonstrate that the proposed approach indeed recovers the spectral tract with satisfactory accuracy on a speech dataset from different genres.
267. **Predicting the Leading Political Ideology of YouTube Channels Using Acoustic, Textual, and Metadata Information**  
    There are different types of users in online media platforms, such as YouTube and Twitch. Both content creators and viewers use these platforms to share their experiences, opinions, and ideas. Through automatic analysis, we observe that there are pronounced differences in personality and political affiliation between different users in each of these platforms. To understand such differences, we analyze the text, audio, and metadata of videos belonging to four major political parties, as well as three distinct YouTube channels. Our results indicate that political ideology of viewers on these channels is significantly different from content creators. Further, we discover that only the content creators have higher level of political affiliation compared to viewers.
268. **PT-CoDE: Pre-trained Context-Dependent Encoder for Utterance-level Emotion Recognition**  
    We propose a novel multimodal neural network, named PT-CoDE, which integrates a sequence-to-sequence (seq2seq) decoder to effectively predict an utterance-level affective value. The motivation behind this work is the fact that, although the sentiments of human are observable from the utterance level, the sentiment of an utterance usually increases in the acoustic-space and decreases in the textual-space of a text document. In order to capture the characteristic of each modality, we introduce a number of models to capture the \emph{emotion vector} of an utterance. A significant challenge is to encode both the context and the emotion of the utterance into a single representation, as the human contains multiple modalities. To this end, we present an attention based decoder that enhances the contextual feature and also learns to encode the emotion vector of a sentence into a word-level convolutional network. Moreover, we propose to aggregate the contextual and emotion features into an output embedding space and feed it to a decoder with separate attention mechanisms. The experimental results on the five benchmark datasets demonstrate the effectiveness of our proposed PT-CoDE.
269. **MRQA 2019 Shared Task: Evaluating Generalization in Reading Comprehension**  
    In this paper, we describe the systems that are competing in the 2019 MARC-QA Shared Task. We provide a detailed description of these systems and elaborate on their reasoning components. We also report the evaluation results obtained by each system, and provide a score for each system, and a detailed analysis of each system's evaluation results.
270. **Grammatical Gender, Neo-Whorfianism, and Word Embeddings: A Data-Driven Approach to Linguistic Relativity**  
    Word embeddings have recently proven useful for improving NLP systems' performance in predicting correctly typed phrases. Here, we propose a simple method of deriving the factor vectors that best describe the gender of a word. The variables are determined by computing the "Breadth" of the factors when estimated in the form of a bipartite graph. Experiments on the LSTM-BOW model show that the variables associated with different languages (German, French) and genders (masculine or feminine) give considerably different ways of describing the same word. Empirically, the influence of gender on the prediction of word embeddings is larger in contexts in which the word is less frequently used. These results highlight a relation between language and gender that is rarely exploited by traditional NLP models.
271. **Fine-Tuned Neural Models for Propaganda Detection at the Sentence and Fragment levels**  
    This paper describes a set of deep learning models trained with a combination of knowledge distillation and transfer learning for detecting automated text manipulation. We present an approach based on Deep Speech Encoder that distills domain knowledge from corpus-based models and then fine-tunes the model with out-of-domain examples. Our experiments indicate that the approach can be effective. We also report a method to analyze the long-term effects of distillation on the outputs of a deep neural network.
272. **Content Removal as a Moderation Strategy: Compliance and Other Outcomes in the ChangeMyView Community**  
    In recent years, change My View (CMV) has become an active community in online communities. In contrast to other online communities, the post scheduling and moderation is a decentralized task, which demands flexible and expert judgment to ensure the safety and stability of the community. In this paper, we perform a study on the behavior of the change My View community, which has begun to evolve in a critical way: it has received tens of thousands of potential posters from around the world in the past year, and each poster usually generates hundreds of content to generate when using the mobile application Change My View. As a result, the number of posts generated are not evenly distributed. We observe a significant increase in the number of offensive, inappropriate, and illegal content generated by both unsupervised and supervised non-uniform manual and semi-unsupervised content removal methods, and we propose a new method, Adversarial Regression with Gaussian Process Regression (ARGPRG) to characterize and mitigate such behavior. A detailed analysis shows that despite the significant amount of content being generated, the decrease in the number of offensive and inappropriate content mainly comes from the contributions of forum users with their solid community participation skills.
273. **The Czech Court Decisions Corpus (CzCDC): Availability as the First Step**  
    The analysis of written and oral judgments is one of the central tasks for all legal judges, yet the technical and formal aspects of this analysis have largely lagged behind. The CzCDC database consisting of 658 judgments from Czech court cases (of different kinds of court cases) provides a rich source of data. The collection includes both the direct text judgments and judgments contained in a reasoning system, and it thus contains a tremendous amount of data for legal reasoning. We present the full text of the judgments collected in the CzCDC. The analyses done in this database can be applied to many legal problems and can also be used as a preliminary material for the statistical analysis of written and oral judgments.
274. **Improving Word Representations: A Sub-sampled Unigram Distribution for Negative Sampling**  
    Words are in general complex units of language, and their semantic relations are linguistically complex. Word embedding models, such as GloVe, attempt to learn the semantic relations between words by associating them to some vector space representations. However, learning such representations requires large volumes of data, which are often costly to obtain. We study negative sampling, the scenario when negative words are selected from unlabeled corpus. As negative sampling is usually far cheaper, it can be used to generate negative word embeddings in much less time than standard negative sampling. In this paper, we propose a new model called Sub-sampled Unigram Distribution (SUD), which constructs a zero-hot representations of unlabeled corpus for negative sampling. SUD optimizes the optimal sampling probabilities of negative word embeddings through a likelihood function with a feature vector. Experimental results demonstrate that the sub-sampled distributions are more favorable to negative word prediction compared to existing models in a number of evaluation datasets.
275. **Domain-agnostic Question-Answering with Adversarial Training**  
    Answer proposals have become the most effective way of addressing the problem of model overfitting in large-scale QA. In this paper, we propose a novel domain-agnostic question-answering model which leverages recent advances in adversarial training to derive the joint distribution of the multiple candidates in a question, thus ensuring that the final answer vector is domain-agnostic. Furthermore, we provide a new training strategy that exploits the disentangled representation learnt by the adversarial network to automatically select a few "real" candidates for the question-answer pair. We extensively evaluate our model on the standard benchmark of the CNN/Daily Mail QA Challenge 2017, where we show that the model not only attains strong performance compared to previous state-of-the-art methods, but also runs orders of magnitude faster.
276. **Diversify Your Datasets: Analyzing Generalization via Controlled Variance in Adversarial Datasets**  
    Deep learning methods for unconstrained visual recognition are known to perform well in high-dimensional environments, but fail in the presence of varied appearance, illumination and dynamics. To solve this problem, we observe that a variety of deep-learning architectures exhibit sensitivity to normalizing constant (NC) factors (e.g. image normalization, normalization onto coordinate axes, etc.). NC coefficients are a well known source of bias and variance for many computer vision problems. Since standard losses, such as the cross-entropy loss, ignore these sources, we propose a simple yet effective objective function, which we call the diversified weighted normalized cross-entropy (DWNCE) loss, for improving the generalization performance of deep networks for unsupervised classification. We evaluated the effect of various VC factors on both the classification performance and the bias-variance trade-off in a variety of unsupervised image classification problems, as well as other state-of-the-art deep network architectures, such as deep residual networks, CNNs, and LSTMs. Our results show that when normalized only via EC, or via the squared regression loss (e.g. cross-entropy, or L2 loss), the diversified weighted normalized cross-entropy losses consistently outperform or are comparable to those of the standard cross-entropy and L2 losses. We observe that when trained on artificial training datasets that include random non-random noise, the diversified weighted normalized cross-entropy losses consistently outperform the cross-entropy and L2 losses. We also show that when trained on image datasets with small perturbations, the diversified weighted normalized cross-entropy losses consistently outperform the conventional cross-entropy and L2 losses, with the exception of test set selection. Finally, we also present visualization of the diversified weighted normalized cross-entropy loss as it relates to the canonical cross-entropy loss for different loss-rates.
277. **On Automating Conversations**  
    Automatic Speech Recognition (ASR) has become an important technology in real-life situations. As the main task in ASR, it involves recognizing the human utterances. Many of the tasks such as interpretation, transcription, and machine translation are highly dependent on recognizing speech. As a speech recognition tool, ASR is an essential and interesting step towards developing a system to facilitate a long range speech understanding and a human-machine interface. In this work, we propose a fully automatic dialog system to simulate natural conversational dialogue with ASR. We explore different speech representations and speech recognition techniques. We find out that acoustic scene and ASR are two natural data sets which can be synthesized together to create a big dataset. Through the experiments, we discover that the ASR models are effective in both low-level and high-level speech models. Using the knowledge about the values of dialogues, we built a low-cost and fast high-quality ASR-RNN system that has a relatively low error rate. The dataset in this paper is freely available at http://phishing.inria.fr/home
278. **Localization of Fake News Detection via Multitask Transfer Learning**  
    Online fake news has brought tremendous challenges to public trust. Currently, most of the fake news detection methods are based on local criteria which do not consider the compatibility between the fake news and the real news. In this paper, we propose a novel multimodal model to learn the local news representation using a neural network and a regression technique. To further improve the performance, we introduce an adaptive attention mechanism to show better handling of out-of-vocabulary words and relevant sentences. Experiments on two public datasets show that our multimodal fake news detector outperforms many state-of-the-art methods, while the quality of local news representations also exhibits a significant improvement.
279. **Disambiguating Speech Intention via Audio-Text Co-attention Framework: A Case of Prosody-semantics Interface**  
    Automatic speech recognition (ASR) has attracted wide attention from researchers since 2000, with a new record of $11.45\%$ top-1 accuracy, achieving human performance of $54.04\%$ in the test set. ASR performance is further improved through further adaptation by deeper neural network architectures. In recent years, the attention-based model (ABM) has been widely studied for disambiguating linguistic intent. In this paper, we propose an attention-based ASR model to disambiguate speech utterances. The proposed model can predict prosodic and semantic segments of speech utterances using two attention sub-networks, one at audio stream level and the other at text level. In order to resolve the interactions between prosody and semantics, an audio-text co-attention mechanism is designed in which the soft labels of speech segments are co-attended by the text labels. In the experiments, the proposed model achieves an improved accuracy of $12.87\%$ over the state-of-the-art ASR model.
280. **Semantic Graph Convolutional Network for Implicit Discourse Relation Classification**  
    We consider implicit discourse relation classification and present an end-to-end framework, which is based on convolutional neural networks, for automatically identifying interrelated latent discourse relations. Unlike existing model-based models of implicitly discourse relation classification, we make no use of explicit discourse knowledge. Rather, we only exploit the contextual interactions in the text to extract relevant discourse features. By employing an auxiliary task for improving classification accuracy, our model can be optimized without requiring any discourse supervision. We evaluate our method on two large-scale corpora, in terms of both F1 score and classification accuracy. Experimental results demonstrate that our model significantly outperforms the current state-of-the-art methods, yielding a F1 score of 64.3% on a benchmark corpus, as compared to the previous best score of 35.3% obtained by a model trained using access logs.
281. **Diamonds in the Rough: Generating Fluent Sentences from Early-Stage Drafts for Academic Writing Assistance**  
    The use of computer-generated text in academic writing assistance systems is gaining momentum, especially for those tasks with very limited (often only a few) labeled examples. We present an approach for generating a text that better captures the style and mannerisms of the author. Our approach uses document collections collected from online education sites and illustrates how to use automatic, pre-generated links to identify potential potential sources of data. We describe the components of our approach, analyze the characteristics of our dataset, and evaluate it in two different settings. In both settings we demonstrate improvements of up to 10 points in average F1 for generated sentences compared to random, ground truth sentences, and up to 6 points in average ROUGE score for generated sentences compared to sentence-level supervision.
282. **KnowIT VQA: Answering Knowledge-Based Questions about Videos**  
    Visual Question Answering (VQA) has been an important research area in computer vision and natural language processing. Most existing VQA methods usually rely on a textual description of the visual contents, which are difficult to understand by simple methods. On the other hand, state-of-the-art VQA methods need to deal with complicated questions about the video, which are easy to answer by statistical methods. To address these problems, we propose a VQA model called KnowIT to learn to answer simple visual questions with the help of the visual information. We also introduce a knowledge-based question-answering model, which can provide effective and robust answers by transferring the learned knowledge to answer similar questions about other videos. We first collect a dataset of 2.5 million images and 5.7 million videos that contains thousands of questions for visual questions. Then we collect a benchmark dataset from the Visual Genome Challenge 2016 to compare our KnowIT model against state-of-the-art VQA methods. To the best of our knowledge, this is the first paper that proposes an end-to-end VQA model and introduces a novel knowledge-based question-answering method. Experiments show that KnowIT is superior to existing VQA methods in terms of accuracy and our knowledge-based question-answering model can even predict the correct answer for the questions it is asked about.
283. **A Search-based Neural Model for Biomedical Nested and Overlapping Event Detection**  
    This paper presents a simple and fast model, named Multi-Sequence Substring (MS-S) rule for unsupervised biomedical nested and overlapping event detection (NEED). Firstly, we follow the KB rule with a left-bank module to track substrings of the input text. Based on the tracking results, a new top-down rule, called MS-S compacter, is proposed for shortening the search space. Secondly, we identify and exploit the combinatorial effects of nested and overlapping events of WSJ news article and longitudinal CRF data in order to mine the hidden information in the event sequence. Thirdly, we further make use of Long Short-Term Memory (LSTM) to capture the topical structure of the data in a sentence and train the proposed MS-S compacter for best performance. Finally, we demonstrate the effectiveness of the proposed model in multi-criteria event detection. The experiments on three public NEED benchmark datasets show that the proposed MS-S compacter obtains the state-of-the-art performance, with a sensitivity of 95.33% and a specificity of 97.65%, which outperforms the other state-of-the-art methods.
284. **Kernel Graph Attention Network for Fact Verification**  
    A considerable amount of natural language inference tasks have been proposed, however the challenge is how to effectively extract knowledge and generalize from very few labeled training data. In this paper, we propose an effective framework for fact verification, which explores graph structures and word similarities among the context sentences to leverage a more accurate and robust feature representation. This framework exploits semantic semantic similarities between words, which guides the model to capture context semantic-meaning associations by modeling the relationship between context sentences. The above formulation brings a novel feature representation based on graph, which allows the model to model and learn contextual semantics, thereby alleviating the need for a lot of resources and converging faster than conventional networks. Experiments demonstrate the effectiveness of our method, and our hypothesis is validated through extensive experiments on Stanford Question Answering Dataset, HIVE and Prover8 Challenge datasets.
285. **Scalable Neural Dialogue State Tracking**  
    One promising approach to learning end-to-end dialogue state tracking models is by training on large amounts of context-aware dialogue data. However, dialogue state tracking is a highly non-convex problem that is inherently ill-posed. This paper makes a fundamental contribution to this problem by using the powerful structure of neural networks and constructing an effective neural dialogue state tracker (DST), which is more scalable than prior approaches. We introduce a novel and efficient way of combining multiple contextual information of dialogue turns, and optimize the entire DST on a sample utterance-based training data set. We also introduce a novel environment parameter which acts as a tradeoff between accuracy and scalability. Our DST achieves state-of-the-art performance on both ATIS and Switchboard benchmarks and scales up to the full scale Switchboard communication system.
286. **Improving Transformer-based Speech Recognition Using Unsupervised Pre-training**  
    In this work we address the problem of exploiting auxiliary learning signals to reduce the need for explicitly engineered features. The key challenge is to discover auxiliary tasks that are useful for improving the performance of the overall system. This is approached by training a fully supervised language model, which is pre-trained on labeled data. We propose two methods to reduce the signal to noise ratio of the model, one is a variational approach and the other is based on an auxiliary task discovery method in a recurrent neural network. Our experiments on TIMIT-2 show that the model learns new tasks that enhance the recognition accuracy for all 12 languages for which we make use of the auxiliary signals.
287. **Automatic Extraction of Personality from Text: Challenges and Opportunities**  
    To understand a person and discover their characteristics, humans first read the text they encounter. In spite of numerous papers on automatic text analysis, how to automatically detect the Personality (e.g., extraversion, neuroticism, etc.) of a given text remains a challenge, because most of the texts contain ambiguity, ambiguity is a common characteristic of human behavior. In this paper, we study the problem of personality detection in online discussion and present a set of experiments to show how to accurately extract personality from the text, taking into account the technical aspects and the required set of personality attributes. The developed methods are evaluated on a large-scale corpus and experimental results show that the method is effective to detect the personality traits.
288. **Transformer-based Acoustic Modeling for Hybrid Speech Recognition**  
    Recent advances in neural network-based acoustic modeling for hybrid speech recognition have lead to a promising set of automatic speech recognition models which have been shown to outperform conventional models. However, an open challenge is how to construct a family of models for hybrid systems that are both highly efficient and highly accurate. In this paper, we present a new family of hybrid acoustic models which share the commonality of hybrid acoustic modeling with attention-based acoustic models. The proposed model learns the relationship between the frequency of each segment of the speech signal and the recognition performance of the hybrid model. Experimental results on the standardized Hub5 dataset show the superior performance of the proposed model compared to an attention-based hybrid model and neural network models.
289. **Speech-VGG: A deep feature extractor for speech processing**  
    In recent years, deep learning has been applied to several important tasks in speech processing, including automatic speech recognition (ASR), automatic speech transcribing (ATS), automatic speech recognition (ASR-TA), etc. Despite the success, there is still room for improvement in most of these applications. One of the critical components of these tasks is the noise robustness. Noise removal is one of the most important components to develop a successful ASR system. We propose a deep feature extractor named Speech-VGG to address this problem. Unlike most of existing work in the field, we use multiple audio features to predict the noise level of each time frame. The key idea of this work is to decompose the noisy audio sequence into several components using Wavelet Transform and the cosine similarity is applied to each component. The learned features are then used to estimate the noise level of each time frame and then combine these estimates using Support Vector Machines (SVM) with different depth. The result of this system is called Speech-VGG. Our experimental results indicate that Speech-VGG achieves state-of-the-art performance. In addition, Speech-VGG provides an efficient means to apply various noise removal methods, such as Robust Principal Component Analysis (RPCA), Noise Contrastive Estimation (NCE), Noise Contrastive Registration (NCR), Zero-Shot Audio Denoising (ZSAD), etc.
290. **GPU-Accelerated Viterbi Exact Lattice Decoder for Batched Online and Offline Speech Recognition**  
    We propose a novel batched online and offline speech recognition system for the ubiquitous smart speaker. We use Viterbi equations to construct a matrix of cooccurrences of batched discrete events. To reduce computational time, we first train a cluster of random position-dependent Viterbi equations with an approximate batch size of one, then synchronize the cluster with a small random position-dependent matrix. By fusing the resulting matrix with a weighted average, we obtain a batched online model whose model size can be as low as 1. We use a real-world phoneme database for training the component models and establish that our proposed system achieves comparable recognition accuracy to the baseline system trained on the full database and by using a matrix of the normalized vector magnitude instead of a matrix of the normalized vector amplitude.
291. **One-Shot Template Matching for Automatic Document Data Capture**  
    Template matching methods are becoming more popular for generating and retrieving human-readable text data. However, most of existing template matching methods fail to satisfy users' needs of flexibility and productivity when it comes to web-scale data capture from social media, personal documents, or other types of Web 2.0 data. These challenges mainly relate to the fact that users' concerns about the missing elements in the data, i.e. spelling mistakes, lack of accuracy and consistency of retrieved data, can hamper the effectiveness of such methods. In this paper, we propose a simple yet effective method called One-Shot Template Matching (OSTM), which enables a template to be matched for a single captured snippet of text data. The OSTM employs a hierarchical LSTM which is able to capture the complicated sequential patterns in a text data by assigning attention weights at different levels. Using attention weights and tracking errors during testing, OSTM improves the retrieval accuracy by nearly 30% in terms of accuracy and 70% in terms of recall compared to previous template matching methods. We also conduct user studies on OSTM to evaluate the effectiveness of OSTM for different types of data and user population.
292. **Toward estimating personal well-being using voice**  
    One of the challenging problems in social science and psychology is to design models that can predict human behavior and can perform well when people are exposed to data from the task of interest. Due to the expensive and time consuming process of collecting data from the users, the use of automated speech recognition is becoming increasingly common. Research suggests that users sound less happy and satisfied if their voices are criticized. In this paper, we investigate a quantitative way to estimate the subjective level of satisfaction of voice using the audio signal. To measure the level of satisfaction, a suite of models are built and tested for classifying the audio segments. These models are trained and tested on two different data sets. For the first data set, we have already collected the speech recordings of one million users from more than 10,000 websites. The audio segments of the data set are split into three sub-segments: a time period of between two to five minutes, an emotion level over the twenty five minutes, and a subjective level over the following twenty four minutes. Our findings suggest that our approach based on AutoRec allows us to efficiently estimate the subjective level of satisfaction with the user voice at a level higher than individuals. We compare the results with existing automatic metrics that evaluate the overall audio quality. The objective comparison shows that, on average, our model with AutoRec yields an F-measure score of 0.85. We also study the accuracy of the individual models when applied to different languages. The experiment results indicate that the results in some cases are as good as that of humans when applying the model in the different languages.
293. **Transductive Parsing for Universal Decompositional Semantics**  
    We introduce a new decompositional logic, Disjunctive Logic Programming (DLP), which requires no {\em language}. Disjunctive Logics, including DLP, have recently attracted much attention in research in computational linguistics. The language model of DLP is relatively small and simple: it is closely related to the syntax of disjunctive normal form (DNF). Although an intuitively well-founded logic, the syntax is unparsable. For this reason, a rich encoder-decoder grammar is needed. In this paper, we propose a new grammar which requires rich encodings. It turns out that the proposed grammar is more parsable and expressive than the one used in the existing DP systems. A flexible error-checking scheme is needed to ensure that the correct outputs are produced by the parsing engine. We provide an error-checking algorithm and apply it to several languages of interest to us.
294. **Capturing Greater Context for Question Generation**  
    We investigate the correlation of visual context and question context in the task of question generation. To this end, we present a dataset, Visual Context, for unsupervised question generation that contains pairs of images that (1) are short, (2) both contain the object of interest (eg. a bird) and (3) contain the natural language expression in the form of a question (eg. "Is that person the famous singer?".) We evaluate our dataset on two datasets: a real world benchmark in the Open Movie database, and the online Java scene dataset. In both datasets, we obtain strong correlation between question context and visual context. When training models for question generation, the context model should capture visual context at a minimum, and one can then ask better questions by asking more appropriate visual context questions. For the Open Movie dataset, we also find that the model achieves significant improvements when using visual context. On the Java scene dataset, the model achieves better performance when asked more appropriate visual context questions. This finding makes sense: by asking more appropriate visual context questions, we are able to generate more interpretable questions that are more difficult to answer by human.
295. **Man is to Person as Woman is to Location: Measuring Gender Bias in Named Entity Recognition**  
    When we use search engines and other services to support our decision making process, as researchers in computer science, we have a responsibility to understand why search engines use features such as gender to categorize documents. This paper presents a data-driven approach for measuring gender bias in named entity recognition (NER) applications. Our approach is to develop "reference" entities such as teachers, teachers, students, police officers, or inventors, and then use their gender labels to train models. We evaluated our methods on five data sets: the New York Times, New York Times (NYT), Chicago Tribune, Associated Press, and LITIS Rouen. We used different statistical and machine learning models. For each of the five data sets, we used different combinations of statistical and machine learning approaches to assess the effect of gender bias.
296. **Improving Diarization Robustness using Diversification, Randomization and the DOVER Algorithm**  
    Diarization is an important textual data representation method which, if used correctly, can help solve many real-world problems. Many diarization methods have been proposed in the past, mostly based on handcrafted features and domain-specific knowledge. This paper explores two data representation methods that rely on these handcrafted features and knowledge, namely clustering-based and diverse-based methods. The former captures high level clustering features which are adaptive to every specific diarization task; the latter exploits the overall representation of the input data and the original feature space into a one-hot vector encoding. Both approaches work on unsupervised vector spaces for text data. The clustering-based method exploits the low-rank information in the unsupervised data and it further learns how to segment the data. The diverse-based method uses the data diversity and it progressively expands the vocabulary of possible unsupervised features. The experiments carried out on six different benchmark datasets for diarization from different domains show that both of the proposed approaches perform better than the state-of-the-art methods, under many different evaluation metrics.
297. **Low-Resource Sequence Labeling via Unsupervised Multilingual Contextualized Representations**  
    Sequence labeling has emerged as a challenging supervised learning task due to its remarkable capacity in recovering high-quality and meaningful predictions from utterances. However, applying conventional sequence labeling methods to low-resource language settings remains challenging, especially when only a small number of annotated examples are available in the target language. To alleviate this issue, we propose a Multilingual Contextualized Recurrent Neural Network (MC-RNN), where RNNs are learned to communicate with each other and fuse information from multiple languages. For example, during a sentence prediction process, a target language-specific RNN-based context encoder (e.g., Spoken Language Understanding Model) distills the input text from an RNN-based context decoder (e.g., statistical machine translation model) and then generates the corresponding prediction. On the one hand, MC-RNN is more suitable for low-resource language applications since it combines both the merits of language-independent multi-layer contextualized RNNs and the superiority of multi-source embedding strategies to realize cross-lingual information flow. On the other hand, it can be easily incorporated with existing sequence labeling methods, and its interpretability is highly beneficial for downstream NLP applications. To validate our methods, we conduct experiments on two popular low-resource machine translation tasks: Chinese-English and English-German translation. Our experiments demonstrate that our methods can be successfully used to achieve substantial improvements over various state-of-the-art sequence labeling methods for language modeling.
298. **Combining Acoustics, Content and Interaction Features to Find Hot Spots in Meetings**  
    There is a growing interest in incorporating biometric data into dialogue systems as it enables low cost, low call to action delivery. Past work on this topic has shown that integrating acoustic and contextual information can help improve accuracy and scalability of the systems. This paper introduces an acoustic-contextual embedding method for utterances extracted from video conference call recordings. The acoustic representation is formulated with a relative frequency expression of the speaker on the acoustic feature. The contextual embedding method extracts both the local context (temporal coherence and dynamic topic coherence of the conversation) as well as the global context (relations between the topic of the utterance and the associated words of the conversation). Through experiments, we show that using contextual embedding improves the matching accuracy from 75% to 86%. In addition, we demonstrate how using an end-to-end approach from acoustic to contextual embedding for eliciting human judgments improves the quality of the matching score by an average of 10% and eliminates up to 85% of false matches.
299. **GF + MMT = GLF -- From Language to Semantics through LF**  
    Within the field of Natural Language Processing (NLP), Gender Gender Disambiguation (GID) is often considered as one of the key NLP tasks, if not the most challenging, since it requires very fine-grained recognition of gender information. Currently, however, there exist very few systems which are able to achieve this goal. However, this is beginning to change with recent advances in multi-modal data analysis and visualisation techniques. In this paper, we explore the relevance of multi-modal fusion techniques in the context of gender recognition from language (GLF). Using several benchmark datasets, we investigate the effectiveness of different modalities in the context of GID. We show that GLF methods able to produce very accurate and precise gender recognition results using simple mono-modal data can be quite easily integrated with existing state-of-the-art multi-modal fusion methods (i.e. Multimodal Fusion Techniques (MFT)) and achieve comparable or even better accuracy results using more refined modalities such as audio-visual cues.
300. **A Transformer with Interleaved Self-attention and Convolution for Hybrid Acoustic Models**  
    While deep learning has been very successful for natural language processing tasks, most recent methods perform best when the amount of labeled training data is limited. However, it is widely known that different sentence structures and different acoustic modeling methods are beneficial for different tasks and that hybrid acoustic models can outperform their standard single model counterparts for certain tasks. The goal of this paper is to present a new method for model building that combines these ideas in a way that is specific to a particular task. The proposed method of self-attention deep learning utilizes a single attention mechanism, like the transformer, to reflect the long-term dependency in the utterance. The length of the attention is set at a desired range which reflects the difficulty of the sentence structure. To match the features of the words using the attention, the self-attention layer learns word representations that are the same length for all the words in the sentence. The word representations of the single attention layer are fed to a convolutional layer with a three-class fully convolutional neural network (FCN) to model the acoustic inputs of the sentence. In experiments on Stanford Sentences with Audio (SVA), the proposed model achieved superior results to the state-of-the-art deep learning models.
301. **Speech-XLNet: Unsupervised Acoustic Model Pretraining For Self-Attention Networks**  
    Recent advances in self-attention networks (SAN) have achieved state-of-the-art performance on both language modeling tasks such as RNN and phoneme recognition. In this paper, we propose an unsupervised acoustic model (UnSA) that learns the self-attention connection of acoustic models for end-to-end speech recognition. Unlike conventional self-attention models which condition the non-local contextual information on-the-fly and rely on a pre-defined prior, UnSA directly takes the acoustic history of each frame into account in an unsupervised manner. To improve the speech recognition performance, we propose a neural network called DNN-XLNet that consists of two deep neural networks: an encoder network and a decoder network. For the encoder network, we propose a bidirectional long short-term memory network (Bi-LSTM) to capture the long short-term dependency relationship between the multiple feature representations. For the decoder network, we jointly learn the contextual features of the current frame using a sequence-to-sequence (seq2seq) model. With the attention matrix learned from an unsupervised manner, we use a bi-directional LSTM for aligning feature representations to each other and to the contextual word embedding features, respectively. We evaluate the proposed approach on the Switchboard corpus with three different types of SAN architectures and different self-attention mechanism, and the experimental results demonstrate the effectiveness of UnSA.
302. **Controlling the Output Length of Neural Machine Translation**  
    Neural machine translation has shown promising performance in a variety of applications. Its output length in source-side has been a popular topic of study. In this work, we investigate the effect of removing output reordering of the neural machine translation model, in which the source side is divided into multiple sub-sub-spaces, and introduce the `unpredictable' sub-space setting, which is implemented by a differentiable random walker. We find that this simple modification leads to a translation error that is not only in the original sub-space, but also varies with the ordering of sub-sub-spaces, producing a `confusion' effect. To tackle this problem, we propose two approaches to increase the ambiguity of the reordering. One is a simple simple technique, the other is a data augmentation approach. The simple approach reduces the ambiguity by replacing the source side training data with the sub-sub-spaces, which is verified by running the model in `unpredictable' sub-space, but produces an improvement. The data augmentation approach increases the ambiguity by replacing the target side training data with the same sub-sub-space, which does not suffer from the same effects of confusion.
303. **Instance-Based Model Adaptation For Direct Speech Translation**  
    We propose a novel approach to tackle the speech translation task in this work. Instead of modeling an independent speech segment, as conventional approaches usually do, we adopt a weakly-supervised method to align a large amount of speech speech-to-translation pairs. By relying on a new training objective, which penalizes the mismatches of the aligned pairs, we are able to train a completely new sequence-to-sequence model with almost no prior training data. In particular, it is extremely easy to design our mechanism by proposing few rule-based candidate alignments. Furthermore, by relying on the proposed weakly-supervised method, our model outperforms strong baselines in terms of both perplexity and word error rate.
304. **Correction of Automatic Speech Recognition with Transformer Sequence-to-sequence Model**  
    Automatic Speech Recognition (ASR) systems often generate random speech transcriptions, or ASR training sets. To improve the effectiveness of ASR, it is necessary to prevent the human researcher from the necessity to transcribe the input speech. A solution to this problem is the use of Transformer as the encoder of ASR system. Transformer is a sequence-to-sequence model and it has the ability to model the natural sequence of sentences. Therefore, it can help to reduce the necessity to transcribe the input speech. However, the processing of ASR input speech by Transformer is challenging because the translation of ASR input speech consists of arithmetic expressions. To facilitate the ASR training and to mitigate the negative impact of arithmetic expressions, we propose the use of the Transformer Transformer architecture, which is a combination of BERT and Transformer. We introduce two conventional ASR training sets as an experimental validation for ASR correction and show that the proposed model can improve the performance of ASR by approximately 20%. In addition, we also test the model on a standard benchmark database and the results show that this model is able to effectively improve the ASR error rate by approximately 24% compared to conventional ASR system.
305. **Analyzing ASR pretraining for low-resource speech-to-text translation**  
    Deep neural networks (DNN) can automatically learn acoustic model for speech recognition without human annotation, which augments ASR capabilities for low-resource language pairs. Most of previous work on speech-to-text translation has focused on training a phrase-based Wavenet model by utilizing ASR paired data in the training phase. However, recent papers show that DNNs also learn and generalize better by pretraining. For example, recent ASR-based monolingual speech-to-text translation works pretrained on Bengali ASR and English ASR data. We test the effectiveness of pretraining in two low-resource settings, i.e., English-Spanish speech-to-text translation and English-German speech-to-text translation with machine translation, using a neural machine translation model with monolingual ASR target and ASR source speech corpus. We show that pretraining improves both test accuracy and language modeling for English-Spanish speech-to-text translation. When compared with the baseline models pretrained on a large amount of data without pretraining, pretraining also improves test accuracy on both English-German speech-to-text translation and English-Japanese speech-to-text translation. The source code of the experiment will be made publicly available.
306. **QASC: A Dataset for Question Answering via Sentence Composition**  
    This paper introduces QASC (QA-Semantic-Attention-Composition), a new large-scale dataset for question answering. The question consists of a semantic sentence, a natural language question, and a natural language answer. The dataset is automatically generated and consists of 5,250 sentences and approximately 8,000 different answers. The challenge of this work is to evaluate models with attention mechanisms, which have shown good results on modeling textual entailment. The dataset contains both natural language questions as well as human-written answers, and is manually annotated by humans. QA-Semantic-Attention-Composition is annotated as an auxiliary task for the QASQ2017 Shared Task. Moreover, QASC is of higher quality than the other QASQ Shared Task datasets that were released in the last year. It contains both grounded language questions and answers with natural language descriptions, which is more similar to human questions. Finally, this paper describes our approach to align question representations with corresponding natural language descriptions, and explains how we achieved state-of-the-art performance on the QASQ2017 Shared Task and in the broader context of natural language understanding tasks.
307. **A Unified MRC Framework for Named Entity Recognition**  
    Although named entity recognition (NER) is an essential task for many natural language processing tasks, only a few existing NER systems employ a unified framework that can simultaneously handle several types of NER tasks (e.g., named entity alignment, NER entity alignment for case-sensitive tags, and NER entity coreference resolution). For example, most existing systems leverage different methods for each type of tasks but none of them can utilize all of them. This paper proposes a unified NER framework named Stable Transducer (ST-Transducer) which performs end-to-end knowledge transfer and leverage all of the state-of-the-art methods under the assumption that the source and target corpora share a similar NER vocabulary. The proposed ST-Transducer utilizes NER techniques such as attention mechanism and parser-guided decoding to align source and target corpora in an unified framework. Moreover, the proposed ST-Transducer combines some hybrid methods of the two types of tasks in an iterative way. The experimental results have demonstrated that the proposed approach outperforms the baseline models by a significant margin.
308. **Generating a Common Question from Multiple Documents using Multi-source Encoder-Decoder Models**  
    Question answering (QA) systems have an important role to address the problem of the distribution of data available in human QA tasks, such as listening comprehension, reading comprehension, and writing comprehension. The best methods rely on the generation of a set of candidate documents in a document set to complete the target QA task. However, the generation of the target documents is considered a hard task and hence traditional text mining and text processing methods are not helpful. In this paper, we propose a novel approach to the question answering task using natural language generation models. The proposed method utilizes a multi-source text encoder-decoder model that can perform generalization on a large amount of documents while maintaining sufficient amount of noise. The evaluation results on the standard benchmark datasets show the effectiveness of our proposed method.
309. **L2RS: A Learning-to-Rescore Mechanism for Automatic Speech Recognition**  
    Automated speech recognition (ASR) is one of the most challenging research problems in the world of automatic speech processing. Many ASR systems have demonstrated the capability to be successful on large-scale datasets, but training an ASR system remains difficult. This problem has been linked to the possibility of forgetting, i.e. the lack of properly-correlated training data for an ASR model, leading to the mis-recovery of acoustic features. Many recent studies have studied a transfer learning approach to improve the robustness of ASR models to temporal context; however, the traditional R2RS approach assumes a straight-forward process of resuming the signal of interest. In this paper, we propose a learning-to-rescore mechanism that can be incorporated into the existing R2RS framework, allowing the realization of a straight-forward approach to resuming the signal. The resulting model is referred to as the R2RS-L2RS model. The L2RS-L2RS model has the capability of having its high-quality acoustic features retained even when the signal of interest is partially or completely lost, where previously, conventional L2RS models had been shown to be less robust. Experimental results on several datasets show that the proposed method outperforms standard R2RS and other state-of-the-art methods for real-time ASR.
310. **The SIGMORPHON 2019 Shared Task: Morphological Analysis in Context and Cross-Lingual Transfer for Inflection**  
    We describe the SIGMORPHON 2019 Shared Task, an international competition for automatic morphological analysis in context and cross-lingual transfer, which asked participants to predict the morphological relationships between two sentences. In this task, sentences can be of various morphological structures and the objective is to provide low-dimensional embeddings of such sentences, in the form of multilingual n-grams, that can be used for morphological analysis tasks. We present baseline models for English morphological analysis and evaluate the importance of different morphological information. Then we combine both embeddings in a new embedding space using adversarial training and compare the resulting embeddings with those predicted by our best performing model on English. We demonstrate the potential of multilingual embeddings and compare the embeddings to the ones predicted by our best performing model on French and German.
311. **Fast and Accurate Knowledge-Aware Document Representation Enhancement for News Recommendations**  
    Large-scale document representations have been shown to be effective for knowledge extraction. But for good recommendation, several models have to be considered in order to accurately capture the user's interests. For such models, an effective meta-model has to be learned to extract useful user information. However, previous works learned such models by deep autoencoders (DAE) without considering the fact that the users' interests vary according to different articles in a news item. This work proposes to model news item preferences and also enhance news item representations in a knowledge-aware way. Specifically, we focus on news recommendation on specific news news, or just news items, and propose a novel model called WSAN for this task. WSAN first embeds news item preferences into word vectors, which are then directly fed to a LSTM to capture the users' preferences in a high-level semantic space. Experimental results show that this approach outperforms state-of-the-art methods by 1.7% in terms of rank-1 accuracy and 4.3% in terms of F1 score.
312. **Attention Optimization for Abstractive Document Summarization**  
    Abstractive document summarization (ADS) is the task of automatically generating informative summaries. Current ADS techniques focus on incorporating external resources such as Wikipedia and news articles. These approaches have shown to be effective for topics such as computer science and sports. However, their applicability to other topics, such as philosophy and medicine, has not been explored. We propose a new attention-optimized mechanism for ADS that efficiently integrates external knowledge, and allows for both generation of informative and informative summaries. We benchmark our approach on two datasets of 15m and 10m words, and a third dataset of 6m words. We find that our method is able to substantially improve over several state-of-the-art ADS techniques.
313. **\'UFAL MRPipe at MRP 2019: UDPipe Goes Semantic in the Meaning Representation Parsing Shared Task**  
    In this paper we describe our submission to the MRP 2019 shared task on semantic meaning representation parsing. We used a new semantic representation which is based on a syntactic structure: regular expression regular strings. We defined a dictionary for mapping regular expression sequences to representations. We submitted a consistent, tight parsing model for the shared task.
314. **An Empirical Study of Efficient ASR Rescoring with Transformers**  
    Improving speech recognition (SAR) capabilities in low-resource settings remains a difficult challenge. In recent years, we have seen promising advances in domain adaptation via transformers, which can effectively capture the speaker and model context of a target domain without labeled data. However, less attention has been paid to the transformer-based methods in SAR and especially a synthesized data setting. In this work, we investigate two popular transformer-based approaches to efficient transformer-based rescripting of SAR, namely encoding and splitting. We analyze both of these methods empirically and compare their results with each other. Our extensive results suggest that the split-based approaches are significantly faster, compared with encoding, and produce better-quality rescripts than the encoding method. Although we present no attempt to understand which part of the spectrum causes the best performance in terms of rescript quality, we do find that their performances are related to the language modeling steps taken.
315. **ESPnet-TTS: Unified, Reproducible, and Integratable Open Source End-to-End Text-to-Speech Toolkit**  
    In this paper, we introduce ESPnet-TTS, a collection of deep learning based, lightweight end-to-end (TTS) text-to-speech (TTS) speech synthesis software. ESPnet-TTS is developed in an open-source manner and provides comprehensive reference for the further development and evaluation of TTS text-to-speech (TTS) systems, as well as a unified user interface which enables seamless general-purpose TTS synthesis. ESPnet-TTS is built upon a multi-layered convolutional neural network model that enables us to produce high-quality TTS. Through a large set of benchmark scenarios, ESPnet-TTS is thoroughly evaluated, achieving a Mean Inverse Error (MIE) of 7.4% on the VoiceTrans corpus, a score of 7.83 on the Mandarin Dialect corpus, a score of 14.67 on the TIMIT text-to-speech database, and a score of 17.10 on the Macquarie University dataset.
316. **Pun-GAN: Generative Adversarial Network for Pun Generation**  
    Automatic natural language generation (ANGL) is a hot topic in natural language processing and has a lot of application in many fields. Most of the existing methods try to make use of Generative Adversarial Network (GAN) to generate a sentence given a target word sequence, thus it takes a lot of time to train the model and the quality of generated sentence is not good. In this paper, we propose a novel deep generative adversarial network (Pun-GAN) to address the problem. A new GAN is proposed to generate a random sequence of Pun characters at each word position. Furthermore, the introduced GAN is able to learn the relationship between words in a sentence to generate better sentence by eliminating noise which is in the language model. Experimental results of Pun-GAN show that the proposed model generates better sentences than the existing method in terms of both semantic similarity and content similarity for multiple datasets.
317. **Syntax-Enhanced Self-Attention-Based Semantic Role Labeling**  
    Semantic Role Labeling (SRL) aims to assign a class label to an input utterance and it has been used extensively in a number of tasks, including speech and textual question answering. Recently, deep learning based SRL has been achieved promising results. However, most deep learning based SRL methods are simpler than a traditional SRL model with a single input word per class and sentence segment, resulting in poor performances on complex questions. In this paper, we present a syntax-augmented self-attention based semantic role labeling method. To make this model interpretable and learn useful representations, syntax annotations are provided for all available training data and knowledge sources. The proposed method achieves state-of-the-art performance on standard SRL benchmark datasets, surpassing previous methods that relied on pre-trained neural network models with a large number of training instances. In addition, by jointly using syntax and memory network encoders, the proposed method is able to achieve faster training speeds.
318. **Promoting the Knowledge of Source Syntax in Transformer NMT Is Not Needed**  
    The training process of neural machine translation (NMT) is usually accelerated by incorporating auxiliary signals which include syntax in the form of phonemes. In this paper, we investigate the use of syntax for improving generalization performance of NMT. Our experiments show that syntax can lead to a substantial increase in BLEU score, especially for low-resource languages. However, we demonstrate that augmenting the auxiliary language information by syntactic patterns only boosts the BLEU score only moderately. On the contrary, the enhancements in beam score of an ASR system are slightly higher. Our findings motivate an alternative approach to improving generalization performance, which is not evident from the enhanced beam scores.
319. **Recognizing long-form speech using streaming end-to-end models**  
    End-to-end speech recognition, which directly models acoustic and language modeling, has recently achieved new state-of-the-art performance. In this paper, we present a new dataset and novel evaluation method that enables the analysis of end-to-end speech models. We annotate 10,150 utterances on the Wall Street Journal (WSJ) corpus and use this annotated dataset to compare two recently proposed streaming end-to-end models, which are based on multitranslatron and mixed-words models, and a standard end-to-end model. We also compare their performance with respect to conventional end-to-end models. We find that the standard end-to-end models outperform the multitranslatron model in almost all settings. In particular, the mixed-words model provides an average word error rate (WER) reduction of 11.1% on the WSJ dataset compared to the end-to-end model, and an average relative WER reduction of 7.6% compared to the multitranslatron model.
320. **Wasserstein distances for evaluating cross-lingual embeddings**  
    Existing cross-lingual embedding evaluation approaches fall into two main categories: shallow and deep. Deep evaluation methods, especially WGANs, approach this problem via a supervised learning framework. However, most approaches to evaluate cross-lingual embeddings for word similarity tasks are based on Wasserstein distances. Therefore, the popular Wasserstein embeddings in WGANs have recently attracted attention due to their high efficiency of approximating Wasserstein distances. In this paper, we generalize the Wasserstein distances as the analysis of a large family of multi-scale divergences by establishing a new family of convergence properties for such distances. More specifically, we propose new convergence guarantees for an arbitrary nonconvex Wasserstein distance. We also propose a new initialization scheme of the Wasserstein embeddings. Experiments on several large-scale embedding datasets show that our proposed method achieves state-of-the-art performance on word similarity tasks, especially on semi-supervised tasks.
321. **Multi-Document Summarization with Determinantal Point Processes and Contextualized Representations**  
    We introduce the first document-level ranking model that directly predicts document-level rankings using Determinantal Point Processes (DPPs). The DPP model is trained on a dataset of documents in order to extract DPP probabilities from their documents, which are then used to create a document-level ranking model. We use a recently developed contextualized representation method to encode both the document content and context, and show that this encodes very well into multi-document summarization. The model obtains an overall ROUGE score of 83.9 on the MEDTest-2 dataset, outperforming both single document models. Furthermore, we investigate contextual factors that affect the ranking, such as one-shot content embedding for "hundreds of" documents, and show that a document's own document-level ranking does not correlate with contextual information. Finally, we analyze the internal representations learned by the DPP model, showing that it is capable of accurately predicting whether a document is relevant, using the topics it contains.
322. **Comparison of Quality Indicators in User-generated Content Using Social Media and Scholarly Text**  
    Social media has become an invaluable source of information about the Web-users. Moreover, these data can be further utilized to provide valuable insights and help enrich knowledge. A number of studies on quality assessment of user-generated content such as reviews and blogs are reported and analyzed in the literature. However, the quality evaluation of user-generated content is still missing in literature. In this paper, we study the effectiveness of using four types of metrics for quality assessment of user-generated content, namely similarity, coprorization, concreteness and low rankness. We compare the quality scores obtained using the different metrics to establish the potential of using these metrics in different study applications. We analyzed a corpus of 98,000 Web pages and 53,000 reviews to show the effectiveness of the four quality measures for content quality assessment. We evaluated the quality scores of user-generated content by comparing the four quality metrics in terms of rank loss, volume loss, concreteness loss and score gap with an input-evaluation interval of $[1,2]$.
323. **Cross-Lingual Vision-Language Navigation**  
    This paper presents a cross-lingual solution for the vision-language navigation task. Our approach consists of building a robust environment mapping from monolingual images to fine-grained multi-modal sentence descriptions. For a target image pair, our system extracts semantic features of the dialogue agent that are fed as inputs to the map, and then constructs a multi-modal scene description with a language model trained by a deep convolutional neural network. A qualitative and quantitative evaluation shows that the proposed method can handle the following types of interactions between the vision and language modalities: recognition of key strokes, speech key-strokes, and multi-modal context.
324. **Detecting gender differences in perception of emotion in crowdsourced data**  
    This work proposes a method for the detection of gender differences in perceived emotion in crowdsourced data. Gender based emotion detection can be viewed as an alternative way of analysing the emotion expressed by the crowd, in which emotions are identified by contrast to the demographic characteristics of the crowd. Crowdsourcing platform have become a useful medium for the validation of affect recognition by the crowd, since it allows the recognition of emotions that are not possible to observe by observing the expressions of individuals in isolation. The prediction of future emotions of a crowd based on perception of the current emotion and an emotion intensity are studied. The proposed method was evaluated with two datasets (an emotional speech emotion recognition and a sarcastic speech emotion recognition) and compared to state of the art techniques. Experiments show the robustness of the proposed method, and further investigations are presented to assess the temporal dependencies of the proposed model.
325. **Development of Clinical Concept Extraction Applications: A Methodology Review**  
    Concept Extraction is the process of identifying, parsing and recognizing linguistic expressions and phrases in documents. Its aim is to provide a structured representation of the meaning of the document in order to assist in analysing and understanding its content. In this work, we aim to investigate the development and applications of Concept Extraction approaches in clinical practice. We provide a methodological overview of Concept Extraction based on the results of a small number of research studies on Concept Extraction methods using clinical texts. Our work is an attempt to provide a high-level overview for researchers and practitioners who are involved in Concept Extraction of the general approaches and practices developed and adopted in clinical practice. The study of Concept Extraction methods is hampered by the complexity of clinical practice, the multidisciplinary nature of Concept Extraction methods and the unavailability of large-scale and consistently annotated corpora. In this work, we discuss the research studies that have assessed the development of Clinical Concept Extraction approaches in clinical practice. We further discuss the existing methods that are currently being developed and apply them in the task of Concept Extraction analysis and application. We cover recent performance measures that have been proposed to evaluate the performance of Clinical Concept Extraction approaches.
326. **SpeechBERT: Cross-Modal Pre-trained Language Model for End-to-end Spoken Question Answering**  
    While Natural Language Understanding (NLI) models have achieved outstanding performance in predicting natural language answers, leveraging the speech interaction to improve the quality of the answer remains a challenging problem. Recently, extensive research efforts have been devoted to training neural models that jointly predict the language (e.g. text) and question (e.g. paraphrase) by leveraging multimodal data. Although these approaches have made significant progress in leveraging data from both modalities, the lack of multimodal contextual information hinders the explicit modeling of the overall interaction process. In this paper, we present SpeechBERT, an end-to-end pretrained language model that achieves a 7.2% improvement on BLEU over the original SQuAD model. Extensive experiments conducted on a large multi-modal data collection (MUC-R$^2$) demonstrate that it is indeed possible to use the above-mentioned neural networks and multimodal data to achieve improved performance.
327. **Yall should read this! Identifying Plurality in Second-Person Personal Pronouns in English Texts**  
    Personal pronouns (e.g., you, your, their) can be used with sentences where the sense of the person is not obvious (e.g., attributions). A number of researchers have studied this problem for the English language, typically by comparing languages of similar syntactic complexity or nominal systems. However, we analyze human data and find that there are cases in which a non-person can be used with sentences containing a single person, as long as the person pronouns are correctly labeled. We demonstrate that existing methods for distinguishing between non-person and person pronouns require large amounts of resources and not suitable for deployment on resource-constrained systems. We propose a simple way of identifying a non-person pronoun without using any resources and without modifying any language modeling. We test the approach on a wide range of second-person personal pronouns extracted from English Wikipedia, demonstrating improved accuracy over several baselines and suggesting that the approach may be useful for identifying non-person pronouns.
328. **FineText: Text Classification via Attention-based Language Model Fine-tuning**  
    We present FineText, an end-to-end neural text classification model that utilizes attention-based language model fine-tuning to boost the text classification performance. The proposed model is equipped with a high performance encoder-decoder network (the encoder) and a new attention model (the attention), both of which are seamlessly integrated into a common hidden layer. We empirically show that fine-tuning the encoder based on unlabeled text data increases the model's performance and yields better performance for text classification tasks. Extensive experiments show that FineText outperforms state-of-the-art supervised text classification models.
329. **Automatic Reminiscence Therapy for Dementia**  
    Research on the diagnosis of Alzheimer's disease continues to advance, but its course is unclear. It is now clear that despite effective treatments for earlier stages, very early detection can limit recovery. Reminiscence therapy can potentially be used to reduce the risk of Alzheimer's disease by identifying vulnerable participants and restoring them to their pre-training level. The vast majority of research in this area has focused on patients with mild cognitive impairment (MCI). Despite obvious importance in such a setting, extensive clinical studies on MCI have been conducted only in a relatively small cohort of MCI patients (n=12,827) and only one study with 15,000 participants was included in the 2016 Alzheimer's Disease Neuroimaging Initiative (ADNI) Neuroimaging Highlights publication. In this paper, we conduct the largest, population-based study of reminiscence therapy for MCI. We describe our methodology and methodologies for data collection, study design, participants, and outcome measures. We also present the data preprocessing pipeline, evaluation protocols, and results of the main analyses we performed on the ADNI dataset. Our results suggest that randomized controlled trials with live participants are highly unlikely in the early stages of reminiscence therapy. To our knowledge, this is the largest and most comprehensive population-based study of reminiscence therapy for MCI.
330. **Exploring Author Context for Detecting Intended vs Perceived Sarcasm**  
    Most sarcasm detection systems assume that the two comments are written by the same author. In this paper, we present an approach to detecting sarcasm by considering the context of both the authors' comments and the reader's expectations. First, we extract features based on both the authors' comments and the reader's expectations from a corpus of comments. We then compare the extracted features with the sentiment values of the comment text, in order to detect sarcasm. This paper presents experiments with four popular sarcasm datasets: the Chat Zone (Deleuze et al. 2011), MovieLens Captions (Brigden et al. 2014), and the Facebook Comment Captions (Emerson et al. 2010). The comparison results show that our proposed method can detect sarcasm well.
331. **Current Limitations in Cyberbullying Detection: on Evaluation Criteria, Reproducibility, and Data Scarcity**  
    This paper attempts to shed light on why current, highly successful cyberbullying detection methods cannot sufficiently solve the issues addressed in earlier research and limitations of current, relatively young and thus unexplored datasets for this important yet unsolved problem. To this end, we investigate the evaluation criteria, reproducibility, and data scarcity problems, and illustrate how these concerns have severely compromised current state-of-the-art, in terms of missing datasets, too brittle evaluation metrics, and lack of sample-and-science comparisons. To facilitate discussion, this paper also introduces an open-source toolkit for benchmarking cyberbullying detection methods, which can be used to provide concrete insights into the limitations of current systems, as well as propose ways to overcome these issues. In addition to evaluating current state-of-the-art methods, we focus on describing our experience with developing CyberbullyingToolkit, comparing its performances with those of other state-of-the-art tools, as well as exploring related opportunities in comparison to future research.
332. **Towards Online End-to-end Transformer Automatic Speech Recognition**  
    Over the last few years, end-to-end sequence-to-sequence models have been increasingly used in Automatic Speech Recognition (ASR). We investigate an end-to-end sequence-to-sequence neural network which uses a sequence of mini-batches to compute an attention mask on top of the original input sequence. We make two key contributions: first, we show that the network is able to learn the attention mask on its own and does not require any annealing steps for final word prediction. Second, we present a new model for end-to-end ASR which leverages such an attention mask to transform the original input into a compressed representation which is fed to a standard transformers for the final word prediction. This approach is less complex than traditional ASR systems and less computationally intensive as it only adds to the decoding time at the end of the network. Our model achieves state-of-the-art results on the Switchboard Switchboard and the Voice Search WMT 2014 dataset. It also outperforms previous methods in simplified bilinear log-speech and phoneme-based features, on both WMT 2014 and 2014 En-De sub-datasets.
333. **Evaluation of Sentence Representations in Polish**  
    Semantic role labeling is one of the core requirements for tasks such as sentiment analysis and paraphrasing. In this paper we present a lexicon based on monolingual polarity labels obtained from Wikipedia and an English machine translation system, and use these to train sentence embeddings and to evaluate their effectiveness for the task of sentiment classification. We also test the performance of various neural architectures and different document-based representations. Our best performing model achieves a moderate F1 score of 81.9% for sentiment classification. We note that performance results of different methods vary significantly: for example, English-Polish emotion-word embeddings provide marginal gains while making minor mistakes on this task.
334. **Measuring Conversational Fluidity in Automated Dialogue Agents**  
    Automated Conversational Artificial Intelligence (ACA) has seen a huge increase in research attention in recent years, in particular due to the increasing availability of data and computational resources. While the general goal of A/B testing for collecting customer feedback in conversation agents is to find the best possible response, when there is additional complexity involved it is important to identify the level of fluency or fluency in the response, that better suits the conversational needs of the agents. As conversational fluency is critical to business success, this work uses an open dataset, (current version, version 1.0), as a testbed to study fluency in an open-domain personal chatbot for online classroom instruction. The first contribution of this work is a baseline dataset consisting of the responses from 200 Conversational Fluency Assessment exams provided by the interactive messaging service company Skrill. We then introduce a novel approach to measuring conversational fluency that draws on a hybrid model that leverages data and machine learning (ML) algorithms for measuring fluency in other, related datasets. To compare the efficacy of our method, we benchmark the proposed approach against those of three of the best published methods and find that both the proposed approach and the selected approaches yield results comparable to the best published methods, albeit with improvements coming from our proposed approach in conversational fluency. We conclude with a detailed analysis of the proposed approach and interpretable exploratory analysis of the datasets and the ML algorithm that produced them.
335. **DENS: A Dataset for Multi-class Emotion Analysis**  
    Emotions are widely studied in psychophysics and psychology. Besides measuring human affect, emotion analysis can also help to investigate some types of situations and predict future events in human behaviors. However, emotion analysis is mainly focused on quantitative data. The majority of existing emotion databases, however, are of a single emotion class (e.g., happy) and do not consider the variability in emotion types (e.g., anger, happiness, sadness). To address this problem, we develop a multi-class emotion database containing 52 diverse emotion categories, including happy, angry, sad, surprise, surprise, fear, disgust, joy, and sadness. This database includes face images collected from 16 different emotions. Using this database, we propose a multi-class emotion detection and classification system that makes use of three relevant measures: (i) mean absolute error, (ii) prediction of emotion categories, and (iii) distribution of emotion categories on the emotion scale. The experimental results on the multi-class emotion database and the trained classification model indicate that it can outperform previous emotion detection algorithms on various standard evaluation metrics.
336. **Stem-driven Language Models for Morphologically Rich Languages**  
    We propose a novel stem-driven language model for morphologically rich languages. In contrast to the traditional language model in which the decoding agent generates an initial random word and then switches to a different sequence to the target word, we introduce a new model that generates several plausible language sequences. We propose a classifier that leverages the plausibility scores of the generated language sequences to select the right word from the language model. We demonstrate that our model improves the state-of-the-art morphologically rich language modeling approaches in two evaluation tasks: the MTExT corpus and a 3rd-grade speech recognition task.
337. **Meta-Learning with Dynamic-Memory-Based Prototypical Network for Few-Shot Event Detection**  
    An event detector is a powerful detector that is capable of quickly detecting new classes of events, such as speech, scene, and illumination changes, with few labeled examples. The existence of such detectors are limited to a small number of predefined event classes and supervised learning methods have been proposed for few-shot event detection. This paper proposes a novel event detector, named dynamic memory-based prototype network (DMMP), to learn event detection from a large number of no-shot and few-shot events. Specifically, we propose two different strategies to construct the DMMP, and then utilize a dynamic memory-based prototype network (DMMP-D) to learn the event class-specific characteristics from the few-shot event classes. Experimental results on two benchmark datasets show that the proposed DMMP can achieve the state-of-the-art results compared to previous methods.
338. **Sentiment Analysis for Arabic in Social Media Network: A Systematic Mapping Study**  
    Due to the advent of social media, people do not only express opinions based on social and economic issues. There is also an increasing demand for online opinion mining and opinion ranking systems to evaluate and predict the opinions expressed by users. In this paper, we present a data-driven automated system, based on Natural Language Processing, for Arabic sentiment analysis, based on tweets. This approach is applicable for various Arabic language communities and news media. We examine and compare our new system using different sentiment evaluation corpora. The framework includes sentiment classification, sentiment polarity classification, sentiment level prediction, and opinion ranking. To this end, we investigate different classification models, word embedding models, and sentiment analysis techniques. We also propose a set of new sentiment analysis models which can be used to rank the tweets. We show how using these models can help us rank the sentiment of tweets in Arabic language communities. Our proposed models are trained and tested on Arabic and English language communities on several datasets.
339. **Meta Learning for End-to-End Low-Resource Speech Recognition**  
    In this paper, we propose a data-efficient end-to-end neural speech recognizer based on a self-attentive hierarchical model. The main contribution of this work is a careful analysis of the theoretical properties of the proposed model. To the best of our knowledge, this is the first work that studies end-to-end neural speech recognition from end to end. Experimental results show that self-attention-based hierarchical models are very data efficient, achieving almost 80\% WER with 2 hours of training data. We further conduct ablation analysis to reveal the success of the proposed end-to-end framework. Our method is evaluated on the IEMOCAP low-resource speech recognition corpus. It achieves 3.3% WER. In the 2-hour IEMOCAP test, which is not even close to our previous results, the proposed end-to-end model outperforms a self-attentive hierarchical model by 5.3%. We believe that the proposed end-to-end model will have a very important impact on end-to-end speech recognition in low-resource environments.
340. **Latent Suicide Risk Detection on Microblog via Suicide-Oriented Word Embeddings and Layered Attention**  
    Automatic suicidal ideation detection in social media is a challenging task since the communication format in social media make users available for others to contact or tweet, calling for easy-to-interpret language features. Additionally, it is vulnerable to deceptions in language engineering used by suicidal ideators to hide their true intentions. In this paper, we propose a suicidal ideation detection task to build a new lexical suicide word embedding from public online suicide language. In the experiments, we trained two embedding methods on our new suicide language. The experiments are done on suicidal thoughts dataset and experimental results show that the suicide word embeddings based on suicide language outperform the lexical suicide word embedding.
341. **ViGGO: A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation**  
    Generating detailed text is a challenging task for both natural-language processing (NLP) and machine learning (ML). In this paper, we propose a large video corpus to assist the development of automatic generation systems in the domain of open-domain conversations. For each frame of the video, we automatically identify and generate natural-language captions using a convolutional neural network (CNN). The automatic captions can be used to guide the generating process, and improve the overall quality of generated texts. The initial corpus was produced by systematically gathering video snippets containing the dialogue with annotators and speaker identities provided by Mechanical Turk. The second part of the paper is devoted to create a more generalizable video dataset, i.e., ViGGO, which will be a better baseline for text-to-video generation. ViGGO has close-ended answers to the generation questions, which have a fixed sentiment. Further, this dataset has more comprehensive information about the human participants, such as the questions they asked, their position on the dialogues, the duration of the dialogues and their actual utterances. To test the utility of the newly generated corpus, we leverage an automatic quantitative evaluation protocol on the proposed dataset. The experimental results show the effectiveness of the proposed dataset and validate the effectiveness of the CNN models. Moreover, a qualitative analysis of the dialogues in ViGGO demonstrates the usefulness of the more comprehensive information.
342. **Memeify: A Large-Scale Meme Generation System**  
    Over the past decade, the internet has evolved from a safe and friend-to-friend network to a dangerous and hostile one. Online harassment has become a large-scale problem that affects millions of people everyday. With social media becoming more and more popular, there is a need to devise effective tools for automatic detection of these online threats. In this work, we present a Memeify system that exploits Twitter to perform content-based detection of cyberbullying. To the best of our knowledge, Memeify is the first work that addresses content-based cyberbullying detection in real-world Twitter data. We demonstrate the utility of our system on a new dataset that consists of about a million tweets with associated tweets annotated for the presence of cyberbullying. Our model yields highly accurate results for both semi-supervised and no-supervised feature learning methods.
343. **Induced Inflection-Set Keyword Search in Speech**  
    We develop a version of Word2Vec for Automatic Speech Recognition (ASR), Induced Inflection-Set Keyword Search, which uses a simple technique of augmenting Word2Vec word embeddings with an augmented probability distribution over the inflected stems of word sequences. In an evaluation of 14 languages, our approach improves word recognition rates of F-1 by 12.6% for machine-translated input, when compared to standard "pseudo-word" reconstruction with an equally-sized word representation.
344. **Attention-Gated Graph Convolutions for Extracting Drug Interaction Information from Drug Labels**  
    Neural networks (NNs) are a powerful tool for classifying and processing data, but the challenge of model inference requires significant time and resources. We show that Deep Graph Convolutional Networks (DGCNs) are effective for aligning an input molecule and its target molecule by learning an implicit transformation from molecules to molecules and vice versa. The transformation allows us to capture the chemical or physiological relation between the two, even without the input of drug labels. Then, a DGCN is trained to identify the drug-target pairs that are likely to have the same molecular-chemical properties, and for these pairs, only a subset of atoms are predicted to be relevant for drug-target interactions. We apply our approach to the task of drug response prediction on a subset of the Complete Genomics dataset, and show that DGCNs are able to perform as well as state-of-the-art models with human-in-the-loop label predictions. Finally, DGCNs have been applied to extract semantic drug-target interactions in the REPEACH database.
345. **Unsupervised pre-traing for sequence to sequence speech recognition**  
    Unsupervised pre-training of a neural network model for speech recognition in raw speech is an important problem for future applications in both human-computer interface (HCI) and speaker verification (SV) systems. In this paper, we present the first method to perform unsupervised pre-training of an encoder-decoder model with the goal of improving its performance by a substantial amount (35.71% relative gain) without explicitly considering a target rate in the model training. We also provide an easy way to perform such unsupervised pre-training using a novel pre-training step. We assess our method on the Wall Street Journal (WSJ) dataset, i.e., utterances at 0, 1, 5 and 10 seconds in length. Experimental results show that (1) the proposed method significantly improves the unsupervised pre-training ability of a recently proposed model, i.e., the Convolutional Recurrent Encoder-Decoder (CRED), with 11.5% relative gain over the baseline baseline; (2) pre-training without target rates helps the model train faster as compared to pre-training with target rates, and also improves the model generalization capability, as compared to the CRED model. This promising preliminary research opens up many potential future research opportunities and research directions in real-world applications of unsupervised pre-training for speech recognition in raw speech.
346. **What does BERT Learn from Multiple-Choice Reading Comprehension Datasets?**  
    We investigate BERT's ability to learn from multiple-choice reading comprehension datasets with attention constraints. We observe that BERT's performance in this task is significantly lower than it was on the in-domain data alone. We analyze this and show that in addition to impairing performance on this task, BERT systematically learns to predict what could be included in the question for providing an answer. We propose an analysis of a subset of the BERT provided as pre-training on Stanford Question Answering Dataset. Based on this analysis, we propose a new definition of a question sentence that requires the answer to be a valid answer to an open-ended question, while not making any specific task assumptions about the question. By adding a consistency condition to the pre-trained BERT model, we find that in addition to performance on the existing target questions, BERT learns to predict question-specific answers. Finally, we train and validate a full-fledged model on this dataset that successfully completes multiple-choice comprehension tasks and obtains results competitive with models trained from scratch.
347. **Effect of choice of probability distribution, randomness, and search methods for alignment modeling in sequence-to-sequence text-to-speech synthesis using hard alignment**  
    In this paper, we experiment with the adaptive soft alignment model, namely model adaptive soft alignment (MAST), for text-to-speech synthesis. We introduce a fixed-size dictionary containing 85% of its words. An alignment term is generated by minimizing the loss over alignments of alignments of generated words, which is similar to the conventional sentence-level alignment term in text-to-speech systems. Model parameters, soft alignment parameters, and a pre-trained LM language model are obtained by data augmentation, randomization and hard alignment search. In the experiments, the empirical results show that the uniform sampling alignment results in the same performance compared with the fixed-size dictionary-based alignment model. Experiments using different alignment algorithms and languages indicate that the MAST model is more efficient than the conventional alignment models when the input language and the alignments of generated words are non-uniform.
348. **Multitask Learning For Different Subword Segmentations In Neural Machine Translation**  
    It is challenging to generalize a neural machine translation system from a limited corpora to a multilingual set of corpora due to the numerous features to describe each language in the set. To alleviate this problem, we present a multitask learning framework to encourage the neural machine translation system to use its knowledge of the structure of a source language in learning subword segmentations, and only use the source part of a sentence to construct word embeddings in a target language. Experiment results on various languages and corpora show the effectiveness of our multitask model. In addition, we also show that our model can produce multi-word segmentations that are closer to the natural language output when compared with an unsupervised system, while achieving similar segmentation accuracies.
349. **Transformer-Transducer: End-to-End Speech Recognition with Self-Attention**  
    In this paper we introduce a new paradigm of end-to-end speech recognition using self-attention mechanism. A data unit is first composed of two units, self-attention and encoder. The self-attention unit (SAN) is a bottom-up attention that attentively generates the hidden representations in the encoder. There is no speech segment information to be fed to SAN. After a speech segment is complete the SAN is composed of two residual blocks (RG and RGB) with a post-processing post-frequency and a spatial attention, thus it can capture self-attention. In addition, the self-attention is spatially weighted across all the positions of a word by adopting the self-attention for a word. The self-attention units are integrated in the encoder and the decoder of the neural network. Therefore, the self-attention and the sequential decoder become the utterances in a long speech sequence. We show that the proposed paradigm, named Transformer-Transducer (T-Transducer) is able to achieve significantly higher accuracy than strong baseline systems for neural network based end-to-end speech recognition.
350. **A Hierarchical Location Prediction Neural Network for Twitter User Geolocation**  
    Although the extensive popularity of microblogging websites such as Twitter has been witnessed with the rise of social networking sites such as Facebook, the social footprint information has been largely ignored by the related user interfaces in these social networks. With the growing amount of geolocated data that is generated daily and available to both users and social networks, twitter user geolocation has gained increasing attention from researchers and social scientists. In this paper, we proposed a new Hierarchical Location Prediction Neural Network (HLPN) for tweet geolocation to provide the Twitter users better location prediction capability. Specifically, the HLPN utilizes the recurrent neural networks for spatio-temporal representation of input tweet sequences. More importantly, a novel bidirectional attention mechanism is introduced in the HLPN architecture to selectively pay attention to the most important tweets and extracts the most useful features. Finally, three different types of hyperparameters are investigated for improved performance on our Twitter geolocation dataset, resulting in the state-of-the-art performance on this benchmark dataset.
351. **Sketch-Fill-A-R: A Persona-Grounded Chit-Chat Generation Framework**  
    We present Sketch-Fill-A-R, a persona-grounded conversational agent which learns to ask questions about real-world images. We ask professional photographers to discuss, and answer, photographs of their own work. To learn to ask these questions, we frame the task as a tag-set learning problem and train a triplet classifier to discriminate the query posed by the agent. To generate new conversation questions, we learn a generative module that applies multi-head attention to high-level semantic features of images. This module enables the agent to combine domain-knowledge with low-level visual representation to form its answer. We demonstrate that our framework learns to detect the contextually relevant questions, and to generate responses accordingly. We evaluate Sketch-Fill-A-R on a new dataset of approximately 700 photographs of professional photographers from the Flickr30k Entities dataset. Results show that Sketch-Fill-A-R is capable of learning to ask images that are explicitly related to the answers and can achieve state-of-the-art performance in the task of photo-driven query answering.
352. **Evaluating the Factual Consistency of Abstractive Text Summarization**  
    In this work, we are interested in evaluating the factuality of abstractive summarization systems. We benchmark three (stylized) methods of abstractive summarization: natural language sentence summarization (NLMSS), using Abstractive WordNet, and web document summarization (WDS), using MSCOCO. We describe the evaluation metrics used for evaluating these systems, and compare them with the existing tools. Our analyses and findings show that word vectors, while very effective for SMT, are less effective for summarization. Our experiments also show that there is a strong correlation between the factuality of abstractive summaries and their quality.
353. **Sequence-to-sequence Automatic Speech Recognition with Word Embedding Regularization and Fused Decoding**  
    We propose a new neural sequence-to-sequence method that operates directly on the word embedding representation and can be trained in an end-to-end manner. The key idea is to regularize the translation of the predicted vectors to new contexts with an attention-based model. The regularized translation vectors enable us to exploit the structure and variance of word embeddings in order to achieve better performance. We propose a novel decoding mechanism that combines a fused decoding component and a attention-based regularization layer, in order to adaptively extract features that capture the local aspects of speech. Experimental results on the Switchboard and CHiME-3 datasets show that the proposed model outperforms previous state-of-the-art algorithms, particularly when applied to low-resource tasks.
354. **Online News Media Website Ranking Using User Generated Content**  
    A knowledge base of news media websites, also known as news content, is an essential prerequisite for such an application. However, in many cases, the quality of this content varies significantly among different sites. Hence, it is imperative to learn the ranking algorithm of news content. In this paper, we propose a learning model that integrates topic modeling and topic modeling with user generated content. The proposed model is very accurate and efficient in terms of optimization as compared to the state-of-the-art unsupervised methods. Finally, we demonstrate how the proposed model can be used to rank news content on the websites of real media websites.
355. **Towards Successful Social Media Advertising: Predicting the Influence of Commercial Tweets**  
    There is a booming industry in turning social media into a platform for advertising, primarily targeting consumers in their area of interest. As people increasingly use social media to exchange opinions and get news, traditional advertising strategies that focus on fanbases have not been able to maintain their popularity. This challenge has spurred research into cross-branch market insights and user modeling. Given that current advertising strategies rely on predicting user behavior, a machine learning technique can provide a more comprehensive picture of user preferences in order to better determine commercial marketing strategies. Specifically, we propose using machine learning to predict commercial tweets as a first step in the ad- targeting pipeline. In the article, we describe how we leverage data from all the major commercial networks and a machine learning approach to predict commercial tweets. We also provide a set of case studies, which demonstrate that the new research finds insights into a diverse set of commercial properties and leads to higher advertising revenue.
356. **Multi-Module System for Open Domain Chinese Question Answering over Knowledge Base**  
    This paper describes a machine-learning approach to open domain question answering (QA) over knowledge base (KB). Our system incorporates multiple modules to support multi-step reasoning. Multi-column language model is modeled as a joint process of automatic classification, dynamic linking and question matching. For question selection, we analyze textual patterns in its word parts to select suitable candidates for an answer, from the base knowledge base. For final answer generation, we propose a system based on a model based on deep neural network. Experiments show that our proposed multi-module system reaches a higher F1 score compared with previous methods on both the ChineseNewsQA dataset and the QSI dataset.
357. **RPM-Oriented Query Rewriting Framework for E-commerce Keyword-Based Sponsored Search**  
    Sponsored search (also known as ad display) typically requires the integration of several factors (namely search engine, ads and advertisers) to enable better targeting and thus more conversions. In this paper, we propose a novel framework called RPM-Oriented Query Rewriting (RPM-OQR) for E-commerce Keyword-based Sponsored Search (E-KSBS). In our framework, each advertiser is an RDF schema and a query is an RDF record (possibly merged with other schema-related information). Both advertisers and search engines have access to the RDF schema and query records, therefore the underlying search engine needs to be powered by an RDF query-based backend system to satisfy the advertisers' ad targeting requirements. We first present the overview of RDF knowledge representation formalisms and RDF triples and their relationships, and then present our RPM-Oriented Query Rewriting (RPM-OQR) framework based on E-commerce keyword-based RDF and its performance evaluations. Additionally, we perform a comparison between the three proposals and a random decision making baseline method. We also highlight some limitations of our approach and some improvements for future work.
358. **Modeling Inter-Speaker Relationship in XLNet for Contextual Spoken Language Understanding**  
    In this paper, we propose a model based on Capsule Networks (CapsNet) for contextual spoken language understanding. The model assumes that there are inter-speaker relationships between the transcripts and speakers' utterances, which can be shared or eliminated by appropriately structuring the latent space. To improve the effectiveness of the proposed model, a prior on both the encoder and the decoder sub-networks is adopted in order to learn a prior of intra-speaker relationships, without human labeling. For several tasks, experiments show that our model outperforms the state-of-the-art approaches significantly in terms of multi-label recognition accuracy.
359. **Adaptive Ensembling: Unsupervised Domain Adaptation for Political Document Analysis**  
    Many existing political document analysis systems are supervised models, which require labeled data to be accurately classified into the correct political category. Since this process requires human labor and labeling cost, there is a great need to automate this process. In this paper, we propose a domain adaptation method for classification of news documents according to political labels. In our method, each document is classified into two different domains by using a feature-based classifier, which is trained on labeled data from one domain and unlabeled data from another. Our method is based on maximum likelihood estimation and adopts the kernel linear regression to find the optimal features. The presented method is unsupervised and does not require any labeled data to achieve its goal. Our method has been implemented on a live NewsWeb-2 dataset, and shows an average accuracy of 98.9% in two different different domains of political documents (Democrat and Republican). In contrast, the obtained accuracy of the supervised classifiers is about 80%. The source code is available at https://github.com/mmohammad/Adaptive-Ensembling.
360. **Towards Unsupervised Speech Recognition and Synthesis with Quantized Speech Representation Learning**  
    This paper presents a framework for unsupervised speech synthesis and recognition using quantized representations of speech waveforms. We define a novel synthesis task based on the construction of quantized speech waveforms, and apply this framework to multiple state-of-the-art unsupervised automatic speech recognition models and a novel recurrent neural network based model. The performance of the proposed framework is evaluated on the LRS2 benchmark task, where we achieve new state-of-the-art results. We also evaluate the performance of our proposed models on the SWIT-TSC 2017 dataset. Experimental results show that the proposed model outperforms existing deep neural network based models for the task of unsupervised speech synthesis.
361. **A Richly Annotated Corpus for Different Tasks in Automated Fact-Checking**  
    We introduce a new annotated corpus with over 6,000 fact-checking claims, each annotated with sentiment, category, date and source. The fact-checking claims are organized by task, and we employ two main strategies for determining the entities, claims and sources that a claim should be evaluated against, which can be done with different annotators. The datasets are made freely available.
362. **Quantifying the Semantic Core of Gender Systems**  
    There has been significant recent interest in using machine learning methods to verify semantic properties of gender systems. In this paper, we introduce a formal semantic core characterization of a particular kind of semantic core which performs reliable recognition of assigned gender labels. We formalize the core characterization process using a probabilistic model of the inferred gender classification probabilities (JPCP) produced by the underlying semantic core. Using this core characterization process, we develop a model for classifying the frequency of a term in a set of terms (term frequency counts) and which builds a supervised classification model based on the inferred class probabilities for terms in the corpus. We evaluate the accuracy of our classification model by applying it to some commonly used datasets (different data sizes, different scenarios) and evaluating the statistical significance of the likelihood ratios of the estimated probabilities.
363. **Does Speech enhancement of publicly available data help build robust Speech Recognition Systems?**  
    Speech enhancement is a method to enhance speech signal so that it will be intelligible to human listeners. Its goal is to preserve the quality of the speech signal. Enhanced speech signal is in general best in its form when the noise in the speech signal is low. However, improved speech enhancement is quite valuable for the speech recognition systems. In this paper, we develop a method to enhance speech signal using more commonly available speech signals and a non-human speech database. Using an automatic speech recognition system that is trained with the noisy speech signals, we measure the enhancement of the speech signals by the difference in a baseline speech signal and the enhanced speech signal. The enhancement is obtained using the image enhancement function and the transformation factors. Experiment results show that the enhancement of speech signals using publicly available data improves the performance of the speech recognition system by reducing the utterance level-word error rate by 9.99% and the Mel-Cato distance by 2.14% on the challenging IEMOCAP benchmark.
364. **Transformer-based Cascaded Multimodal Speech Translation**  
    This paper presents a deep learning based cascaded multimodal speech translation model. It involves two cascaded stages: a deep transformer network that generates word sequences from text input, followed by a multi-source code vector decoder that feeds the generated sequence to the decoder of a multi-modal translator. The decoding of the multimodal code vector is performed directly in the deep transformer. Although the deep transformer has been shown to be effective in speech translation, its application to multimodal speech translation has not been investigated so far. In this paper, we show that deep transformer-based cascaded multimodal speech translation can achieve improved performance in generating speech inputs, as well as reducing the number of mis-translations generated. We also introduce three datasets from which our experiments are performed, i.e., Multi-modal Code-switching English (ME2E), Multi-modal Code-switching Chinese (MS2C), and Multi-modal Code-switching Japanese (MS2J). The experiments have demonstrated the benefits of the proposed cascaded multimodal speech translation model over standard encoder-decoder architecture.
365. **Incorporating Interlocutor-Aware Context into Response Generation on Multi-Party Chatbots**  
    We propose an end-to-end trainable model to generate multi-party conversation responses on chatbot based on context information from both the chat and human conversation. Our model performs a multi-level contextual analysis of the conversation, by using entity mentions in the chat history as context representations in a Neural Attention Sequence-to-Sequence (NASS) model. Then, we incorporate NASS and a context encoder-decoder framework, which uses entity mentions as an additional source of context information for contextualization. We show that our model consistently improves the performance of the Multi-Party Chatbot System (MPCS) over baseline models trained with existing discourse semantics information.
366. **BPE-Dropout: Simple and Effective Subword Regularization**  
    We present an approach to improve the performance of any recurrent neural network on a sequence-to-sequence task by employing a character-level word embedding, such as BPE (Bidirectional Encoder Representations from Transformers) or CBOW (Convolutional-Bidirectional Transformers). We do so by placing each character of the word embedded in a separate convolutional layer and train it separately to generate all of its weight vectors. We refer to the model as BPE-Dropout and evaluate it on character-level tasks that utilize segmented word embeddings. We demonstrate that our method successfully regularizes these models on several language modeling tasks, including sentiment classification and automatic speech recognition. We also demonstrate that these models are much more robust to cross-modal changes in distribution of the input data (e.g. changes in word embedding distribution) and learning from low-resource datasets, like a MNIST image.
367. **a novel cross-lingual voice cloning approach with a few text-free samples**  
    This paper proposes a method for cross-lingual voice cloning using a few sample aligned sentences and the nearest neighbor feature alignment method. Our experiments show that a good speech quality is obtained using a set of natural sounding sentences with varying length for the training data.
368. **Findings of the Third Workshop on Neural Generation and Translation**  
    This paper presents the findings of the third Workshop on Neural Generation and Translation (NLT), held on June 19th, 2017 in Linz, Austria, and is a follow-up paper to the first Workshop on Neural Generation (2009). The title of this Paper comes from the field of Pattern Recognition and Machine Translation. This report summarizes the main work of the third Workshop and presents the main contributions of the participants. The major approach taken is to capture the natures of sentences for the purpose of generation of certain kind of (often unrealistic) sentences, by a deep neural network, and to incorporate these natures for translation into the translation model. In addition, the contribution of each participant, in particular the area of implementation details and of comparative studies, is also discussed.
369. **Transferable End-to-End Aspect-based Sentiment Analysis with Selective Adversarial Learning**  
    Despite the recent achievements in aspect-based sentiment analysis, to the best of our knowledge, transferable end-to-end approaches are still in their infancy. To build end-to-end architectures for aspect-based sentiment analysis, we face the challenge of modeling not only aspect-based relation but also aspect-based aspect-by- aspect relation. Towards this end, we propose a novel Transferable End-to-End Aspect-based Sentiment Analysis with Selective Adversarial Learning (TEAS). Our method is based on a unified attention model which takes into account aspect-by- aspect relation with a simple attention mechanism. As a result, aspect-by- aspect attention mechanisms are transferred from the source domain to the target domain, and the target domain sentences are projected back to the source domain. Furthermore, we adopt a multi-stage adversarial learning strategy to strengthen our proposed attention model, leading to transferable aspect-by- aspect relation representation in a semi-supervised manner. Extensive experiments on two public datasets demonstrate the effectiveness of the proposed method.
370. **Hidden State Guidance: Improving Image Captioning using An Image Conditioned Autoencoder**  
    This paper studies the task of image captioning where a neural network caption is produced by making use of a predefined image description given as input. This paper introduces a novel method for image captioning. The algorithm is based on the concept of conditional autoencoder (CAE) which utilizes the image-conditioned distribution of the output produced by the captioning network in order to generate an encoded message. To learn the CAE architecture, we propose a new data augmentation method called elementwise uniform (EU) algorithm which forces the representation of the image to not be dominated by its local maxima and common minima, thereby increasing the interpretability of the output. The experimental results on Flickr30K benchmark dataset show the state-of-the-art performance of the proposed algorithm in terms of a macro mean evaluation metric. Moreover, the proposed algorithm can handle weak image captions, i.e., captions with a few semantically significant words.
371. **Ensembling Strategies for Answering Natural Questions**  
    In this paper, we study the problem of modeling natural questions as compositional questions and show that existing deep learning models, by considering the scoring rules as a starting point, cannot scale effectively to a large number of logical questions. We propose two modifications to the traditional modeling procedure of recurrence relations, namely using multiple stochastic models, which significantly improve the performance. Moreover, we introduce a new type of reasoning, which consists in expressing the truthiness of a literal in the form of a scoring rule. The results show that not only does our method achieve new state-of-the-art performance on five benchmark datasets, but also outperforms the state-of-the-art approaches for more than 90\% of the cases, indicating the potential of our method in answering large-scale natural questions.
372. **Predicting Discourse Structure using Distant Supervision from Sentiment**  
    Modeling discourse structure is an important task in natural language processing, where we can use linguistic information to predict the discourse relation in a text. The task has been addressed via a variety of models, and a state-of-the-art model, neural machine translation, is well-established. Recent work has shown the value of distant supervision in addition to standard supervision. However, existing techniques require ground truth discourse structure, which is hard to obtain for most languages. We present an approach to predicting discourse structure with distant supervision from sentiment. We show that such distant supervision can be effectively used by fine-tuning the standard sentence-level transition model. When evaluated on two manually annotated datasets from English and Russian, we find that, compared to using solely sentence-level supervision, using sentiment induces an additional gain of 3.1 BLEU points on English and 2.6 BLEU on Russian.
373. **How does Grammatical Gender Affect Noun Representations in Gender-Marking Languages?**  
    The task of classifying noun phrases into their gender has recently received interest as a means to investigate typological variations in language. In this work, we use five languages (English, German, Chinese, French and Japanese) that have been the focus of previous work (Jakob, 2001; Fisher, 2004; Liu, 2008) as testbeds to study the effectiveness of word embedding techniques that encode gender information in high-dimensional word embedding spaces. We examine the impact of grammatical gender on noun representations in a cross-lingual gender classification task. We find that the presence of gender information in a language strongly impacts the performance of the model. Specifically, in a language that is more gendered, language-level contextual features play a greater role in determining the class labels for a particular noun phrase. Furthermore, the performance of gender-sensitive models can be further improved by weighting contextual features, which is different than the weighting that has previously been studied in lexical gender classification.
374. **Let Me Know What to Ask: Interrogative-Word-Aware Question Generation**  
    Automatic question generation is an interesting but challenging task. Existing methods typically face the problem of predicting questions from a given document and current evaluation measures only consider sentence quality of the question, ignoring the interrogative aspects that may arise from a question. In this paper, we study the problem of question generation from an original text containing both question-word pairs and mentions of interrogative entities. To solve this task, we introduce an interactive question generation model, which automatically generates questions according to the query, but also learns to ask the correct query for these interrogative entities in the task-specific dataset, if any. Extensive evaluations on the ACE 2005 dataset show that our model can generate more accurate questions and clauses, compared to the state-of-the-art approaches.
375. **Towards Generalizable Neuro-Symbolic Systems for Commonsense Question Answering**  
    This paper presents an extension of our ontology-guided neural network model for commonsense question answering (CSQA) to the collaborative role-playing domain. We explore ways to modify the model to incorporate commonsense knowledge, which can help improve the model's generalization performance. Our model is inspired by the recent research on commonsense networks and reflects the recent work in the commonsense reasoning literature. We adapt several standard models from that literature into the CSQA domain and show how it can be generalized to incorporate commonsense knowledge. We observe that the generalized model results in better generalization performance. The empirical results are promising.
376. **Let's FACE it. Finnish Poetry Generation with Aesthetics and Framing**  
    We propose an architecture to translate poems using vector space vectors and deep learning models. The system is specifically designed to handle Finnish poetry and to take into account the framing of the text. Firstly, the system must be able to generate poem with different styles using the same model. Secondly, the system should be able to generate the poems in an aesthetic sense. In the third step, the poet must be able to determine his own style and produce poems with particular style. It is a complicated task to combine the merits of vector space vector spaces and deep learning models. Therefore, the technical contribution of this paper is to propose a system using a combination of vector space vector spaces and deep learning models to translate poems. In addition to evaluating the proposed system with the same types of poems, the system is also tested in another classical Finnish poetry corpus to compare with the conventional WYSIWYG and also with a current state of the art for aesthetics enhancement. A comparison with state of the art systems is also made. We find that our system obtains the same results as the conventional WYSIWYG system, but better results when we need to be aesthetic in our poems.
377. **SMS-WSJ: Database, performance measures, and baseline recipe for multi-channel source separation and recognition**  
    The ability to isolate the source signal from the background noise in a multi-channel source separation task has a variety of applications in transmission science and communications. While the precision of separating a signal in a multi-channel source separation task is not well understood, there are some recently published multi-channel source separation studies that demonstrate very promising results. However, those studies do not make any specific assumptions regarding the data to be separated and may not be applicable to the task of recognition of Multi-Channel (or multi-way) SMS signals. We present a database, called SMS-WSJ, to enable benchmarking of SMS source separation tasks using data from real experiments and/or re-experiments of existing algorithms in different situations and in different applications. We also present a new SMS-WSJ dataset in which all the multi-channel source signals from randomly separated channels are used for training a Multi-Channel (or multi-way) Recognition (MR) system. We also investigate performance of different convolutional neural network based algorithms (v2 and v3) and some feature selection methods. In addition, we examine the impact of signal processing parameters on the recognition accuracy and find that when the source signal is properly filtered (at a minimum signal rate), improved recognition accuracy is achieved in most cases. However, at very high signal rates, the performance of most of the existing methods degrades significantly and some very strong baseline systems that do not make any assumption about the signal to be separated (i.e., channel to be separated) are used. The proposed database, database and datasets will make it possible to establish research baseline strategies and compare them with other multi-channel signal separation methods. We will also provide a new dataset (SMS-SJ) for multi-channel source separation and recognition task using real multi-channel data and source signals from wireless networks, which will be made publicly available to the research community. We believe that this database and the new dataset will also facilitate further research in other multi-channel source separation and recognition applications.
378. **Toward Gender-Inclusive Coreference Resolution**  
    We present a framework for resolving coreference relations in the broad family of `topicalized syntax-based entailment systems' where each supporting text is associated with a set of coreference relations. Our framework is based on a language model in the form of Transformer which implicitly learns to reorder the relations, and infer such an expanded coreference graph through a novel weighted pairwise projection algorithm, which we propose to optimize using empirical error minimization. Our experiments show that the language model enables handling situations where earlier sentences have long coreference histories while later sentences lack such histories and thus fail to form a full coreference graph. Experiments also demonstrate that, for general knowledge graphs, the smaller set of coreference relations will benefit the overall accuracy.
379. **A Latent Morphology Model for Open-Vocabulary Neural Machine Translation**  
    Multi-hop dependency parsing provides an effective way to express dependencies that are meaningful for multi-hop applications. In this work, we propose a new multi-hop dependency parser that relies on a latent semantic representation of sentences that captures more structural information. This latent representation is obtained by leveraging BERT's capacity to encode high-level semantic information. We then augment the parser with attention and a recurrent neural network-based attention mechanism that is applicable across dependencies. Evaluations on three different text-dependent tasks (e.g., Question Answering, Multimodal Sentiment Analysis and Dependency Parsing) demonstrate that our parser performs well against previous parsers on two language pairs (English-French and English-German), on two different language pairs (English-French and English-German) and on a large-scale corpus (SensIoT).
380. **A Framework for Building Closed-Domain Chat Dialogue Systems**  
    In this paper, we present an extension of the Expert System Language (ESL) described in [1], a high-level language for representing end-to-end dialogues. The key idea is to use a low-level language, which is not necessarily logical, and which may also contain undesirable syntactic constructs, to express necessary actions and facilitate the execution of user-initiated dialogues. To generate such a low-level language, we adopt a Reinforcement Learning (RL) framework which learns to select a set of information transfers in the low-level language and use them to generate a high-level language as a representation of dialogue results. The low-level language is trained using a Reinforcement Learning (RL) algorithm that encourages it to summarize the dialogue history and to consist of fluent phrases that describe the most important events in the discussion. By comparing the output of the low-level language with the output of the high-level language in an expert system, the RL algorithm ensures that the generated conversation is close to the original dialogue. The proposed approach enables the creation of closed-domain dialogue systems which are more than an average improvement over the current state-of-the-art systems, and thus better perform their tasks in a human-machine environment.
381. **Discourse-Aware Neural Extractive Model for Text Summarization**  
    Text summarization is an essential task in machine learning. Traditional methods generally extract and summarize sentences into an overview sentence or summary-style output. However, extracting a summary-style representation is not appropriate for abstractive texts such as news articles or scientific papers. Recent studies have demonstrated that current neural models can also benefit from the lexical, syntactic and discourse information in a document and produce summaries that are both semantically and syntactically accurate. However, most existing models operate directly in the input-output space, whereas the salience relations in a document are still considered in the representation learning process. In this work, we propose a discourse-aware Neural Extractive Model (NEM) for abstractive text summarization. The proposed model is a part-of-speech-aware neural model that can incorporate discourse information into its extraction process by taking into account the relation between the input text and the final summary. We evaluate the performance of our model on three data sets, i.e., the SemEval'2015 Task 9 English news article dataset, the AAAI'2015 SQuAD dataset and the STSSP dataset. Experimental results show that our model significantly outperforms the previous state-of-the-art models for abstractive text summarization. Moreover, we observe that our model is significantly more robust to out-of-domain data.
382. **Adapting Multilingual Neural Machine Translation to Unseen Languages**  
    Neural machine translation (NMT) has advanced rapidly recently, and its performance has recently been improved by deep learning. However, translating between multiple languages is challenging due to the following two factors: 1) poor feature representation and lack of parallel corpora, and 2) lack of parallel corpora, which are generally used for large-scale parallel data acquisition. Therefore, the NMT model should be adapted to the characteristics of the target language without using any parallel corpora. In this work, we propose a novel multilingual NMT model, named Adaptive Multilingual NMT, in which translation is performed based on a multilingual context model which utilizes the local context information extracted from the source language and the statistical information from all source languages to perform the translation. To further further improve the performance, we propose a new transfer mechanism for integrating parallel data. We evaluate the proposed model on four benchmarking datasets. The experimental results show that the proposed model achieves the state-of-the-art performance. In addition, we also study the impact of transfer mechanism. Our code is publicly available.
383. **Time to Take Emoji Seriously: They Vastly Improve Casual Conversational Models**  
    With the introduction of Emoji, conversational agents have been hit with a major translation problem: Models that only produce natural language text. In a conversational application, for instance, a model for dialog should answer questions and be able to answer non-verbal messages in real time. In this paper, we study time-to-text (T2T) models trained to generate responses, and propose that these models can be improved by using Emoji. We first explore the impact of different combinations of emoji on the time-to-text performance of convolutional neural network (CNN) language models. Our experiments indicate that using emoji does indeed improve the text quality and result in a 5-11% improvement in T2T performance.
384. **Jointly optimal dereverberation and beamforming**  
    Massive omnidirectional satellites now safely orbit the Earth, and vast amounts of data are produced by the science instruments they carry. With each satellite passing overhead, an accurate spaceborne telemetry system can dramatically improve the performance of these instruments. In particular, a beamforming control approach has been proposed to enable safe control of the scientific payload during payload collision avoidance maneuvers. This approach has been shown to work well in practice, but directly optimizing beamforming control requires information on the task to be performed as well as the probabilistic reliability of the estimations. In this paper we take a first step towards enabling beamforming control directly using a probabilistic robustness index. By transforming beamforming control to a 1-D combinatorial optimization problem, we derive its optimal solutions and show that (1) it is equivalent to a nondecreasing function in terms of the beamforming error, and (2) due to the space constraint, we can efficiently solve this combinatorial problem. Using this analysis, we devise a novel deorbiting algorithm that uses the solutions of the combinatorial problem, through direct sampling of the space environment, to generate control commands to avoid collision, and demonstrate the effectiveness of our approach on real data.
385. **ON-TRAC Consortium End-to-End Speech Translation Systems for the IWSLT 2019 Shared Task**  
    This paper describes the end-to-end speech translation system that participated in the 2019 IWSLT shared task, which included both standard English and Czech translation approaches. In this paper we describe the pipeline and architecture of the system, describe how we reached the best results in a shared task (STS-translate) for both languages, and highlight the individual contributions of the systems.
386. **Phenotyping of Clinical Notes with Improved Document Classification Models Using Contextualized Neural Language Models**  
    Clinical notes have become an invaluable source for researchers in science and industry and they are both challenging and expensive to obtain. Machine learning algorithms are able to extract useful information from them and improve disease identification, but are often unable to extract information at the abstract level. The large volume of clinical notes makes it difficult to find patterns in them without manually filtering, and complex patterns pose a challenge to retrieval algorithms. We have created an algorithm for extracting information at the document level that incorporates a context information model over temporal dependencies. The algorithm is trained using rich labeled data from electronic medical records. We perform a pilot clinical study to evaluate the performance of the algorithm using records from the National Lung Screening Trial. The results show an 8% improvement in recall when compared to the original text classification algorithm. We report on the follow-up study that shows the importance of context-aware topics in an increasing amount of electronic medical records. This research brings us one step closer to the ability to automate clinical notes and provide information on the disease status of more patients and to faster patient cohort management.
387. **Fill in the Blanks: Imputing Missing Sentences for Larger-Context Neural Machine Translation**  
    Neural machine translation has achieved state-of-the-art performance recently. However, it is still far from perfect: missing translations are often easy to generate, especially for high-resource languages like Korean. We propose a simple yet effective method to recover missing translations for multilingual neural machine translation. The key idea is to mimic human decoding, i.e., generating sentence hypotheses at the start of the decoding process to infer a full sentence at the end. The hypothesis samples are conditioned on language-dependent semantic features, and the resulting hypotheses can be handled by standard sequence-to-sequence neural models. Experimental results on WMT14 English--German show that our proposed method can achieve up to 2.6 BLEU, translating sentences with up to 492k words and incorporating these data back into the training process.
388. **A Neural Topic-Attention Model for Medical Term Abbreviation Disambiguation**  
    Medical term abbreviation is widely used in clinical practice. Traditionally, biomedical abstracts contain a word embedding in which the words are extracted from the documents using related terms of interest. However, few studies focus on the automatic disambiguation of abbreviation words. Many clinical terms have different usage patterns, and automatically determining their correct usage can help to improve clinical terminology resource utilisation and productivity. Existing medical term abbreviation disambiguation methods focus on particular clinical sub-areas, such as diagnosis and medical procedure descriptions. In this paper, we propose a neural topic-attention model for clinical term abbreviation disambiguation. Our model can determine the correct usage of a sub-phrase within a given medical term by learning a deep vector representation of the context words, which captures the general usage of the term. In addition, we investigate an ablation study to assess the effectiveness of topic word embedding. Experimental results show that the proposed neural topic-attention model outperforms state-of-the-art methods.
389. **Building an Application Independent Natural Language Interface**  
    The usability of existing text editing applications is heavily influenced by the choice of the appropriate components. In this paper, we consider the task of building an application independent natural language interface that is optimized for user experience and is not just about function over form. We propose to cast natural language input as a pipeline that extracts representations of both phrases and sentences. We first build a set of simple sentence representations from shallow context and built a human-evaluated sentence ranking model for ranking phrases. Then, we train an auto-encoder model to classify phrases based on their representations. We use a large publicly available corpus of dialogue data as input to show that our model is able to predict the most common phrases (as extracted by the auto-encoder) for a given sentence.
390. **LSTM Easy-first Dependency Parsing with Pre-trained Word Embeddings and Character-level Word Embeddings in Vietnamese**  
    This paper presents an attention based, early stopping, attention dependent, parsing framework. Easy-first parsing is a simple, low cost algorithm that accepts input chunked-word-sequence and produces parses of each chunk. The ability to annotate the chunk with both free word-character units and word-character embeddings is very important as it helps the parser focus on the most informative words in the input chunk. Simple-first parsing helps parsing efficiency as the trained word embeddings can be used during parsing to help speed up the model while reducing the complexity. Compared with the state-of-the-art LSTM based Easy-first parser, the simple-first parser has a faster runtime with higher accuracy. On the benchmark dataset (VNLI), simple-first parser outperforms all previous parsers.
391. **Dreaddit: A Reddit Dataset for Stress Analysis in Social Media**  
    Emotions are an important component of personality. Social media such as Twitter can provide us with a rich source of information on social status. Some individuals become depressed when coping with stress and such individuals might experience worse mood. As a result, stress can be a mental health problem that is indirectly addressed by employing social media. As this paper presents a dataset of health status in Twitter, the aim is to capture the level of stress in a social media user and in turn classify this stress as positive or negative. The work is performed through statistical methods using feature selection techniques. Some methods such as Long Short-Term Memory (LSTM) are based on static information. However, users often use posts in a spatio-temporal fashion such as by stating time-stamped information. Further, tweets have associated information about day-to-day activities that also play an important role in stress detection. This work classifies stress using the LSTM. The Classifiers are tested using both of these static features along with some temporal related features such as weather related and day-to-day activities.
392. **Implementation of an Index Optimize Technology for Highly Specialized Terms based on the Phonetic Algorithm Metaphone**  
    In this paper, an index optimization technology for highly specialized terms based on the phonetic algorithm metaphone is developed. With the development of the global database, the case study of the service provider of an emergency room is applied for a comparison. The developed index implementation of terms of high term specialization and operation efficiency are compared with the typical index implementation of terms of low term specialization. For evaluating the performance of the developed index implementation, performance values are used as the performance evaluation measures and the comparison results are used as the performance metrics. The index implemented with the developed index optimization technology is expected to reduce the redundancy among the terms and increase the term specialization rate in terms of variation rate.
393. **Generalization through Memorization: Nearest Neighbor Language Models**  
    Recent work has demonstrated that deep learning models that are trained and tested over a single dataset achieve state-of-the-art performance in other datasets and, as a result, are better able to generalize to new ones. However, there are also significant gaps between training and test, especially in settings where large amounts of labeled data are not available. The answer to this question has two parts: (i) how do the model decisions affect training and test performance? and (ii) how do these representations relate to prior knowledge of the domain? In this work, we first show that we can learn representation spaces that significantly outperform the traditional top-down, feedforward data model. We then show that these representations, learned over a small amount of labeled data, can be used to generalize a general model to a new dataset, significantly outperforming the best performing architectures trained over the entire dataset, all trained using full labeled data. Finally, we show that this generalization gain is not simply a by-product of faster learning, but is actually a meaningful factor in the generalization performance.
394. **CN-CELEB: a challenging Chinese speaker recognition dataset**  
    This paper presents CN-CELEB, a challenging dataset for automatic speaker recognition in Chinese. CN-CELEB is the first large-scale, multi-speaker Chinese dataset consisting of English recordings and Mandarin recordings. It is composed of more than 300 hours of speech and more than 35,000 hours of character sequences, and contains the best known Chinese speaker recognition performance. The Mandarin speech, as well as a previous Chinese speakers' dataset, were released previously as public resources (http://www.ku.edu/~gazetteer/content/tedarino.html). For the Chinese speech version of the dataset, the current best experimental results on training a Convolutional Neural Network (CNN) to recognize Chinese speakers are 34.1% (with softmax) and 41.5% (with max-pooling), which are both the best results obtained to date on the Chinese speakers' version of the CN-CELEB dataset. For the Mandarin speech version of the dataset, a CNN-based baseline of 67.5% accuracy is obtained with bag-of-words input only, which is the best performance obtained to date. Both data sets can be found at: https://github.com/communicampus/CN-CELEB.
395. **Sequence Modeling with Unconstrained Generation Order**  
    Sequence modeling tasks have been shown to benefit from disentangling the non-sequential content of a sentence from its spatial ordering structure. In this paper, we propose a unsupervised model that disentangles the latent ordering information without human supervision. We propose to use a bottom-up graph to form a latent ordering between sentences, which we define as the span order of the shortest paths between latent units. We train the latent ordering model to be able to predict a latent ordering from a set of input sentences while minimizing the discrepancy between the predicted order and the ordering predicted by the top-down ordering model. We perform extensive experiments on tasks such as sentence classification and retrieval, showing the benefits of unsupervised sequence modeling for tasks that require disentangling and analyzing sequential structure.
396. **Harnessing the richness of the linguistic signal in predicting pragmatic inferences**  
    Text-based methods that leverage linguistic information to support behavior prediction have recently gained attention in both academia and industry. The expressiveness of natural language allows for a representation of complex phenomena that would be difficult or impossible for computer models alone to capture. This motivates research into an increasing variety of approaches, from information fusion to the symbolic construction of pragmatic inferences. Here, we focus on inferences expressed as postulates in postulates clauses, and show that machine learning based on linguistic information has the potential to outperform existing approaches on three downstream tasks. Our findings emphasize the value of explicitly modeling linguistic relations in computational inferences and guide future research in this space.
397. **A neural document language modeling framework for spoken document retrieval**  
    Machine learning-based approaches are investigated for spoken document retrieval (SDR) systems, either taking advantage of existing speech recognition algorithms or directly trying to learn distributed representations of the document. However, no attempt is made in taking the syntactic structure of the speech into account for their learning, since a semantic interpretation of text can be clearly misleading. In this paper, we present a neural document language modeling framework for spoken SDR, which learns distributed representations of text in order to perform document retrieval. To incorporate syntactic information about the document into the language modeling procedure, we use a neural tagger to generate phonemes conditioned on word embeddings. Further, we employ hidden layer re-ranking and skip-gram model to enforce the learned representation to be independent of the syntactic structure of the document. Experimental results on two benchmark datasets show that the proposed framework outperforms existing techniques for document retrieval, especially for the cases of low-resource languages, which the results demonstrate to be a challenge for existing SDR approaches.
398. **Learning to Customize Language Model for Generation-based dialog systems**  
    With the spread of information technology, communication technologies like Internet, mobile and social media have become the real world and human communication are on the rise. The dialogue generation problem is to generate a natural and engaging dialogue response in real world by conversing with a user. The model must not only successfully build a dialogue system but also be accurate. The text that a human reads to make a dialog response is high dimensional, noisy, repetitive and repetitive. This makes traditional models such as shallow utterance based models less suitable. In this paper, we present a language model to mimic human language in order to enhance the accuracy of the generated responses. Based on human language model, we develop a second model that can respond to the prompt and capture the intent of the user. This model is a sequential hierarchical component of encoder-decoder architecture and the model is trained end-to-end with reinforcement learning. It works for both natural and generated responses and we evaluate our model on Amazon Mechanical Turk data set.
399. **Great New Design: How Do We Talk about Media Architecture in Social Media**  
    Many major news organizations have "branding": the discussion process that defines the name of an organization, which involves daily interactions with a broad audience of users on social media. When news organizations design their communications architecture, it reflects and builds on their brand and customers' expectations. The aim of this work is to show how design processes influence people's perception of design from their perspective of media architecture. In particular, we consider the design of "manageable" social media resources, i.e. resources that help users manage their experience and behaviors. Such "manageable" social media resources consist of (i) an entry list for users with whom users can communicate, and (ii) assistance mechanisms to let users understand how their experience and behaviors will be accommodated. Our results show that when people receive news, they have specific understandings about these design elements: how users can interact with each other and the type of interface that allows users to communicate, and the level of authority a news organization has in the design process. People have understanding about media architecture through an interaction with the design process, and design decisions have a marked impact on people's perceptions.
400. **Do Multi-hop Readers Dream of Reasoning Chains?**  
    Why does a deep neural network (DNN) require at least $3.3$ layers to learn a pattern of $n$ entities? How many examples (single-hop or multi-hop) are needed to bridge the gap between $n$ and the ability of a DNN to learn a pattern of $n$ entities? Is it possible for DNNs to map a single example to a pattern that can be readily distinguished from other examples? Are there any implications to other recurrent neural networks (RNNs) that learn pattern-matching abilities? The answer to these questions can potentially impact a number of tasks including machine learning, biology, language understanding and neuroscience. We show that deep single-hop RNNs can learn patterns of $n$ entities with an efficient $32.5\times$ reduction in size compared to the size of the DNNs to which they were trained. In addition, we analyze how the ability to efficiently approximate a pattern develops through learning, and then introduce a method for creating an approximate $n$-to-pattern mapping. We also show that when using a small, random subset of $n$ examples, it is possible to learn pattern-matching abilities similar to those in the human brain, but with $3.3\times$ the number of network parameters.
401. **Document-level Neural Machine Translation with Inter-Sentence Attention**  
    Learning to produce high-quality, accurate and interpretable end-to-end translation from a source sentence to its corresponding target sentence is a fundamental yet difficult problem that can significantly improve the quality of the final translation. Recent neural machine translation systems achieve highly competitive performance in this task and have recently been used in practice. However, these approaches are not generally applied to documents, which provide a richer and more informative source of data and a more flexible model of communication. Our method introduces a novel connection between source and target sentence-level translation, in which the source sentence-level translation system is coupled with an attention-based decoder that generates a sentence-level output in an attention-like manner. The construction of the model is established within a generative adversarial training framework, which ensures the model to capture the local and long-distance contextual information of the source sentence. We evaluated the proposed method on several public machine translation datasets, achieving state-of-the-art results. In addition, the proposed model is able to generate a higher-quality sentence-level translation than a purely attention-based approach. Finally, we use the proposed model as an objective metric for downstream document-level neural machine translation. In particular, we find that our model outperforms a more conventional attention-based baseline that directly predicts target sentences, demonstrating the importance of the proposed approach to both the end-to-end neural machine translation and the downstream application.
402. **Attention Is All You Need for Chinese Word Segmentation**  
    In this paper we present a deep learning system for Chinese word segmentation. Our system uses an attention mechanism as its base, while a novel mechanism we developed is used to alleviate out-of-vocabulary words. Our results show that the attention mechanism allows the network to focus more on a few targeted words. In addition, the capacity to mitigate words that do not occur in the training corpus is greatly reduced. We also find that optimizing a linear model produces a better model than using the cross-entropy loss. As a consequence, our results show a significant improvement on both word segmentation and character segmentation, and the accuracy can be further improved by combining our model with a neural machine translation system.
403. **Naver Labs Europe's Systems for the Document-Level Generation and Translation Task at WNGT 2019**  
    In this paper, we present the results of systems submitted to the WNGT 2019 shared task, corpus-level machine translation (MT) and document-level translation (T) tasks for translation directions outside of the training set. We outline the formal submission guidelines, evaluation metrics and results for the task as well as proposing a preliminary evaluation dataset.
404. **Machine Translation of Restaurant Reviews: New Corpus for Domain Adaptation and Robustness**  
    There is an increasing need for data and tools to evaluate the performance of machine translation models across domains. This paper describes a new restaurant review dataset, called RestaurantDB, that will enable domain adaptation of neural machine translation models. The restaurant review domain has been poorly studied. To the best of our knowledge, the RestaurantDB dataset is the first to include restaurant reviews in a domain. To the best of our knowledge, this is the first publicly available restaurant review dataset that contains restaurant reviews from three distinct domains. In this work, we present a novel approach to generating test sets to support the development of domain-adaptive neural machine translation models. We report the accuracy of the domain-adaptive models on RestaurantDB and propose improvements to the baseline model that will improve the generalization performance of the model.
405. **Uncover Sexual Harassment Patterns from Personal Stories by Joint Key Element Extraction and Categorization**  
    The personal stories of online female users have a great potential to reveal subtypes of harassment in cyberspace. We consider a task of extracting a pair of high dimensional attributes of sexual harassment, a person's first and second names and their appearances in posts written by the same person, from user-generated content. We focus on identifying such user-generated patterns by taking advantage of our knowledge of the previous instances of sexual harassment, and design a system based on an encoder-decoder framework, using both recurrent neural networks and probabilistic graphical models for interaction between a query-passing decoder and a global store-destination controller. Results show that our proposed joint framework is able to successfully retrieve and extract such key elements, and also perform accurate classification of these patterns, without any knowledge of the existence of previous instances of harassment.
406. **When Choosing Plausible Alternatives, Clever Hans can be Clever**  
    This paper discusses the role of heuristic heuristics in choice problems involving plausibility measures. It shows that the use of heuristic heuristics significantly reduces the expected loss incurred in comparison with optimal choices.
407. **A Robust Data-Driven Approach for Dialogue State Tracking of Unseen Slot Values**  
    In this paper, we develop a robust data-driven method for tracking slot values of dialogue state trackers, without access to knowledge about slot types or dialogue state files. By employing a probabilistic model to estimate the probability of Slot-value association, we make strong inferences about the number of possible combinations of slot types and dialogue states, which are often insufficient for tracking. We provide extensive simulations and real data examples that show that our method is significantly more accurate than the state-of-the-art data-driven methods.
408. **On the Linguistic Representational Power of Neural Machine Translation Models**  
    Neural machine translation (NMT) has seen tremendous progress in recent years. However, surprisingly, little is known about the representational power of these models, and in particular, the role of information representations and languages. To better understand the influence of representation power on NMT, we introduce NMTRL, a large-scale NMT model trained on parallel corpora of English and Japanese. NMTRL evaluates the representational power of NMT models by making use of language models that have been trained on large-scale parallel corpora. We show that model performance on English-Japanese is strongly impacted by the amount of parallel data used to train the language model, the quality of translations produced by the language model, and the language model's words memorization. We further demonstrate that modeling language models with bidirectional RNNs yields the best performance, obtaining an absolute improvement of 1.8 BLEU points over the best competitive model on the WMT'14 English-French task. Finally, we explore the role of source representations in the translation process, demonstrating a significant positive impact of source representations on BLEU scores. Our findings suggest that NMT models are learning linguistic representations that encode grammatical information and representational power are a crucial but not a sufficient condition for the effectiveness of NMT.
409. **Improving Generalization of Transformer for Speech Recognition with Parallel Schedule Sampling and Relative Positional Embedding**  
    Deep neural networks have shown outstanding performances in speech recognition recently, however, learning the model parameters simultaneously can induce problems with better generalization. In this work, we proposed a new network architecture by stacking a parallel transformer to provide an approximation of the joint hidden network. Further, we propose an efficient training algorithm, which simulates a relative positional embedding based on the change in conditional probabilities. In this way, the relative positional embedding can help to learn the similarities and differences of pairs of parallel networks. Using RNN, we can embed the words to the network parameters by converting the whole probability space into a pairwise joint probability space and then determine the joint parameters of each network in parallel. Extensive experiments on TIMIT, AMI and LibriSpeech benchmark datasets show that our proposed network architecture improves the generalization of the models, where the accuracy gains are consistently above 15% and better than 35% on average.
410. **Select, Answer and Explain: Interpretable Multi-hop Reading Comprehension over Multiple Documents**  
    In this paper, we address the problem of multi-hop reading comprehension over multiple documents. Our model identifies textual spans as question-answer pairs and then generates a natural language (NL) explanation using deep neural networks to describe the logical reasoning in the entailment relationship of the question-answer pairs. We demonstrate the effectiveness of our model through a comprehensive evaluation over four challenging datasets, i.e., StarRef, TCM Reading Comprehension Dataset, OpenRefQA Dataset and MultiMCQ Dataset. We achieved state-of-the-art performance on all the datasets.
411. **ZEN: Pre-training Chinese Text Encoder Enhanced by N-gram Representations**  
    To read the data effectively, we propose a Siamese Text Encoder (TERE) model using a pre-trained Text Encoder (TE) network. Unlike previous works which applied the pre-trained TE network as a replacement for the original TE network, we adopt the pre-trained TE network to the encoder network and train TERE jointly. TERE, an improved variant of TE, captures more complex word properties in Chinese text by combining the similarities between the input and reconstructed word. The proposed TERE model can achieve 5.9% and 2.1% in F1-score on Open-Domain Sentiment and Open-Domain Sentiment Classification datasets, respectively. For the Open-Domain Sentiment Classification dataset, the pre-trained TERE model could achieve 0.54% and 0.39% on the test set. Our TERE model outperforms several recent state-of-the-art approaches for Chinese text classification tasks.
412. **Credibility-based Fake News Detection**  
    There has been an exponential increase in fake news over the past few years. Many communities are supporting and exploiting fake news. To address this issue, there has been much work on fake news detection. However, in contrast to previous works, we investigate credibility-based fake news detection with a visual approach. Visual content is of great help in detecting fake news. Using a dataset of 5.8M articles from Wikipedia and 6.5M news articles from the mainstream media, we proposed a 4-class CNN framework to detect fake news in two languages (English and Russian). In order to investigate different measures of fakeness, we then perform an ensemble of our 4-class CNN models. It achieves an accuracy of $92.52\%$ for English and $81.81\%$ for Russian, which are the highest reported fakeness accuracies. Our results demonstrate that the proposed method is effective for detecting fake news, making it a useful tool for researchers, journalists, and users of social networks.
413. **Human and Automatic Detection of Generated Text**  
    Automatic text generation is an important problem which can be applied to various problems and applications. In this paper, we propose a neural network model to detect generated text documents. As text data is an expensive resource, a process of selective sampling is utilized to reduce the text space, and a novel neural network is proposed to classify the documents based on the difference between the original and generated texts. The training of the network is implemented by minimizing a neural network loss function. The experimental results demonstrate that the proposed method can obtain more accurate text classification for low-resource text generation.
414. **Machine Translation Evaluation using Bi-directional Entailment**  
    Machine translation (MT) is the process of mapping a human written text into natural language (NL). In recent years, MT has become a widely used tool for many natural language processing tasks such as text summarization, data integration, and question answering. Recent studies have been done on MT evaluation and performance measure to objectively measure the quality of MT outputs. However, the quality of MT output is a subjective assessment which depends on human opinion. In this paper, we propose a Bi-directional entailment based N-gram MT Evaluation method which is based on sentence-level features extracted from the MT outputs of the neural MT model and LDA annotations of English source sentences. Our experiments on two MT datasets in different languages show the effectiveness of the proposed method in comparison with three baseline methods.
415. **GRAPHENE: A Precise Biomedical Literature Retrieval Engine with Graph Augmented Deep Learning and External Knowledge Empowerment**  
    Extraction of heterogeneous structured biomedical information on PubMed and other biomedical sites, such as DBpedia, is a highly complex task, because the terminology and semantics are vague and inter-related and important concepts may not be mentioned in the literature, especially when making inferences. An effective biomedical information extraction is also very challenging, since humans are hard to understand ambiguous concepts. Deep learning can be adopted to tackle biomedical information extraction in our paper to maximize efficiency, especially when annotation is costly. Our model, termed GRAPHENE, combines the unique strengths of graph processing and deep learning, and provides information on textual and biomedical concepts in the original text. In this paper, we propose to integrate the biomedical knowledge in the Biomedical Information Retrieval (BIR) task by taking advantage of external knowledge and a graph-based deep learning model. Specifically, to overcome the problem of terminology uncertainty, we first propose the method to extract all relevant biomedical concepts by employing standard relation extraction techniques. Then, a deep network is proposed to map biomedical terms into an embedding space, which is not only convenient to interpret the textual text, but also is useful for supporting the extraction task. Experiments are conducted on two real-world biomedical datasets and our model outperforms other state-of-the-art algorithms.
416. **Posing Fair Generalization Tasks for Natural Language Inference**  
    In this paper we investigate how to build a model that performs well in a number of natural language understanding tasks with a fair accuracy. To this end, we propose a method for testing whether a learned model is fair when it is applied to a task at hand with only (possibly) positive examples of the task. We develop a general framework in which the objective is to minimize the proportion of examples required to achieve good performance in a natural language understanding task. This framework is generic and can be used to examine models trained on tasks other than those given in the data. We test this method on three datasets, two tasks involving knowledge base completion and one involving the sentiment analysis of newswire articles. Experimental results show that our method is effective in reducing the need for positive examples in various tasks and settings.
417. **Machine Translation in Pronunciation Space**  
    We describe a method for improving machine translation by using machine learning to learn the pronunciation space. We encode speech information in a vocoding dictionary that specifies each word in the spoken vocabulary in terms of phonetic features. The encoding is guided by a pre-trained linear support vector machine to minimize similarity with the target phonetic representation, while accounting for its similarity with an "incorrect pronunciation dictionary". Experiments on English-French (F$^2$) and English-Russian (RG$^2$) languages show that our method has a statistically significant advantage over a phoneme-based method for both languages.
418. **Question Answering for Privacy Policies: Combining Computational and Legal Perspectives**  
    This paper focuses on a multi-class approach for question answering that combines the benefits of machine learning with the normative and legal approaches for the protection of personal information and due process. We assume an adversary seeks to infer a privacy policy from an open-domain dataset which, on one hand, limits the information that is available about the user and on the other hand, offers a platform for learning from the user's response to a task, which is essential to enhance the predictive modeling capabilities of question answering systems. In this work, we train a conditional generative model for questions whose answers are expected to fall into an expected privacy policy. This approach is then used in conjunction with a state-of-the-art machine learning algorithm for the predictive modeling of response to a task. We evaluate this approach on the datasets from two different domains, and show that it yields more accurate answers to task-oriented questions while still being sufficiently general to be used in the context of other questions that fall into different privacy policies.
419. **MRNN: A Multi-Resolution Neural Network with Duplex Attention for Document Retrieval in the Context of Question Answering**  
    Recently, there have been a number of research efforts in document retrieval based on the extraction of long documents (e.g., biomedical articles) from biomedical literature. This task becomes more challenging when considering the uncertainty in the form of irrelevant or noisy words. In this paper, we present a multi-resolution neural network model, MRNN, for question-answering based document retrieval. A sequence of sequence layers, including a cell of single rank, a cell of multi-rank and a cell of span layer are designed to encode a document, and the relations among the documents are exploited to construct the query-document connection. Experimental results show that MRNN achieves state-of-the-art performances in several benchmark datasets.
420. **Controlling Text Complexity in Neural Machine Translation**  
    Language models are now ubiquitous in modern neural machine translation systems. Despite their success, language models still face serious challenges with regards to processing large amounts of text. Many language models in the literature lack knowledge of the lexical representation of the text, meaning that they do not automatically capture all the nuances of complex sentence structures. In this paper, we present a method to control the complexity of language models at the sentence level, and apply it to two state-of-the-art neural machine translation (NMT) systems. We evaluate our method by comparing its performance to a strong baseline on two languages with intricate sentence structures. We find that our method outperforms a number of standard language models on both datasets. Furthermore, we show that our method successfully controls for the lexical representation of text in NMT models, and further shows that such control leads to improved performance on downstream text. Finally, we show that our method can help mitigate the tendency of current NMT models to produce inappropriate transcripts for rare words and phrases.
421. **BAS: An Answer Selection Method Using BERT Language Model**  
    In this paper, we propose an answer selection method using BERT language model. We use domain expert's ratings for assigning the question to different answer candidates. Unlike previous studies that employ a conventional attention based method for question scoring, we integrate the attention model into the language model for both BERT training and evaluation. This method performs better than previous ones in terms of both BLEU and METEOR metric, indicating that our method is robust to noisy annotations. Furthermore, by integrating the attention model into the model training process, our method shows better performance on the standard benchmark datasets.
422. **Integrating Dictionary Feature into A Deep Learning Model for Disease Named Entity Recognition**  
    With rapid growth of Electronic Health Records, there is a great need for automatic disease named entity recognition. Most existing systems usually depend on external corpora (e.g. PubMed) for learning the disease named entity. However, generating these corpora is expensive and labor-consuming. To overcome this problem, in this paper we propose a novel model that integrates dictionary feature extracted from human annotated data and dictionary feature learnt from medical texts. Our model learns the importance of disease named entities in different medical text. Experiments on two publicly available disease named entity recognition datasets (NIH-Rural1 and CI-Rural1) show that the proposed model outperforms previous state-of-the-art models.
423. **Improving Bidirectional Decoding with Dynamic Target Semantics in Neural Machine Translation**  
    Neural machine translation has shown promise for large-scale machine translation. However, state-of-the-art approaches are not yet mature enough to meet the requirements of end users in terms of quality and robustness. With the growing demand for high quality NMT systems, we investigate how to improve decoding by introducing a novel approach, referred to as bidirectional decoding with dynamic target semantics, which produces low-quality words and phrases according to their semantic relatedness to the target word. To demonstrate the potential of our approach, we implement our decoding framework on top of the Bi-directional Transformer model. We evaluate the effectiveness of our framework on two translation tasks: monolingual Japanese-English and English-German translation. We further analyze the resulting high-quality outputs by analyzing the semantic relatedness of the words and phrases. We further show the potential of the dynamic target semantics by executing a series of experiments in a scenario where the target is selected randomly from a large vocabulary.
424. **Review-based Question Generation with Adaptive Instance Transfer and Augmentation**  
    Due to the high cost of feature engineering and retraining in a question answering (QA) system, existing systems require a large amount of training data in order to achieve good performance. However, it is very difficult to get sufficient training data for all queries. Existing works focus on generating a wide range of questions from a few labeled training examples and successfully employ instance adaptation (IA) to alleviate the uncertainty of unseen queries. We study the problem of natural question generation and introduce a novel adaptive question generation framework. In our framework, we first develop a natural language question generation network (NL-QG-Net) that can generate a wide range of questions from few labeled examples. Next, we adapt the training of NL-QG-Net based on the queries' labels. Additionally, we propose a class-agnostic instance transfer strategy that enables us to improve the performance of NL-QG-Net by applying a large-scale instance tagging in the language models. Experiments on four benchmarks show that our approach is effective and can achieve the state-of-the-art performance.
425. **embComp: Visual Interactive Comparison of Vector Embeddings**  
    Similarity measures are ubiquitous for many computer vision problems, but how to detect the inner-relation among different types of images is still an open problem. In this paper, we address this problem by introducing a novel framework called Embedding Vector Embedding (EmbComp). EmbComp finds an embedding space in which a pair of visual data points belongs to the same semantic space, but the embedding space is different from the training data space. For example, an image with the phrase 'crab' on it can be learned by embedding it in the semantic space 'liver', while an image with the phrase 'fly' on it can be learned by embedding it in the semantic space 'body'. Our proposed EmbComp is based on vector multiplicative projection on the Euclidean distance and nearest neighbour search in the embedding space. It is able to find the closest embedding spaces to semantic space by exploiting the similarities between them, and to efficiently learn these spaces. It supports efficient search for different types of similarities. Our empirical results show that EmbComp is able to automatically learn high-quality visual similarities for a wide range of real-world tasks including semantic object/instance retrieval, image classification, image classification with subspace clustering, human pose estimation, and aspect ratio estimation. In particular, EmbComp achieves state-of-the-art performance on these tasks.
426. **Knowing What, How and Why: A Near Complete Solution for Aspect-based Sentiment Analysis**  
    In this paper, we propose a framework for aspect-based sentiment analysis. It leverages on deep learning methods to obtain features which describe sentiment direction in a sentence. This allows the detection of semantics and inferring the sentiment of sentences. We test our approach in a pilot corpus and show that it achieves substantial improvement over other state-of-the-art sentiment analysis systems, including a state-of-the-art BiLSTM method. We also show that the sentiment of sentences can be estimated even if they have different aspect-types, thus indicating its ability to identify semantical expressions.
427. **A Novel Approach to Enhance the Performance of Semantic Search in Bengali using Neural Net and other Classification Techniques**  
    Bengali is a very popular language among millions of people around the globe, yet its language information is usually ignored by language learners in online English classes. In this paper, we explore the usage of neural net in classifying the words of Bengali word based on their surrounding context. We firstly present a comparative analysis of the performance of neural net and traditional NLP tools on Bengali corpus using corpora of Bengali word. Next, we use historical Bengali corpora to validate the proposed system and compare the performance of neural net and traditional NLP tools on this corpus. We also use semantic similarity analysis on various candidate words in English corpus and extract the concept embeddings from the candidate words based on their context. The evaluation results show that the proposed approach can boost the accuracy of word classification to about 70.33% without affecting the quality of the resulting document.
428. **Examining UK drill music through sentiment trajectory analysis**  
    This paper focuses on examining the influence of topical features of drill music on its perceived sentiment spectrum. We present a dataset consisting of over 200,000 Instagram posts over two months (Aug - Sep 2018) as well as a sentiment trajectory analysis model to allow us to explore the temporal evolution of genre as a function of topical features. We demonstrate how topical features can be used to measure the quality of a social media snapshot (i.e. time-series) of a song's perceived sentiment at a smaller granularity. We find that (a) a study on lyrics popularity which uses word frequency and weighted topic importance as features can improve upon shallow word frequency approaches; and (b) the trajectory trajectory of a song can be used to predict its position in the song-genre hierarchy and to understand the spread of trends in the genre.
429. **Analysing Coreference in Transformer Outputs**  
    We analyze the performance of Transformer-based neural machine translation on the task of coreference resolution, specifically coreference accuracy and linguistic unit resolution. In particular, we focus on the transformer outputs as input to the system, rather than those produced by a decoding model. We observe that compared to a baseline model for coreference resolution, the output of Transformer significantly outperforms the baseline model at the highest scoring ability score (on the 500-level of normalized CoNLL-2000). Further, we observe that the output-based Transformer model, when using the top layer of a Transformer, improves performance on various multi-domain corpora. To assess the viability of our findings, we compare the performance of Transformer with various existing models in similar configurations, as well as introduce new models that we are using for other coreference tasks.
430. **What does a network layer hear? Analyzing hidden representations of end-to-end ASR through speech synthesis**  
    We present a deep learning approach that relies on an end-to-end system with an encoder/decoder framework that directly translates speech into text without any handcrafted features. It is trained in an end-to-end fashion, with both components employing convolutional layers. In order to take advantage of the information in the utterance from the generator to determine the output, we utilize an attention network which maps an encoder's output into a score computed by using the encoder's latent representations. This attention score is then passed to an LSTM network to learn the hidden representations that will be utilized in the next LSTM layer. We introduce different training schemes that gradually fine-tune the end-to-end network, and have been tested on the Switchboard speaker recognition task and on the CoNLL-2005 test set. We find that the attention network learns the model's encoder representations for end-to-end ASR, thus enhancing the discriminative power of the decoder layer.
431. **A Holistic Natural Language Generation Framework for the Semantic Web**  
    We present a framework for the generation of non-deterministic descriptive sentences for domain-specific text. Our approach allows for the creation of text that is not only coherent, but also fluent and informative, allowing a variety of applications in the Semantic Web, including Web search and document summarization, and synthetic conversations in online forums.
432. **Learning to Annotate: Modularizing Data Augmentation for Text Classifiers with Natural Language Explanations**  
    The issue of natural language explanations for data augmentation is crucial to both large-scale text classifiers and deep learning models. In this paper, we propose an efficient natural language explanation architecture for text classifiers by decomposing text classifiers into sequence-to-sequence architectures and introducing an attention module on the sequence-to-sequence model. Our architecture can be easily deployed with different models to learn both human-interpretable and machine-interpretable attention models for text classifiers. The human-interpretable attention model leverages natural language understanding to produce an interpretable target text, and the machine-interpretable attention model returns the inference result for the corresponding text instance. Compared with previous approaches, our proposed architecture can generate meaningful textual explanations with less resources while achieving the state-of-the-art results in the BLEU-4 score.
433. **Affective Behaviour Analysis of On-line User Interactions: Are On-line Support Groups more Therapeutic than Twitter?**  
    However, attitudes and opinions are subjective and there is no clear cut set of markers that can be used to assess how an individual will react to a given piece of information, such as news article, comment, text message, etc. To capture these factors, researchers use analysis of multi-dimensional data, which can be described using numeric representations of attributes. One such numerical representation is the emotion level. In this paper, a study on emotion recognition in offline social support groups from the online journal Inside Higher Ed, shows the value of emotion representation. From the experimental analysis, it is observed that non-fluents exhibit higher emotional profiles compared to the seniors. One study shows that, in order to capture information on support groups and offline social interactions better, the arousal level, interdependence of emotions and dimensionality of emotional profiles need to be taken into account.
434. **On the Effectiveness of the Pooling Methods for Biomedical Relation Extraction with Deep Learning**  
    Neural Networks have been successfully applied to the task of Relation Extraction, achieving an outstanding performance on the benchmark datasets. Deep learning methods, on the other hand, have demonstrated their capabilities in many other natural language processing tasks including text classification, machine translation, and speech recognition. We aim to investigate the effect of pooling methods, defined as soft-alignments between words and relations extracted from a relation tree, on the generalization ability of a deep learning model in the task of relation extraction. Our findings show that dropout and maxout can give rise to state-of-the-art performance. More so, maxout shows an increased robustness to noise and alignments other than word alignments, and it is able to generalize well to unseen relations and even unseen words. We also find that pooling can greatly decrease the number of annotated relations required to train the model. We perform a set of experiments, conducted on the Bernoulli Extractor task, that are aimed to understand the relationship between the accuracy of a model trained with dropout and the number of annotations required for training.
435. **Emerging Cross-lingual Structure in Pretrained Language Models**  
    Recent work has shown that pretrained language models can be used to achieve state-of-the-art results in many downstream NLP tasks (e.g., language modeling, document summarization, sentiment analysis). However, empirical evidence suggests that these models are learned in a cross-lingual manner, i.e., not only in the same dataset for training, but also across different language pairs (e.g., English and French). In this work, we study the cross-lingual characteristics of pretrained language models, by learning their cross-lingual distributions with multilingual variational autoencoders (MVAE), with a focus on new language pairs (German and English), and evaluate them on downstream NLP tasks. Experiments demonstrate the significant value of cross-lingual pretraining. On a sentiment analysis task in German, a pretrained language model outperforms a pretrained language model trained on English alone by a large margin. Our study also suggests that pretrained language models may benefit from downstream tasks that involve data from new languages.
436. **A Failure of Aspect Sentiment Classifiers and an Adaptive Re-weighting Solution**  
    The aspect classification task aims at identifying the group of words, on the basis of a set of aspect-labeled texts, that expresses a particular opinion about the aspect of interest. The problem has achieved success in the past and it has shown a great potential in handling the complex of aspects of different types, e.g. in deciding whether a product is unsafe or if a movie is fit for children or elderly people. Although a number of very popular aspect classification systems have been proposed, their performance is often not satisfactory. In this paper, we analyze a number of aspects of the texts and find that most of them do not match the characteristics of what the task is based on. Motivated by this analysis, we propose an aspect re-weighting solution, which performs re-weighting on the terms that provide the most objective support to the aspect, the terms that are further adapted on the output of the system according to their degree of aspect engagement. The proposed solution is compared with several existing methods, including classical aspect classification and a newly proposed approach, and we demonstrate the effectiveness of the proposed approach on several aspects. In addition, we describe two challenging cases where it is difficult to perform robust and accurate aspect classification: text copying and lengthy or plagiarized text.
437. **Emergence of Numeric Concepts in Multi-Agent Autonomous Communication**  
    Machine learning and artificial intelligence techniques such as deep neural networks have become increasingly popular tools to analyze machine learning models. This paper proposes an evolutionary analysis of non-linguistic communication signals from a multi-agent cooperative system to show that a certain type of fuzzy logic allows an AI agent to predict new activation patterns that enable the further development of new communication forms. Furthermore, this paper demonstrates a potential application of fuzzy logic to pattern discovery using data fusion, a type of machine learning that analyzes structured, time-sequential signals. As a case study, this paper utilizes a simulated multi-agent system in which multiple human players collaborate to construct a soccer team and generate a season-long record. Using these signals, the corresponding actual numeric concept representations are discovered that demonstrate emergent numeric concepts that improve team performance in soccer.
438. **Predictive Engagement: An Efficient Metric For Automatic Evaluation of Open-Domain Dialogue Systems**  
    This paper introduces an Efficient Metric (EMA) for automatic evaluation of dialogue systems (EDS). EMA is a new, simple and practical measure that focuses on the performance of the top-performing dialogue system (i.e., the dialogue system being compared) rather than the performance of an end-to-end dialogue system. The empirical study over the BU- Dialogue corpus shows that EMA can be adopted for a wide range of evaluation measures. Further, we have benchmarked EMA over two case studies and found that EMA is an efficient method for automatic evaluation.
439. **Recurrent Instance Segmentation using Sequences of Referring Expressions**  
    Recent approaches to instance segmentation use multi-scale convolutional neural networks (CNNs) which perform a binary segmentation of the instance, conditioned on the instance's bounding box. This work proposes a novel way of segmenting an instance by encoding the referring expression in the network output directly. We propose to incorporate a sequence of phrases into the network output as well as consider multiple region candidates for each particular phrase. We first used two different multiscale architectures, convolutional and recurrent, to address the challenging task of separating an instance of one object from those of another. The proposed method, referred instance segmentation (RI), outperforms both the state-of-the-art methods on the Pascal VOC, MSCOCO and SUN benchmarks. We show that the proposed approach outperforms both encoding and decoding modules of each CNN and improves segmentation accuracy by 15-20% compared to state-of-the-art methods.
440. **Unsupervised Cross-lingual Representation Learning at Scale**  
    Representation learning for Natural Language Understanding has become a very active area of research due to its widespread applications in many NLP tasks, such as machine translation, question answering, summarization, and image captioning. The necessity of achieving the cross-lingual representation learning for multilingual NLP tasks, however, has not been well-studied yet. In this paper, we show that feature-level statistics can be used to learn cross-lingual representations that generalize well even in scenarios with diverse sources of data, like text corpora. In particular, our experiments show that the features learned by the best performing model in the shared task for eight language pairs using 20 training and 1 test corpus, outperform those of a supervised deep learning model by more than 15 BLEU points.
441. **Multi-Paragraph Reasoning with Knowledge-enhanced Graph Neural Network**  
    Contextual multi-sentence reasoning, where sentence understanding is a prerequisite, is an important challenge for neural models. Previous works usually pursue the method of multi-paraphrase with auto-encoder as a way to improve multi-sentence representations. However, few studies focus on graph-based interpretable representations that can be further enhanced by the graph neural network. To this end, we propose a new graph-based neural network model that can further enhance the contextual multi-sentence representations. As a concrete example, we generate more precise long-distance mentions to boost the contextual multi-sentence representations. This is achieved through a novel bi-directional information fusion process that can be effectively controlled by an external network controller. To validate the effectiveness of our proposed model, we compare it to several baselines in the task of multi-sentence relation classification. Results show that our model significantly outperforms several existing state-of-the-art models.
442. **Small-Footprint Keyword Spotting on Raw Audio Data with Sinc-Convolutions**  
    Keyword spotting (KWS) from raw audio data remains a very challenging task due to the very large noise distribution. Although deep neural network models have been proposed to tackle the KWS problem in the supervised setting, they often suffer from the sensitivity to the amount of background noise. This sensitivity is due to the model's restricted regularization parameter space. In this paper, we present a novel two-stage deep neural network architecture that achieves high detection rate and fast inference speed while retaining similar classification accuracy with a less number of parameters, compared to supervised approaches. Experiments on two different data sets show that the proposed model outperforms all recent baselines in terms of detection rates and inference speed.
443. **Discrete Argument Representation Learning for Interactive Argument Pair Identification**  
    In recent years, interactive argument detection has gained increasing interest due to its potential for use in various tasks in computational argumentation and learning theories, such as annotation of arguments for reading comprehension. Since these tasks require identification of argument topics, argument retrieval, and annotation of arguments for understanding, they require the learning of representations of arguments in a pretrained distributed representation. However, most existing approaches for interactive argument detection are restricted to representations for binary arguments, ignoring the possibility of representing multi-way arguments. This paper proposes a novel model, Online Argument Representation Learning (ORL), for interactive argument detection, which leverages recent advances in semi-supervised argument representation learning to learn and represent interactive argument representations in a multi-way manner. In the proposed approach, a learning agent for interactive argument detection is formed by forming representations for argument topics and argument keywords. For retrieval, a key observation is that, in a database, the argument topics are distributed over a spatio-temporal set of arguments, which may be presented as multi-way signals. ORL uses the multi-way signs of the argument topic representations to provide better retrieval results by leveraging global information to identify arguments across different sentences. For annotation, ORL uses a modified sequence-to-sequence learning approach to annotate sentences for argument topic and keyword labels. The proposed model is evaluated on two different datasets, namely Stanford and Arguments for reading comprehension. The experimental results show the superiority of ORL, particularly on retrieval, annotation, and understanding tasks, compared to the current state-of-the-art approaches.
444. **Incremental Sense Weight Training for the Interpretation of Contextualized Word Embeddings**  
    Contextual word embeddings generate interpretable word representations based on the contexts in which the words occur. However, the interpretation of such contextualized embeddings is challenging for human beings. The direct application of the contextualized word embedding theory to natural language understanding is however still in its infancy. In this paper, we address the uninterpretable contextualized word embedding problems by proposing an incremental sense weight training approach. We reinterpret the contextualized word embeddings by designing the standard sense of the word by performing the sense weight training. Intuitively, this simple sense weight training can be viewed as incrementally determining the ``base sense'' for the word in the context. We empirically show the effectiveness of the incremental sense weight training in several tasks, e.g., character-level language modeling, character-level sentiment analysis, and part-of-speech tagging.
445. **Adversarial Language Games for Advanced Natural Language Intelligence**  
    Artificial intelligence systems such as computers and intelligent tutoring systems can be trained by "natural language grammars" to give answers to their questions. Recently, there has been much interest in adversarial approaches to training language models to distinguish between their natural language input and their answer, using techniques that could potentially be applied to other fields. In this paper, we introduce and describe a recently proposed family of adversarial language games, the generalized game, and its various variants, for the improvement of natural language natural language understanding, given as inputs to a language model. The generative-supervised models, generated by the language game, then compete with a task-conditioned error-correction model trained on large corpora, aiming to improve their understanding of their inputs. We illustrate the generality of the language game with the detection of typos and semantic differences in a toy model. On the other hand, we show that the different games make different contributions to the generalization of the error-correction model, that is, through its natural language understanding of its inputs.
446. **Coreference Resolution as Query-based Span Prediction**  
    Coreference resolution is a task that is fundamentally important to a spoken-text understanding system, which is used to resolve mentions of entities and their relations to the given context. Although the task has been intensively studied over the past two decades, its practical use is currently limited by the lack of large annotated corpora with correctly resolved coreference instances, which are required to learn the task's statistical models. To bridge this gap, we propose a novel framework for coreference resolution that treats the coreference resolution as a question-based sentence-pair tagging task. We then propose a deep neural network model that considers both coreference resolution and sentence-pair tagging tasks simultaneously and exploits the important discriminative power of the two modules. We compare our model with a popular state-of-the-art coreference resolution model on the benchmark NIST PTB corpus, and experimental results show the superiority of the proposed model in both task-driven and non-task-driven aspects.
447. **Self-Attention and Ingredient-Attention Based Model for Recipe Retrieval from Image Queries**  
    This paper presents a novel self-attention and ingredient-attention-based model for recipe retrieval from images. This model efficiently enhances the retrieval performance by 1) leveraging attention on textual properties, e.g., descriptions of ingredients, to generate better-quality responses, and 2) selectively attending on ingredient-level information to avoid bottleneck responses caused by many irrelevant ingredients and regions. We propose to enrich the latent space by searching for high-quality ingredient-level responses, whose contents are then applied to a multitask logistic regression model. In a systematic evaluation of more than 36,000 recipes, we observe significant improvements over the baseline model by 6.8% and by 15.7% with attention weights on textual properties, and substantial improvements with attention on ingredient-level information.
448. **Deepening Hidden Representations from Pre-trained Language Models for Natural Language Understanding**  
    The objective of language understanding is to predict the logical form of a natural language sentence given its meaning. In this work, we introduce several simple yet effective deep-learning architectures that learn from pre-trained language models and classify ambiguous sentences in large corpora. Our experiments show that one such model has an absolute improvement in terms of BLEU@4 of 5.5-7.0 with 16-bit floating point quantizers and a relative improvement in terms of BLEU@4 of 3.9-4.0 with 32-bit fixed point quantizers. While this is a relatively small gain for pre-trained language models, it gives strong evidence that they are performing well in various language understanding tasks. Furthermore, we show that without access to gold clauses, our pre-trained language models can achieve at least state-of-the-art accuracy for English natural language inference on the Web. The code of this work is publicly available.
449. **Low-Resource Machine Translation using Interlinear Glosses**  
    Limited corpora and poor parallel corpora are an important challenge to low-resource machine translation research. This problem is particularly acute for isolated languages, such as Chinese, where a large portion of the training text is unavailable or corrupted by spelling mistakes. This paper develops an approach for reducing the impact of such errors on low-resource machine translation by using interlinear glosses, by introducing a different alternative to using a parallel sentence from a source language as parallel input. The proposed approach significantly outperforms the state-of-the-art on low-resource English to Chinese.
450. **Exploring Hierarchical Interaction Between Review and Summary for Better Sentiment Analysis**  
    With the rapid growth of online social media, users make their opinions known and share their opinions with others. People tend to read and write opinions and feedback on online social media to understand the feelings, sentiments, and reactions of others. In such a social network, the first step towards understanding the user's opinion and sentiments is to recognize their metadata. These metadata are generally expressed in review and the user's rating. Based on reviews, we extract the full information from the text. However, the majority of the existing works do not pay attention to the hierarchical interaction between review and summary in processing user's opinion and sentiments. In this paper, we present a novel deep learning based model for sentiment and opinion classification from review and summary data. For user's review, we apply a histogram based extraction to summarize user's metadata, and for user's summary, we employ a attention based Recurrent Neural Network model to process user's metadata and compute the sentiment label. To evaluate our model, we use two publicly available datasets, TREC 2013 Feedback and MTurk user review datasets. Experimental results show that our model outperforms existing state-of-the-art models. In terms of precision, we achieve 93.33%, 95.10%, and 95.79% in processing user reviews, user's reviews, and their summaries respectively. Our model performs better than some state-of-the-art approaches by achieving 93.66%, 95.38%, and 95.29% in accuracy for user review and summary, respectively.
451. **Understanding Knowledge Distillation in Non-autoregressive Machine Translation**  
    The capacity to learn on a data source is essential for current state-of-the-art machine translation systems, but training data scarcity remains an issue for many translation tasks. Transformer model for neural machine translation achieves strong results on a lot of translation tasks such as translation of biomedical texts. However, the Transformer architecture lacks efficient knowledge distillation to train more general models, and this result may be related to the models' capacity to use big unlabeled data. In this work, we analyze the effect of a knowledge distillation strategy (synthesis and distillation) in an encoder-decoder neural machine translation model. We observe that the model performance is highly sensitive to the strategy, and there is an alignment between the performance of the original training model and the models' capacity to use large unlabeled data.
452. **Porous Lattice-based Transformer Encoder for Chinese NER**  
    Recent NER methods often focus on learning sentence embeddings of characters, e.g., i-Vectors or Character2Vec. However, recurrent neural network models provide more flexible structures for capturing complex local features and more effective information aggregation. More specifically, they can be applied to continuous input tasks such as Chinese NER with hard constraints on the frequency and direction of word tokens. In this paper, we propose a pore-based model to explicitly model the structural correlations between words, which is more efficient than the conventional Vectors and character-level encoders. Furthermore, we introduce the Porous Lattice-based Transformer (PLT) to construct a rich local context for network representation and perform global attention of features in the encoder. Extensive experiments on both Chinese-English and English-French NER tasks show the effectiveness of the proposed approach.
453. **Incremental Text-to-Speech Synthesis with Prefix-to-Prefix Framework**  
    This paper proposes an efficient Incremental Text-to-Speech Synthesis model, that learns both prefix-to-prefix (P2P) and suffix-to-prefix (S2P) rules incrementally by reducing the syntactic trees. The model comprises two stages, in which first, text-to-char and text-to-character are learned by both fixed-length training and rule expansion. And the second stage, which is the key to the efficiency of our model, is learned by dividing the source text into subsets by searching the characters within the source text, applying the same training and expansion rules. The two-stage model achieves better qualitative performance compared with other incremental methods and at the same time improves the performance on generating speech from the source text. The experiments show that the proposed model improves the mean absolute errors by 16% compared with the state-of-the-art method for text-to-char generation and 13% compared with a baseline S2P model.
454. **Query-bag Matching with Mutual Coverage for Information-seeking Conversations in E-commerce**  
    Query-bag matching (QBM) has been investigated to reduce false positive classification rates of machine learning models. However, there has been little work in this direction in E-commerce search, as direct queries are very expensive and time consuming for building and testing a model. To this end, we propose a mutual coverage method which takes into account partial classifications of partial queries (corresponding to "context") and thus can reduce false positive classification rates. The resulting models are able to improve the search efficiency by keeping a higher fraction of false positives. We evaluate this method on a real-world search query data set. In this work, we present results in terms of Mean Average Precision (MAP), Full-F1 score, False Positive Rate (FPR), and Open Graph F1 score. We also discuss how mutual coverage affects the overall search efficiency.
455. **SubCharacter Chinese-English Neural Machine Translation with Wubi encoding**  
    Neural machine translation (NMT) models produce weak translation outputs, lacking the semantic information of the original. Most approaches to improving NMT often rely on auxiliary systems or data augmentation techniques. Recently, there has been much progress in deep bidirectional neural networks that allow word-level translations, such as GloVe. However, in contrast to bidirectional neural models, these models require copying the entire sentence into memory and therefore are very memory-intensive. In this paper, we propose a new way to improve subcharacter-level Chinese-English translation models using Wubi encoding. We observe that Wubi encoding can not only capture more semantics of the original sentence but also be significantly faster. We also observe that Wubi encoding leads to better translation quality as measured on the WMT17 Chinese-English data. Our code is available online: https://github.com/wdullahalp/subcharacter_encoding.
456. **SentiLR: Linguistic Knowledge Enhanced Language Representation for Sentiment Analysis**  
    An important and challenging task in sentiment analysis is to represent and retrieve the sentiments of a piece of text. Recently, many language embedding methods have been proposed, which achieve better performance than the traditional bag-of-words (BoW) model, in particular in the application of sentiment analysis. The widely used bag-of-words (BoW) model needs to acquire a large amount of labeled data and then use the linguistic information (i.e., word embedding) to transform the text into a fixed-size vector. However, the available training data are often scarce, which may hamper the performance of these language embedding methods. In this work, we introduce a novel method called SentiLR, which obtains a mixed-genre corpus and utilizes it as a new source of linguistic information. In addition, we propose a novel semantic similarity measure, which is able to assign the final sentiment labels to the text elements (e.g., characters and words), which is an improvement over the usual word embedding model. The experiments on the SemEval-2017 task-aware sentiment classification and semi-supervised sentiment analysis show that the proposed methods achieve significant improvements over the state-of-the-art methods in both sentiment classification and semi-supervised sentiment analysis tasks.
457. **Shaping Visual Representations with Language for Few-shot Classification**  
    We propose a simple method to learn visual representations that are more easily transferable for the few-shot learning paradigm. We introduce a novel way to discover additional features from unlabeled image data using language descriptions. Specifically, we use unlabeled natural images with single-labeled classes and find common visual patterns using text descriptions. Our framework learns visual representations by finding and preserving such common visual patterns. We evaluate the performance of our approach by adapting a basic few-shot classifier and showing significant improvement. This work highlights how to learn visual representations that are more transferable.
458. **Enriching Conversation Context in Retrieval-based Chatbots**  
    Humans make an average of 28 conversational stops during a conversation, often due to coreference resolution or other impulse-driven phenomena such as heated discussions. We hypothesize that human-computer interaction facilitates greater context-aware search and retrieval. In this paper, we introduce a retrieval-based conversation context enhancement model that allows a model to obtain insight about context at multiple granularities. The key novelty in this model is that the model is made aware of different contexts within the query string at each step of a latent vector space representation of the text. Through the use of different contexts, we show that the proposed model can leverage richer contextualized search results and can contribute to semantic parsing of the natural language input. Additionally, the latent vector representation provides the model with the ability to indicate informative context-aware language factors such as verbosity, so that it can be further used in the context-aware textual entailment task. Experiments with a publicly available dataset show that the proposed model significantly improves upon the baseline model when it has access to additional contexts at each step of the latent vector representation.
459. **Guiding Non-Autoregressive Neural Machine Translation Decoding with Reordering Information**  
    Recently, neural machine translation (NMT) has achieved great success on generating highly quality output. However, due to the latent structure of language models, decoding NMT models at the sentence level is still a very challenging task. In this paper, we propose a novel method named Guiding NMT to help NMT decoding by reordering sentence vectors to be more similar in the latent space. Unlike most methods of information guided NMT (IG-NMT), our method is more effective to guidance the decoding process, especially at the encoding stage. Extensive experiments on Chinese-English translation showed that our proposed method achieves similar translation quality with the state-of-the-art decoder and training method. Moreover, our model is stable on different language pairs.
460. **Toward Dimensional Emotion Detection from Categorical Emotion Annotations**  
    In this paper, we propose a technique for detecting emotion states from categorical emotion annotations. As emotion states have a closed-world property, a labeled dataset is not available in the therapy setting, but only in the clinical setting, where the lexicon is available. The current method for emotion detection in categorical emotion annotations, referred to as emotion matrix factorization (EMF), calculates dimensionality and shape of emotion states based on the normalized intensity feature, and use these to determine whether or not an emotion has been identified. We define the emotion states of a thought pattern as the co-occurrence of adjacent states. As states correspond to states with similar activations, we use weighted K-nearest neighbors (K-NN) to measure the similarity between emotions. Our technique, EmotionMeter, detects emotion states by extracting emotion states from a given thought pattern. We show experimentally that our technique can detect emotion states from a maximum of 84% of a cognitive evaluation of a toy car for healthy people (average accuracy 91.8%), and can detect emotion states from a maximum of 74% of a cognitive evaluation of a baby cat for dyadic couples (average accuracy 92.3%).
461. **Optimizing the Factual Correctness of a Summary: A Study of Summarizing Radiology Reports**  
    Extracting and reporting facts is the central task in medical informatics. Often, patients are described as specific individuals or groups. In this context, the factual nature of the statements made should be taken into account. Therefore, in this paper, we investigate the extraction of useful insights from radiology reports using artificial intelligence techniques. To measure our effort we evaluate the ability to find relevant concepts to provide context-aware information to patient-specific sections in the summaries. We show that extracting concepts that are relevant to the section (i.e., fact) is more important than the more abstract concepts (i.e., attributes). The research enables us to improve the interpretation of abstract statements in the published summaries.
462. **Gextext: Disease Network Extraction from Biomedical Literature**  
    We introduce Gextext, a tool that extracts disease-specific drug-like pathways and temporal trajectories from electronic medical documents (EMDs) automatically. Instead of performing parameter estimation on a set of simulated data, Gextext learns the dependencies between different drug-like pathways using an effective adversarial learning mechanism to generate synthetic disease-specific EMD data. In order to evaluate its effectiveness, we have made three large-scale EMD datasets for the Xingang Experimental Heart Study, which include pathways identified by structural gene expression data. We evaluate Gextext on all three datasets and compare its performance with traditional disease prediction methods, including multi-label and graph-based methods. The results show that Gextext can achieve significantly better performance than most state-of-the-art tools for disease identification.
463. **Towards Domain Adaptation from Limited Data for Question Answering Using Deep Neural Networks**  
    In the era of big data, it is necessary to efficiently and accurately estimate the performance of question answering (QA) models based on limited data. However, current QA models are easily fooled by a small set of random-noise samples of the target domain. There are several methods to alleviate this problem of limited data and similar approaches have been proposed for domain adaptation of other NLP tasks. In this work, we introduce the issue of domain adaptation of QA models and propose a neural network that exploits a small amount of unlabeled data for solving this problem. The main contributions of this work are: (1) we propose a method for unsupervised domain adaptation by using domain knowledge; (2) we investigate different models for domain adaptation; and (3) we propose a technique to adapt the architecture of the network by adapting parameters such as the number of hidden layers and the sampling distribution. We evaluate our models on two different tasks for question answering (i.e., Stanford Question Answering Dataset and SemEval-2010 Task 10) and the experimental results show that our proposed method performs better than state-of-the-art domain adaptation methods on the Stanford Question Answering Dataset and SemEval-2010 Task 10, while the performance on the two tasks is not significantly different (e.g. on SemEval-2010 Task 10 our method achieves 75.8% accuracy).
464. **Open Domain Web Keyphrase Extraction Beyond Language Modeling**  
    Keyphrase extraction is a fundamental step in web search. Unfortunately, this task is relatively low-cost, and is easily solved with statistical models that predict keyphrases in terms of common terms. However, these approaches suffer from several shortcomings. First, keyphrases are often heavily correlated, leading to inconsistent prediction and poor diversity in keyphrases. Second, they typically contain many rare terms, rendering them hard to understand and to detect. Third, these models are based on language modeling, which limits their application to a relatively small subset of the web, thus limiting the scalability and performance. To tackle these problems, we propose a novel approach based on language modeling for extracting web keyphrases, which outperforms prior work on a variety of keyphrase quality metrics. We develop and implement a hyperparameter-free language model, which does not rely on feature engineering and performs well on all metrics. This model, which we call Paraphrase LSTM (PLSTM), improves the previously best-performing model on four standard tasks (clue-word, query-word, answer-word and subject-verb-object) by more than 4%.
465. **Guiding Variational Response Generator to Exploit Persona**  
    We propose a new control parameter called persona. Our experiments show that the proposed persona controls the model's response to an input that was crafted to imitate a target human being. The optimal persona increases the effective learning rate of the model by creating a proper grammar of the desired response and by maximizing model sample coverage. As demonstrated in our experiments, the persona parameter can be tuned dynamically to provide the network with a diverse set of responses, allowing it to generalize better and yield good performance in a wide range of tasks.
466. **Unsupervised Common Question Generation from Multiple Documents using Reinforced Contrastive Coordinator**  
    This paper proposes a novel framework for unsupervised common questions generation from a text collection. The proposed framework leverages context and statistics to generate answers from multi-round collaborative strategy, which is an automated system of design evaluation (DEE) process. Through ELMo approach, we are able to adapt our system to reflect both external and internal context of the user. By capturing topic and domain information during question generation, DEE process is capable of maintaining users' interest and keeping the questions relevant to user's interest. Additionally, context information of the user can be included to control the length of generated questions. Experiments demonstrate that our method achieves superior accuracy to existing state-of-the-art methods.
467. **Why Do Masked Neural Language Models Still Need Common Sense Knowledge?**  
    Recent work has shown that a large set of concepts can be learned from an arbitrary portion of unlabeled data with strong language modelling performance. However, our understanding of why this happens is not very clear. In this work we argue that because of the large number of classes and hidden neurons, simple-to-implement neural language models such as Masked Language Models require more common sense knowledge in order to generalize well to new classes. We empirically explore this under two different architectures of two different methods, Neural Inflection and Sum of Words. We show that there is a connection between the common sense knowledge required for neural language models and the subsequent accuracy of their output. Our experiments with a mixture of four commonly-used corpora show that there is a meaningful gap between the common sense knowledge required for neural language models and the accuracy they achieve. In addition, we find that this gap is significantly reduced when the number of hidden neurons is reduced.
468. **Cross-Lingual Relevance Transfer for Document Retrieval**  
    Cross-lingual document relevance is an important yet challenging problem with significant applications in Natural Language Processing. However, designing effective relevance models is often non-trivial due to the high variability in document sets and dependency structures among languages. In this paper, we propose a new cross-lingual relevance transfer model for similarity search. This model is generic and is applicable to a wide range of languages. Our model utilizes the multi-lingual context of the query-document pairs to learn the similarity similarities between documents. Moreover, the multi-lingual context is treated as a linguistic embedding in order to exploit the document similarity on-the-fly. Experimental results on a standard benchmark and three representative datasets from different research areas show that our model outperforms all baseline methods. Furthermore, our proposed model requires the minimum amount of linguistic annotation and is much faster. We hope that our approach will encourage a bigger number of researchers to apply cross-lingual document relevance to their applications.
469. **The TechQA Dataset**  
    Question Answering systems have made great strides in the past two years and have reached human level performance on many natural language processing tasks. While these systems still lag behind humans in some specific tasks, they have emerged as important components in modern intelligent systems that seek to understand a wide variety of information (web, news, social media, etc.) to help solve real-world problems. However, deep learning models are susceptible to adversarial examples, a major shortcoming of their systems. This can cause a model to not be able to correctly answer questions correctly even if it receives an input that is similar to a clean question. As such, the existence of adversarial examples poses an increasing problem to the trust of the AI systems deployed on our devices. We propose a dataset of 8,1997 QA questions for two classes of questions: (1) precision questions and (2) contrastive questions. While the precision questions have a fixed distribution, the contrastive questions have a probability distribution. These questions were originally created by linguists for automatic reference-based question answering and were released to the public in early 2015, together with the corresponding dataset. In this paper, we describe the TechQA dataset, the challenges that we faced in creating such a large-scale dataset and why we believe it can be useful to facilitate research on AI system robustness to adversarial examples.
470. **Neural Graph Embedding Methods for Natural Language Processing**  
    Natural language processing (NLP) has largely been improved by the implementation of various language modeling methods. However, current techniques are not suitable for efficient word representation due to the complicated graph structure of current NLP models. Furthermore, in recent years, researchers have paid much attention to learning embedding methods to capture structural information of words. However, the way these methods are constructed is far from ideal. In this paper, we propose a graph embedding method to represent words in a simpler way. It leverages the structural relationship of words to represent their semantic meaning. We analyze several state-of-the-art graph embedding models, and observe that the relationship among the words is rarely reflected by their representation. We propose a new neural network architecture, called Cascaded Normalized Bernoulli (CNB) network, to capture the relationship among the words. Our experiments show that our model outperforms the previous methods.
471. **Teacher-Student Training for Robust Tacotron-based TTS**  
    In this paper we present a fully automated pipeline for Tacotron-based Text-to-speech (TTS) systems. It is based on unsupervised learning on over a million sentence pairs extracted from the Stanford Natural Language Inference Corpus (SNLI). The pipeline consists of several machine learning modules. The automatic initialization of the encoder is done by a Bayesian classifier. It is then improved by an attention-based tree-structured encoder which preserves the abstract speech semantic structure over the entire model tree. Finally, the posterior of the model is passed to the decoder and a sequence of decoded words are reconstructed into speech. The proposed system uses attention to adapt the language model to the spoken input. Results of applying this pipeline on the TIMIT benchmark show that with a baseline of a simple decoder the decoded words perform significantly better than a standard unsupervised text-to-speech system. The proposed pipeline also yields a highly robust variant which makes use of a stronger unsupervised language model trained using the proposed attention-based tree-structured encoder. Our final system achieves an RTA of 25.63, and outperforms the state-of-the-art TTS system.
472. **Explicit Pairwise Word Interaction Modeling Improves Pretrained Transformers for English Semantic Similarity Tasks**  
    This paper proposes two novel vector spaces for training pretrained word embeddings to achieve high accuracy on word similarity tasks. The first is an approach based on textual entailment based on pre-trained word embeddings trained on a large corpus. The second is an extension to the classical multiplicative interactions between word embeddings, in which word pairs are modelled as vector combinations and word embeddings are modelled as new training sets. We demonstrate that these methods improve the word similarity performance in both WordNet tagging tasks and the same tasks for BILOC and POS Tagging. We show that these improvements translate to large improvements on the same tasks when cross-correlations between these tasks are taken into account. This work thus demonstrates that text-based word embeddings can be improved by utilizing information in a corpus. Our corpus-based methods thus contribute to the discovery of a better mechanism for extending the capabilities of word embeddings in a language modeling context.
473. **CROWN: Conversational Passage Ranking by Reasoning over Word Networks**  
    Most work on retrieval-based conversation processing focuses on standard question answering (QA) models. However, since humans often react to questions in a more creative manner, systems should consider future content. Inspired by this, we introduce CROWN, a ranking method for adaptive reading comprehension (AC) that ranks text passages according to how likely they are to answer a question asked in future. To do so, we first introduce a document-level conversation network (DNC) which is trained to model an initial document vector, and then used to classify new documents. This hierarchical structure allows us to model a longer time scale of the conversation along with a lower resolution of the vector at a given document. We then use a CNN called Moment Recognition Network to map from the document vector to a context vector, which encodes when a passage is likely to include a set of words, and use it to define a query, as well as a latent ranking score. CROWN uses a greedy procedure to infer the optimal passage by minimizing a cost function derived from the document-level conversation model, and can be used with either two-phase or non-two-phase methods. We show through extensive experiments on the ATIS corpus that using the DNC as a model of a document-level conversation network makes the task significantly easier, and we show that CROWN produces considerably better retrieval results when compared to a competitive state-of-the-art approach that is based on a memory network.
474. **Dependency and Span, Cross-Style Semantic Role Labeling on PropBank and NomBank**  
    PropBank is a widely-used source for prototypical knowledge bases with billions of tokens and a large amount of free text. Through the project Reunion, the leaders of the French National Language Institute (LNI) propose Semantic Role Labeling (SRL) on the benchmarking data provided by PropBank to define the ROUGE semantic metric. In this paper, we describe a method to extract positive syntactic roles from a given corpus and span-form training of Cross-Style Semantic Role Labeling (CSRL) on the BLEU corpus. For SRL, we present a meta-heuristic based on the distance metric between span and syntactic roles. In CSRL, we propose a series of techniques to learn an optimal span-form representation by using a variety of span patterns. We evaluate on the three benchmarking datasets used for the ROUGE of French and report state-of-the-art results compared to recent methods. In a qualitative analysis, we found that some syntactic roles are overrepresented in the POS tagged parts of the corpus.
475. **Transition-Based Deep Input Linearization**  
    In this paper we explore the use of Deep Neural Networks (DNNs) for the inference of product weight matrices. Our input linearization approach leverages DNN-specific weight matrices and assumes that the DNN is driven by a desired state-action value function. Specifically, this value function is assumed to be a linear function of the input vector from which the product weight matrix is composed. At inference time, the final output weight matrix can be inferred by the linearized DNN for a desired set of weights. We show how the state-action value function can be solved for with existing state-of-the-art techniques for value optimization for DNNs. To the best of our knowledge, this is the first step towards the use of DNNs as a universal parameterizer in machine learning. This new approach has the potential to drastically reduce the computational time required for exact inference and to increase the accuracy of the output weights.
476. **Dice Loss for Data-imbalanced NLP Tasks**  
    Tasks such as dependency parsing and named-entity recognition are data-imbalanced, in the sense that most annotated training data is either poorly-annotated or not of sufficient quality. In this paper, we investigate the effect of training data quality on NLP tasks and propose a novel approach for the alleviation of this problem. We define the model's loss for a given task as the weighted sum of the regular loss and a bias term. We show that by adding bias terms to the loss, the task-independent word representation improves the task-dependent performance of most NLP modules. We verify our model with experimental results on several datasets. Our model outperforms existing models by as much as 30% in absolute performance. Furthermore, experiments on the ranking model, which characterizes the degree to which a sentence is dependent on a specific entity, further confirm the advantage of model training on weak training data.
477. **Improving Grammatical Error Correction with Machine Translation Pairs**  
    This paper presents a case study of training a student model on two languages to improve its output by translating pairs of sentences. The model is trained on pairs of English-French pair and English-German pair. We observe that using individual sentences for model training helps towards improving both accuracy and precision of the model in grammatical error correction, which is similar to using bilingual corpora for language learning. The statistical relationships between translation pairs are explored and an algebraic approach is used to identify significant translation factors influencing the performance. The model is subsequently fine-tuned to improve performance when the source sentence is either a target sentence or a pseudo-target sentence. We note that larger generalization error rate is obtained when training from pseudo-target or pseudo-source sentence pairs.
478. **GORC: A large contextual citation graph of academic papers**  
    In this paper, we introduce the first scalable, open-source annotation tool that allows contextual citation data to be shared at a large scale. We describe how we built GORC, a large contextual citation graph for academic papers (GORC). Our experimental evaluation shows that most of the existing work on contextual citation systems focuses on short data sets such as bibliographic facts and user comments. We describe some of the fundamental tasks of the annotation process, including the development of the initial contextual citation graph and generation of the human-annotated work, which is publicly available. We also describe how we built a fully automated pipeline from such initial graph to a user-annotated academic paper, which was able to create over 30% of academic paper citations in the SemEval-2015 Task 5 shared task.
479. **The LIG system for the English-Czech Text Translation Task of IWSLT 2019**  
    This paper describes the LIG system developed at LUNA Computing for the English-Czech Text Translation Task of IWSLT 2019. This system has been integrated into the first official part of LUNA's in-house product system CLIPE. CLIPE provides an interactive evaluation platform for machine translation systems. A specific application of LIG is used for this purpose: the English-Czech machine translation system developed in the first part of LUNA's English-Czech Machine Translation System Project.
480. **Transformation of Dense and Sparse Text Representations**  
    A text representation is usually represented by one or more densely connected subspaces. Such representations are natural for human attention to a few lines of a document which is typically thought of as an essence. A common assumption however is that words have some sort of common structural properties, that is, subspaces that share certain fundamental characteristics. We study three types of such representations: i) compact subspaces, ii) sparse subspaces, and iii) dense subspaces. By focusing on different features that are computed and represented by subspaces, it is possible to understand how they affect the performance of models for natural language processing. We study different text representation methods: namely, dense subspaces, sparse subspaces, and dense subspaces plus word embeddings. We first analyze a number of structural properties and describe how these properties can impact on text similarity and on the performance of text encoding algorithms. Using such structural properties, we then apply simple composition methods to encode the entire text and analyze the effects of text composition on word embeddings. We conclude with an empirical evaluation that shows that models that use dense subspaces perform better than those that use sparse subspaces, while dense subspaces plus word embeddings perform better than those that use sparse subspaces. We believe that our results provide new insights into the role of subspaces in text and computation, and on the representation of information in general.
481. **Using Dynamic Embeddings to Improve Static Embeddings**  
    In this paper, we study the effect of dynamic embeddings on existing static embeddings. In particular, we explore the effects of several factors, including embedding quality, initialization of a neighborhood, learning rate, batch size, and the latent dimensionality of the data, on the performance of embeddings. We demonstrate empirically that static embeddings are found to have little performance degradation with increased embedding quality and that the best performing approaches are two-dimensional nearest neighbors. On the other hand, dynamic embeddings are shown to be particularly effective for faster, more accurate search on the MNIST dataset.
482. **BERTs of a feather do not generalize together: Large variability in generalization across models with similar test set performance**  
    Although BERT has achieved impressive results in many NLP tasks, we question whether it generalizes well across different deep learning models. On the other hand, we find that BERT in a subset of the downstream tasks, such as machine translation, improves only on a particular back-translation step. BERTs of a feather are not generalizable across different models when trained by the same teacher. We also find that BERT-based models with identical models and back-translations of sentence-level corpora across tasks often do not generalize better. These findings imply that careful tuning of the back-translated model is required.
483. **Contextualized Sparse Representation with Rectified N-Gram Attention for Open-Domain Question Answering**  
    Multi-hop word representations such as n-grams or normalized n-grams have been shown to be useful to tackle open-domain question answering. However, most of them are still insufficient to express the relations in question sentences in the text. To alleviate this problem, this paper proposes a new framework, contextualized sparse representation with rectified n-gram attention (CSRA), for open-domain question answering. CSRA incorporates a contextualized sparse representation and a normalized sparse representation to the text to learn the word-concept relation in question sentence. The contextualized sparse representation is enhanced by the positive relations between query and question words to learn the latent concepts with weighted ranking. To show the effectiveness of the proposed CSRA, we conduct extensive experiments on three standard question answering datasets: Yahoo! Answers, Stack Exchange, and Quora. Extensive evaluations on these three datasets show that our method outperforms the state-of-the-art methods.
484. **Enhancing Pre-trained Chinese Character Representation with Word-aligned Attention**  
    Most of existing attention models for Chinese text do not take great advantages of word-aligned parallel data and do not utilize word co-occurrence information. In this paper, we propose to extend our pre-trained pre-trained Chinese character representation (PreCDR) model with additional word-aligned attention. Experimental results demonstrate that this model is able to enhance pre-trained Chinese character representation for reducing word mismatch and achieving improved correlation.
485. **The State of NLP Literature: A Diachronic Analysis of the ACL Anthology**  
    This paper provides a diachronic, extended review of the fields that have contributed to the selection of ACL final reports. The analysis is based on diachronic articles in this field, as well as review papers that were accepted by the ACL conference and were published before its inception. A distinguishing feature of the current task of the ACL is the progressive focus on applications of NLP to corpora from different domains. It has been suggested that although current NLP efforts are focused on applications within a single field of research, the wider applicability of NLP is a far more fundamental goal for the discipline. The methodology presented here can be used to obtain further insights on this issue and the results presented can be used to compare state-of-the-art NLP applications to an ACL document collection from the past.
486. **An Annotation Scheme of A Large-scale Multi-party Dialogues Dataset for Discourse Parsing and Machine Comprehension**  
    The ability to compute answers for natural language utterances enables an intelligent agent to communicate. In many applications, however, such as social support systems, such an agent is required to automatically determine whether the inquiry sought by the user is complete or not. The proposed system aims at automatically processing both a multi-party dialogue and the relevant utterances in it. For this purpose, the output of a neural network-based discourse parser is encoded into an ontology that can be queried via a standard IR framework. An intermediate representation is then obtained, by combining, for each utterance, the ontology encoder with its human-written parser. The input for the current question is obtained by examining both the ontology encoder and the human-written parser. For reference, the semantic meanings of the terms and phrases are first estimated and are used for providing a rough answer. From this answer, the desired answer is obtained by applying a logic processor. Finally, an indication of the most important components of the ontology and parser is returned. Evaluation on a real multi-party dialogues dataset shows the effectiveness of the proposed model, comparing the performance of the existing state-of-the-art system and that of the new system.
487. **How Decoding Strategies Affect the Verifiability of Generated Text**  
    Predicting whether a document will be a fact or fiction is a longstanding problem for computational linguistics. Given a document and a set of candidate facts (headings) and a number of features, such as a given discourse relation between the document and the candidate facts, prediction is formulated as a supervised machine learning problem. For maximum model flexibility, previous work has adopted two forms of decoding strategies: by fully sequentially decoding a candidate fact from headings, and by decoding feature sub-spaces, each of which potentially covers a different subset of the candidate facts. In this paper, we find that this model-agnostic approach leads to overly simplified models, making it impossible to train a very accurate model, and argue that rather than using any particular decoding strategy, all of them should be combined into a single neural network to produce a more accurate model. Experiments with the Gutenberg corpus show that our combined model improves the model's word error rate by 18.6% over prior state-of-the-art models on all of the classes.
488. **Zero-Shot Paraphrase Generation with Multilingual Language Models**  
    Multilingual paraphrase generation (MPSG) is to generate phrases (or sentences) from multiple languages using a shared initial vocabulary. Existing MPSG methods have demonstrated the importance of utilizing multilingual linguistic and domain features in constructing robust models. However, learning multi-lingual representations at training time is less studied, and mostly ignored by existing methods. In this paper, we propose a novel method named Multilingual Word Embeddings with Noise for MPSG (MWE-MPSG), which significantly leverages the unsupervised MWE annotation by applying a domain adversarial training. Our proposed model then extracts and combines multi-lingual word embeddings from two different word-centric languages (one close to the source language and the other distant), where unlabeled target sentences with the same target language are sampled from the source language and the rest of the sentences are sampled from a new, unseen target language. We show that this new method is able to utilize a diverse set of multilingual features, and more importantly, that MWE annotation is helpful for training multilingual MPSG models. Experiments on two parallel data sets show that MWE-MPSG significantly outperforms existing methods.
489. **Table-to-Text Natural Language Generation with Unseen Schemas**  
    We address the problem of generating natural language summaries from documents and question-answering datasets. The most successful applications of table-to-text generation have so far focused on books, but a much larger range of applications exist across a wide range of domains, including online health services and restaurants. Most models for text summarization so far have focused on modeling standard "phrases" such as sentences and paragraphs. We propose the Model for Editing Table-to-Text (MEDT), which can generate table-to-text summaries for any document using a single sentence-pair input. MEDT incorporates the joint modeling of (1) model conditionalities and (2) sentiment state encoding via piecewise linear models, providing the semantic and syntactic properties of words with a more complete description of their meaning. We present and evaluate MEDT on three real-world datasets, demonstrating the effectiveness of MEDT in improving the quality of summary generation.
490. **Fully Quantizing a Simplified Transformer for End-to-end Speech Recognition**  
    An important problem in speech recognition is to efficiently handle unvoiced sources such as languages with low vocabulary sizes. Recent deep neural networks have successfully exploited the inherent redundancy in such unvoiced speech. A straightforward way to reduce the memory footprint of these neural networks is to quantize the weight matrices, resulting in a simplification of the network. However, it is often challenging to design such a network given the vast amount of high-dimensional features. Another important issue is the computation cost of such network, which is the bottleneck of current research in quantization. In this work, we propose a simple and efficient network architecture to fully quantize a simplified transformer network. We use a series of deep neural architectures that together enforce the simplification of the weights matrix and reduce the computation cost at each layer. Using experiments on the CHiME-5 dataset, we demonstrate that the resulting simple network outperforms all known state-of-the-art deep neural networks by a substantial margin in terms of both inference speed and memory footprint.
491. **Question Generation from Paragraphs: A Tale of Two Hierarchical Models**  
    There has been a notable increase in the use of machine learning and information extraction techniques in natural language generation tasks. Most of the work in this area has been in extractive machine learning (EML). In this paper, we introduce a novel framework to use explicit relation modeling for question generation from paragraphs, where all of the extracted relations in the paragraph are derived in the process of back-translation. We used two common tree-based LSTM architectures in order to model these relations and evaluate the quality of the generated question. The experimental results show that the explicit relation models are able to extract meaningful relations and produce higher quality sentences.
492. **Char-RNN and Active Learning for Hashtag Segmentation**  
    Hashtags have become a popular way to convey information to a group of people. However, as hashtags are usually short, tagging them correctly will be challenging. Most current techniques rely on task-specific training data. Our work has taken an opposite approach: we train a standard language model to classify hashtags into different classes. This model is trained to predict the hashtags that will best impact the user experience. We show that our proposed method improves the state-of-the-art models on two standard datasets.
493. **Should All Cross-Lingual Embeddings Speak English?**  
    Cross-lingual embedding refers to how to learn distributed representations that are well-suited to machine translation tasks and have good generalization capabilities to other languages. Currently, the most widely used cross-lingual embedding method is model agnostic and directly predicts cross-lingual embeddings for source languages without performing any pretraining on target languages. While this allows cross-lingual embeddings to be learned from few cross-lingual corpora, it lacks desirable properties, such as allowing cross-lingual embeddings to be used for large-scale cross-lingual language inference and having cross-lingual embeddings to be robust to translation ambiguity. In this paper, we focus on Cross-Lingual Embedding of English-Hindi Language pair, where we describe a method for directly learning cross-lingual embeddings in a model-agnostic manner from little cross-lingual corpora. The proposed method, called ILM, directly predicts English-Hindi word embeddings without any pretraining. The proposed approach is evaluated on four downstream tasks and three baseline methods. The experimental results show that our method outperforms the existing cross-lingual embedding methods. It also outperforms the baseline methods on all cross-lingual language inference tasks by significant margins.
494. **A Comprehensive Comparison of Machine Learning Based Methods Used in Bengali Question Classification**  
    Question classification is one of the most important tasks in machine learning community. This paper presents a comprehensive survey of state-of-the-art question classification methods using the Bengali language. We analyse the different methods from Bengali Question Classification (BQC), Polyglot Text Summarisation and the recently introduced deep learning based approach, Retrieval-based Bengali Question Classification (R-BQC). We evaluate them on a question dataset and evaluate their performance in relation to the suggested performance metric (question rank) of the researchers. The evaluation is done for the first time for this language pair, the results are encouraging with prominent results being obtained using the recent approaches.
495. **Are we asking the right questions in MovieQA?**  
    For Question Answering (QA), a common form of text-based conversation, one needs to score questions to classify an input string into a pre-defined question-answer set. In the MovieQA dataset, one can ask questions by supplying the movie and release the response of an existing question answering system. Previous works focus on learning a model to predict the answers and leverage this answer information to classify the input strings as satisfying or unsatisfying. In this work, we investigate different ways to use the answers for QA, and explore the combination of different methods. We first introduce a new approach to combine three different types of information for data generation. Our new model, by using the answers as examples, produces MovieQA-style data. We then explore the usage of different features and classifiers, and introduce the previously proposed multiple instance learning method. Finally, we evaluate the effectiveness of the different approaches using two language models, with the method based on predictors performing best.
496. **What Would Elsa Do? Freezing Layers During Transformer Fine-Tuning**  
    Fine-tuning is an essential part of modern language processing. An ideal iterative procedure starts with a simplified model first, then fine-tunes it iteratively on the smaller scale model. Although several type of fine-tuning methods have been proposed in the literature, it is still challenging to choose the most appropriate one. We here present a new approach for allocating computation resources during this fine-tuning process by using a hierarchical, flexible model, where each sub-model controls the importance of computation resources. The study is performed on the now standard Task1 dataset (to compare with other NLP frameworks), and we consider both word and character level model attributes. In order to evaluate this approach, we test our model on the latest WMT14 French English translation tasks, and the experimental results demonstrate the effectiveness of our approach compared with the previous baseline. Moreover, the new evaluation method can be extended to other languages, we experiment the effect of cross-language fine-tuning on English-Chinese translation.
497. **Relation-Gated Domain Adaptation for Relation Extraction**  
    Relation extraction is the task of extracting relationships between words in text. Most existing relation extraction systems, however, do not take into account the relationship between word pairs (words with same meaning). They instead capture the pairwise similarities of word pairs using only one node in a word embedding space. However, in many real-world applications, relationships between pairs of words may be observed with more than one node in the word embedding space, which is much more difficult to capture than pairwise similarities. In this paper, we present an approach to bridge this gap by formulating the relation extraction problem as a two-stage neural network: for the first stage, we learn a gated relation relation prediction module that is capable of predicting hidden variables that help model word pair similarities. In the second stage, a two-layer network is trained to predict hidden variables (i.e., soft links) that promote the contribution of each relationship variable to the prediction. The objective function for the first stage is to predict the hidden variables for hidden variables (i.e., weak links) that are of higher relevance to the prediction. For the second stage, we also learn a parameter estimation model in the form of a ridge regression, which allows the method to learn the relationship between input relations and hidden variables. The second stage allows to encode the relationship between word pairs into their representations in the embedding space, which further improves the prediction performance. Experimental results on two relation extraction benchmarks demonstrate the effectiveness of the proposed model, which outperforms other state-of-the-art systems.
498. **Low-Level Linguistic Controls for Style Transfer and Content Preservation**  
    The ability to reproduce or adapt the styles of different input sentences is essential for many applications, ranging from computer vision to machine translation. In this paper, we present a new neural approach to learn visual-level compositional styles, and its application in the context of content preservation. Our approach is a purely neural model based on character-level language models which mimic the features of text, and which we modify for stylization and content preservation. We find that our approach is able to successfully learn good simple visual-level compositional styles as well as complex more complex compositional styles for monolingual training data. We also analyze the learnability of the compositional styles by integrating transfer learning into the neural models. In our experiments, the styles that our method learns achieve higher BLEU scores than existing methods for content preservation, especially when those styles require a transfer task.
499. **Pretrained Language Models for Document-Level Neural Machine Translation**  
    Pretrained language models have shown promising results in text-to-speech and machine translation. However, the training process for these models requires a large amount of labelled data, which is not available in most resource-poor languages. We address this limitation by proposing to use pre-trained language models to replace large amounts of expensive labelled text. These pretrained language models outperform large amounts of available unlabelled text. We also present a framework for data augmentation to improve the performance of pretrained language models on low-resource scenarios. Finally, we present a novel language model named Transfer-JIT-English, which is the first model to take advantage of transfer learning to improve the performance of pretrained language models on low-resource languages. We evaluated our approach on two low-resource translation tasks: Croatian-English and Chinese-English.
500. **iSarcasm: A Dataset of Intended Sarcasm**  
    We introduce a new dataset for automatically constructing linguistic parses of sarcasm. The task of sarcasm detection is an emerging challenge in natural language processing. While many datasets are available for detecting sarcasm, often the underlying semantics of a sarcastic text are not well-defined. For instance, in a web article (for example, news), some of the characters of a sarcastic text may appear to be supporting, questioning, or even complimenting other characters of the text, thus failing to accurately define the sarcasm content. To overcome this shortcoming, we present a new dataset for sarcasm detection, iSarcasm, which aims to generate sarcastic texts by combining (a) paraphrasing natural text and (b) grammar of a sarcastic text. In particular, iSarcasm consists of 55K sarcastic Wikipedia articles that are accompanied by 71K unannotated Wikipedia articles (complete with the type annotations of sarcasm). We manually annotate the sarcasm content in the articles. We show that parsing the annotations of the Wikipedia articles significantly improves the performance of automatic sarcasm detection, especially for the case of non-established semantic cases such as gentle criticism. In addition, we show that predicting sarcasm from the context of natural text is much more challenging than predicting sarcasm from the text alone.
501. **How to Do Simultaneous Translation Better with Consecutive Neural Machine Translation?**  
    Many recent Neural Machine Translation (NMT) models incorporate additional decoding operations which increase the cost of each decoding step. While parallel neural architecture search (NAST) was proposed to improve NMT decoding speed by reusing computation on the source side, previous studies have examined the effect of simultaneous decoding on the encoder side as well. In this paper, we propose an efficient way to apply NAST for simultaneous decoding on the encoder side. We present a novel way of identifying words to be passed along to the NMT decoder by an attentive recurrent neural network (RNN). By considering word overlap as a regularization term, we effectively expand the vocabulary size of the NMT encoder without sacrificing accuracy. Experiments on Chinese-English and Japanese-English translation tasks show that our method improves the NMT performance.
502. **Europarl-ST: A Multilingual Corpus For Speech Translation Of Parliamentary Debates**  
    We present the first available corpus for English-speaking parliamentary debates. It consists of 416 classified files from 23 Parliamentary debate tracks. The second paper provides an overview of the original challenges and features of the corpus. The resulting corpus should be useful to future research, as it has many potential new dimensions and advantages. The third paper provides a detailed study of text classification techniques used on the corpus and the results obtained by the developed algorithms.
503. **Domain Robustness in Neural Machine Translation**  
    Neural machine translation (NMT) is typically trained on large bilingual corpora containing parallel sentences. However, the resulting NMT model is guaranteed to output a fixed number of translations for each source sentence, and cannot be used in practical settings with high-throughput, small-resource languages or for low-resource settings in cases where training data is unavailable. Recent work has shown that such partial domain adaptation may be caused by translating from a domain in which it is known that the NMT model does not generalize, as opposed to translating from the domain where the model works well. We present an exploration of the effect of partial domain adaptation on NMT training and performance, and investigate potential mechanisms behind it. In particular, we analyze the impact of domain shift for the CEC-2018 English-German shared task, and show that by adapting from the EU domain, trained models are less sensitive to domain shift and are able to generalize more robustly. In addition, we show that the effect of domain shift is not entirely reduced if a prior transfer from another source domain is used instead, but rather worse performance is observed in such a situation.
504. **Why Deep Transformers are Difficult to Converge? From Computation Order to Lipschitz Restricted Parameter Initialization**  
    In this work we address the issue of convergence to local minima and quantitatively study how the network optimization function changes as a function of the initialization. Specifically, we propose to study the rate of convergence in terms of the algorithmic complexity, showing that it is less likely to converge to local minima in deep learning than it is in general nonlinear networks. Furthermore, we analyze the effect of training epochs, showing that deep nets trained by gradually increasing the size of the network are more resilient to the algorithmic initializations used for training. Finally, we show that by introducing a regularization term for the initialization, an artificially trained vanilla CNN with only sparse initializations can converge to the proper local minimum.
505. **A Good Sample is Hard to Find: Noise Injection Sampling and Self-Training for Neural Language Generation Models**  
    We study the performance of neural language generation models with a supervised multi-task learning approach on the task of automatic speech recognition. Compared with unsupervised self-training methods, which induce expert knowledge of the model, our approach generates better target utterances in an effort to better learn the discriminative neural network model. Our proposed objective function, which is derived from well-known gradient descent variants, learns a target distribution that better approximates the noisy target distribution as a function of training data. When compared with a cross entropy loss and other competitive baselines, we observe improved overall perplexity and generation of correct target utterances on the TIMIT and Switchboard data sets. Moreover, we examine the perturbation of the target distribution by our proposed objective function and the role of an out-of-domain example model in noisy training.
506. **SEPT: Improving Scientific Named Entity Recognition with Span Representation**  
    Named entity recognition (NER) is a fundamental task in Natural Language Processing (NLP) for semantic information retrieval and information extraction. Despite that NER has achieved outstanding performance in recent years, most of existing methods are usually applied in limited domains and need multiple training datasets with fixed inter-domain alignments. Since NER is not able to directly support name prediction, we propose the span-based method named SEPT (SMSENet). By treating the target named entities as span blocks, SMSENet provides more comprehensive information as the target entities are grouped into different spans while the target named entities are grouped together with their span blocks to deal with the complex situations of multi-domain NER. Extensive experiments show that our method can outperform the state-of-the-art methods in both named entity recognition and named entity prediction, especially in the case of multi-domain NER.
507. **Negated LAMA: Birds cannot fly**  
    A recent work had shown that it is possible to evaluate whether a certain birds can fly. However, no results were presented for this task when the birds were represented as an artificial artificial neural network (ANN). In this work, we propose a simple and effective method for evaluation of birds. The method is based on negating input neurons which may be directly connected to the ANN. We evaluate our method on several benchmark datasets and show that it is able to evaluate whether a bird can fly.
508. **Transforming Wikipedia into Augmented Data for Query-Focused Summarization**  
    A large portion of Wikipedia content is highly technical, consisting of documents that need careful textual treatment. Thus, its statistical coverage can be very sparse. More recently, researchers have started making use of data mining techniques to produce interpretable reports and summaries, but most of them are often textual or they lack the flexibility of in-domain article summarization approaches. To facilitate the acquisition of information from a large source of technical documents, we propose an approach that transforms Wikipedia into an augmented data source. The transformation takes place by simultaneously transforming the coverage of article content (i.e. articles and related sources) and the coverage of non-article content. We have experimented with content-coverage models from the context of query-focused text summarization, demonstrating the utility of our approach.
509. **How Language-Neutral is Multilingual BERT?**  
    The Transformer was recently shown to beat existing state-of-the-art models for both English and German language tasks. While Transformer is known for its multilingualism, there has been little investigation on what its multilingual capacity might be. In this work, we systematically study the multilingual capacity of Transformer in multilingual settings using BERT. We show that multilingual BERT not only outperforms multilingual Transformer but also allows it to achieve new state-of-the-art on three languages, improving the F1 score from 0.49 to 0.59 on English$\to$German, from 0.47 to 0.52 on French$\to$English, and from 0.50 to 0.55 on Dutch$\to$English. On all three languages, multilingual BERT improves the performance of neural machine translation models, giving up to 17% in BLEU@10 in English$\to$German and up to 14% in BLEU@10 in French$\to$English. Our work indicates that BERT achieves multilingualism in a purely language-independent manner by taking advantage of the large amount of contextual information in multilingual corpora, without adding any additional parameter.
510. **Crowdsourcing a High-Quality Gold Standard for QA-SRL**  
    Machine reading comprehension (MRC) models rely heavily on massive parallel corpora, which are expensive to collect. In contrast, text-based machine reading comprehension (QA-SRL) models employ short text summaries, which are easy to collect and generate. However, there are multiple ways to generate summary-based QA-SRL datasets. In this paper, we systematically investigate how to obtain the gold standard for QA-SRL. We introduce two standard QA-SRL datasets and determine which parts of these corpora can be used to train QA-SRL models. We develop a two-step crowdsourcing framework, where, first, a person volunteers their QA-SRL summary and second, the system learns the QA-SRL model from this summary. We then evaluate three existing QA-SRL models. Experimental results show that the resulting gold standard is highly accurate for training QA-SRL models, and the QA-SRL models achieve a state-of-the-art performance on the newly created QA-SRL datasets.
511. **Composing and Embedding the Words-as-Classifiers Model of Grounded Semantics**  
    Semantic grammars are the unifying theory of human language processing. Semantic grammars are probabilistic models for the production of semantic representations of words and phrases. Since data are typically gathered from automatic speech recognition and text summarization systems, the meaning of words and phrases are defined by these systems. We describe a model that, by representing the meaning of words as distributions over the semantic properties of space, can represent language by representation. We describe how the distribution model applies to natural language parsing, and how to integrate it with a model that represents words as probabilities and phrases as classifiers